# Copilot Agent â†” GitHub Actions Integration

## TL;DR: Jak to wspÃ³Å‚dziaÅ‚a?

**Copilot NIE uruchamia testÃ³w bezpoÅ›rednio** - robi to GitHub Actions. Ale Copilot **reaguje na feedback z testÃ³w** i poprawia kod automatycznie.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ITERACYJNY CYKL: Code â†’ Test â†’ Feedback â†’ Fix â†’ Repeat         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. ğŸ¤– Copilot pushes code â†’ copilot/176-testing-infra
2. âš¡ GitHub Actions triggers automatically
3. ğŸ§ª Tests run, coverage checked, lint/typecheck
4. âŒ IF FAIL:
   â†’ Comment posted on issue with errors
   â†’ ğŸ¤– Copilot READS the comment
   â†’ ğŸ¤– Copilot FIXES the code
   â†’ ğŸ¤– Copilot PUSHES again (back to step 2)
5. âœ… IF PASS:
   â†’ PR auto-created
   â†’ ğŸš€ Preview deployed to Vercel
   â†’ ğŸ‘¨â€ğŸ’» Human review + merge
```

---

## SzczegÃ³Å‚owy Flow

### Faza 1: Planning (Human + Planning Agent)

```yaml
User: "Create testing infrastructure"
  â†“
Planning Agent creates plan with test_scenarios.yaml:
  test_scenarios:
    unit:
      - component: "DatabaseService.getBook"
        test_cases:
          - name: "Happy path - returns book"
            arrange: "Mock Supabase..."
            act: "await db.getBook('123')"
            assert:
              - "Returns book object"
              - "Supabase called once"
  â†“
Planning Agent posts to issue #176 as structured comment
  â†“
User reviews and approves plan
```

**Output:** Test scenarios YAML in issue comment (reviewable by humans)

---

### Faza 2: Implementation (Copilot Agent)

```yaml
User: "@copilot implement this"
  â†“
Copilot reads:
  - Issue description
  - Planning agent's comment with test_scenarios.yaml
  - Existing codebase patterns
  â†“
Copilot writes ONLY implementation code (NO tests):
  - services/database.ts
  - routes/books.routes.ts
  - schemas/book.schema.ts
  â†“
Copilot commits to branch: copilot/176-testing-infra
  â†“
Copilot pushes â†’ Triggers GitHub Actions
```

**Output:** Implementation code pushed to branch (NO tests yet)

**Important:** Copilot nie pisze testÃ³w - to robi GitHub Actions w nastÄ™pnym kroku!

---

### Faza 3: Test Generation (GitHub Actions - Automated)

```yaml
GitHub Actions workflow: copilot-branch-pipeline.yml
  â†“
Job 1 - detect:
  - Extract issue number from branch name
  - Get changed files (database.ts, books.routes.ts)
  - Download test_scenarios.yaml from issue comment
  â†“
Job 2 - generate-tests:
  - Run: scripts/generate_tests_from_scenarios.py
  - Input:
      scenarios: test_scenarios.yaml
      changed_files: database.ts, books.routes.ts
  - Process:
      For each changed file:
        1. Match scenarios to file
        2. Read source code
        3. Call OpenAI GPT-4o-mini
        4. Generate complete Vitest test file
  - Output:
      apps/backend/src/__tests__/database.test.ts
      apps/backend/src/__tests__/routes/books.test.ts
  â†“
Job 2 - generate-tests (continued):
  - Commit generated tests to same branch
  - Push to copilot/176-testing-infra
```

**Output:** Auto-generated test files committed to branch

**Cost:** ~$0.02-0.05 per file (GPT-4o-mini is cheap)

---

### Faza 4: Quality Gate (GitHub Actions - Decision Point)

```yaml
Job 3 - test-backend:
  - Pull latest code (with generated tests)
  - Run: pnpm test --coverage
  - Collect results
  â†“
Job 4 - quality-gate:
  - Check: Lint âœ…/âŒ
  - Check: Type-check âœ…/âŒ
  - Check: Tests pass âœ…/âŒ
  - Check: Coverage â‰¥80% âœ…/âŒ
  â†“
DECISION:
  If ALL PASS â†’ Job 5 (create-pr)
  If ANY FAIL â†’ Job 6 (feedback)
```

**Output:** Pass/Fail decision with detailed report

---

### Faza 5A: Success Path (All Tests Pass)

```yaml
Job 5 - create-pr:
  - Check if PR already exists
  - If NO:
      Create new PR:
        Title: Issue title
        Body: Quality gate report + changes summary
        Assignee: Original issue assignee
        Labels: ready-for-review
  - If YES:
      Update existing PR with comment:
        "âœ… Quality gate passed! Updated with latest changes."
  â†“
Job 7 - deploy-preview:
  - Build apps (dashboard/storefront)
  - Deploy to Vercel
  - Generate preview URL: https://pr-176-dashboard.vercel.app
  - Comment on issue with preview link
  â†“
ğŸ‘¨â€ğŸ’» Human reviews PR:
  - Check business logic
  - Test in preview environment
  - Approve and merge
```

**Output:** PR ready for human review + live preview

---

### Faza 5B: Failure Path (Tests Fail) - ğŸš¨ KLUCZ!

```yaml
Job 6 - feedback:
  - Extract quality gate report
  - Post comment on issue #176:
  
âŒ **Quality gate failed**

The tests or quality checks did not pass. Please review and fix:

âŒ TypeScript error in database.ts:42
   Type 'string | undefined' is not assignable to type 'string'

âŒ Test failed: DatabaseService.getBook should return book
   Expected: { id: '123', title: 'Test' }
   Received: { id: '123', title: undefined }

âŒ Coverage below threshold: 75% < 80%
   Missing coverage in: database.ts lines 42-55

---

### ğŸ”§ What to do next:

1. @copilot: Review the failures above
2. Fix the issues in your code
3. Push again to `copilot/176-testing-infra`
4. Pipeline will re-run automatically
```

**Output:** Structured feedback comment on issue

---

### Faza 6: Copilot Reads Feedback & Fixes (AUTOMATIC)

**ğŸ¤– Copilot Bot Behavior:**

```python
# Copilot agent continuously monitors assigned issues
while issue.assignee == "copilot":
    # Check for new comments
    new_comments = issue.get_comments_since_last_check()
    
    for comment in new_comments:
        if "Quality gate failed" in comment.body:
            # Parse feedback
            errors = extract_errors(comment.body)
            
            # Copilot reads:
            # 1. Error messages
            # 2. Failed test names
            # 3. Coverage gaps
            # 4. Line numbers with issues
            
            # Copilot analyzes code
            for error in errors:
                file = error.file
                line = error.line
                message = error.message
                
                # Find root cause
                context = read_file_context(file, line, radius=10)
                
                # Generate fix
                fix = generate_fix(context, error)
                
                # Apply fix
                apply_code_change(file, line, fix)
            
            # Copilot commits
            git commit -m "fix: Address quality gate failures
            
            - Fixed type error in database.ts:42
            - Added missing field in getBook method
            - Improved coverage in database operations
            
            Responding to: [workflow run link]
            "
            
            # Copilot pushes
            git push origin copilot/176-testing-infra
            
            # ğŸ”„ GOTO Faza 3 - workflow triggers again!
```

**Kluczowe:**
1. âœ… Copilot **automatycznie** czyta komentarze z "Quality gate failed"
2. âœ… Copilot **parsuje** bÅ‚Ä™dy (regex patterns)
3. âœ… Copilot **znajduje** odpowiednie pliki i linie
4. âœ… Copilot **poprawia** kod
5. âœ… Copilot **pushuje** automatycznie
6. âœ… Workflow **re-run** automatycznie przy kaÅ¼dym pushu

---

## Iteracyjny Cykl - PrzykÅ‚ad Rzeczywisty

### Iteration 1: Type Error

```
Push 1: Copilot writes initial code
  â†“
Test: âŒ TypeScript error
Feedback: "Type 'string | undefined' not assignable to 'string'"
  â†“
Fix: Copilot adds null check
Push 2: git push (automatic)
```

### Iteration 2: Test Failure

```
Push 2: With null check
  â†“
Test: âŒ Test fails - missing field
Feedback: "Expected book.title, received undefined"
  â†“
Fix: Copilot adds title field mapping
Push 3: git push (automatic)
```

### Iteration 3: Coverage Gap

```
Push 3: With title field
  â†“
Test: âŒ Coverage 75% < 80%
Feedback: "Missing coverage in lines 42-55"
  â†“
Fix: Copilot adds tests for edge cases
Push 4: git push (automatic)
```

### Iteration 4: Success!

```
Push 4: Complete implementation
  â†“
Test: âœ… All tests pass
      âœ… Coverage 82%
      âœ… Lint OK
      âœ… Type-check OK
  â†“
Action: PR auto-created
        Preview deployed
  â†“
Human: Reviews and merges
```

**Total time:** ~30-60 minutes (vs 4-6 hours manual)

---

## Dlaczego Ten PodziaÅ‚ OdpowiedzialnoÅ›ci?

### Planning Agent â†’ Test Scenarios (Human-Reviewable)

**Pros:**
- âœ… Stakeholders mogÄ… zreviewowaÄ‡ "co testujemy"
- âœ… Clear acceptance criteria przed kodem
- âœ… Test scenarios sÄ… dokumentacjÄ…
- âœ… MoÅ¼na approve bez technical knowledge

**Cons:**
- âš ï¸ Wymaga rÄ™cznego review planning phase
- âš ï¸ Dodatkowy krok w workflow

### GitHub Actions â†’ Test Code (Automated)

**Pros:**
- âœ… Zero manual test writing
- âœ… Consistent test quality (same LLM)
- âœ… Very cheap ($0.02-0.05 per file)
- âœ… Fast (parallel generation)

**Cons:**
- âš ï¸ Wymaga OpenAI API key
- âš ï¸ MoÅ¼e generowaÄ‡ suboptimal tests (ale Copilot poprawi)

### Copilot Agent â†’ Implementation (Iterative)

**Pros:**
- âœ… Automatyczne poprawki po kaÅ¼dym fail
- âœ… Nie blokuje human reviewers
- âœ… Learns from feedback loop
- âœ… DziaÅ‚a 24/7

**Cons:**
- âš ï¸ MoÅ¼e wejÅ›Ä‡ w infinite loop (max 10 iterations)
- âš ï¸ Wymaga dobrze napisanych error messages

---

## Konfiguracja Wymagana

### 1. GitHub Secrets

```bash
# W repozytorium Settings â†’ Secrets and variables â†’ Actions
OPENAI_API_KEY=sk-...              # Dla generate_tests_from_scenarios.py
VERCEL_TOKEN=...                   # Dla Vercel previews
VERCEL_ORG_ID=...                  # Vercel organization
VERCEL_PROJECT_ID=...              # Dashboard project ID
```

### 2. Copilot Agent Configuration

```yaml
# .github/agents/testing-specialist.agent.md lub planning-agent.agent.md
monitoring:
  watch_for_comments: true
  react_to_patterns:
    - "Quality gate failed"
    - "Tests failed"
    - "Coverage below threshold"
  
  action_on_match:
    - Read full comment
    - Parse errors (regex)
    - Fix code
    - Commit + push
```

### 3. Issue Assignment

```
When Copilot is assigned to issue:
  1. Planning agent creates test scenarios
  2. User approves
  3. Copilot implements code
  4. Workflow runs tests
  5. If fail â†’ Copilot fixes (loop)
  6. If pass â†’ PR created
```

---

## Monitoring & Debugging

### Check Pipeline Status

```bash
# W GitHub UI:
Actions â†’ Copilot Branch Pipeline
  â†’ See all runs for copilot/** branches
  
# KaÅ¼dy push triggeruje nowy run
# MoÅ¼esz zobaczyÄ‡:
  - KtÃ³re testy siÄ™ popsuly
  - Coverage metrics
  - Deployment logs
```

### Check Copilot Activity

```bash
# W Issue #176:
  - Timeline pokazuje wszystkie Copilot commits
  - Comments pokazujÄ… feedback z workflow
  - Labels pokazujÄ… status (ready-for-review, needs-work)
```

### Manual Override

```bash
# JeÅ›li Copilot nie moÅ¼e naprawiÄ‡:
git checkout copilot/176-testing-infra
# Fix manually
git commit -m "fix: Manual fix for failing test"
git push
# Workflow re-runs automatically
```

---

## FAQ

**Q: Co jeÅ›li Copilot wejdzie w infinite loop?**
A: Workflow ma max 10 iterations, potem fail i manual review required

**Q: Czy mogÄ™ wyÅ‚Ä…czyÄ‡ auto-test generation?**
A: Tak, usuÅ„ job `generate-tests` z workflow lub nie dodawaj test_scenarios.yaml

**Q: Co jeÅ›li testy sÄ… sÅ‚abej jakoÅ›ci?**
A: Copilot je poprawi w kolejnych iterations, lub human moÅ¼e poprawiÄ‡ rÄ™cznie

**Q: Czy to dziaÅ‚a dla E2E testÃ³w?**
A: Nie, E2E sÄ… w osobnym Playwright workflow (nie auto-generated)

**Q: Ile to kosztuje w API calls?**
A: ~$0.02-0.05 per file dla test generation, ~$0.50-1.00 per iteration dla Copilot fixes

**Q: Czy mogÄ™ uÅ¼yÄ‡ lokalnie?**
A: Tak, `python scripts/generate_tests_from_scenarios.py --local`

---

## Comparison: Before vs After

### Before (Manual)

```
Developer writes code (2h)
  â†“
Developer writes tests (2h)
  â†“
Developer runs tests locally (5min)
  â†“
Fix errors (30min)
  â†“
Push to PR
  â†“
CI runs (10min)
  â†“
Code review (1h)
  â†“
Human merges

Total: 5.5h + human time
```

### After (Automated)

```
Planning agent: test scenarios (15min + human review)
  â†“
Copilot writes code (20min)
  â†“
Push â†’ GitHub Actions:
  - Generate tests (2min)
  - Run tests (3min)
  - Deploy preview (5min)
  â†“
Auto-feedback to Copilot if fail (instant)
  â†“
Copilot fixes automatically (10min per iteration)
  â†“
PR auto-created when pass
  â†“
Human review (30min)
  â†“
Human merges

Total: 45min-1h + human review (if 2-3 iterations)
```

**Time savings: 75-80%**
**Cost: ~$2-5 per task (API costs)**

---

## Summary

âœ… **Copilot pisze kod, NIE testy**
âœ… **GitHub Actions generuje testy z scenarios**
âœ… **GitHub Actions uruchamia testy**
âœ… **GitHub Actions daje feedback**
âœ… **Copilot czyta feedback automatycznie**
âœ… **Copilot poprawia kod automatycznie**
âœ… **Cykl repeats aÅ¼ do sukcesu**
âœ… **Human tylko review final PR**

**To jest peÅ‚na automatyzacja z human oversight na krytycznych momentach!** ğŸš€
