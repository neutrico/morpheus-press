milestone: M0 - Infrastructure & Setup
task_count: 18
issues:
- key: T1
  title: Tech Stack Decision Documentation
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nThis task involves creating comprehensive documentation\
      \ of the technology stack decisions for the Morpheus platform. This is critical\
      \ for M0 as it establishes the architectural foundation, ensures team alignment,\
      \ and provides justification for technology choices. Without proper documentation,\
      \ future developers will struggle to understand why specific technologies were\
      \ chosen, leading to inconsistent implementations and potential rewrites.\n\n\
      **Technical Approach:**\n- Use Architecture Decision Records (ADRs) format for\
      \ structured decision documentation\n- Create a centralized tech stack registry\
      \ with version constraints and upgrade policies  \n- Implement automated dependency\
      \ tracking and vulnerability scanning\n- Document integration patterns between\
      \ services (API contracts, event schemas)\n- Establish coding standards and\
      \ linting rules per technology\n- Create decision templates for future technology\
      \ evaluations\n\n**Dependencies:**\n- External: [@typescript-eslint/eslint-plugin,\
      \ @playwright/test, supabase-js, openai, anthropic-ai]\n- Internal: [turborepo\
      \ configuration, shared ESLint configs, TypeScript project references]\n\n**Risks:**\n\
      - Technology lock-in: Document migration strategies and abstraction layers for\
      \ critical dependencies\n- Version drift: Implement workspace dependency constraints\
      \ and automated updates\n- Knowledge silos: Ensure documentation is accessible\
      \ and searchable, not buried in wikis\n- Outdated decisions: Establish review\
      \ cycles and decision deprecation processes\n\n**Complexity Notes:**\nMore complex\
      \ than initially thought due to the multi-service architecture requiring documentation\
      \ of:\n- Inter-service communication patterns (REST, WebSockets, pub/sub)\n\
      - Shared library versioning strategies across workspaces\n- Database schema\
      \ evolution and migration strategies\n- ML model deployment and versioning workflows\n\
      \n**Key Files:**\n- docs/architecture/: ADR files for each major technology\
      \ decision\n- package.json: Workspace dependency constraints and tooling versions\n\
      - .eslintrc.js: Shared linting rules across all workspaces\n- turbo.json: Build\
      \ pipeline and caching strategies\n- README.md: High-level architecture overview\
      \ and quick start\n"
    design_decisions:
    - decision: Use Architecture Decision Records (ADRs) for documentation
      rationale: Provides structured, version-controlled, and contextual documentation
        that lives with the codebase
      alternatives_considered:
      - Confluence/Wiki pages
      - Code comments only
      - Inline documentation
    - decision: Implement strict workspace dependency management
      rationale: Prevents version drift in monorepo and ensures consistent tooling
        across services
      alternatives_considered:
      - Allow flexible versions
      - Manual coordination
      - Separate repositories
    - decision: Document API contracts using OpenAPI/JSON Schema
      rationale: Enables contract-first development and automated validation between
        services
      alternatives_considered:
      - Code-first documentation
      - Manual API docs
      - GraphQL schema
    researched_at: '2026-02-08T16:51:44.149828'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T18:58:11.694665'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 5545ab03
    planning_hash: b67369ed
  technical_notes:
    approach: 'Create a structured documentation system using ADRs stored in the repository
      alongside code. Implement automated tooling to validate and update technology
      inventories. Establish templates and processes for evaluating new technologies.
      Use OpenAPI specifications for service contracts and maintain a central registry
      of all external dependencies with their purposes and alternatives.

      '
    external_dependencies:
    - name: '@typescript-eslint/eslint-plugin'
      version: ^7.0.0
      reason: TypeScript-specific linting rules across all services
    - name: madge
      version: ^6.1.0
      reason: Dependency graph visualization and circular dependency detection
    - name: npm-check-updates
      version: ^16.14.0
      reason: Automated dependency update tracking and management
    - name: swagger-jsdoc
      version: ^6.2.8
      reason: Generate OpenAPI specs from code comments in Fastify services
    files_to_modify:
    - path: README.md
      changes: Add architecture overview, quick start guide, tech stack summary section
    - path: package.json
      changes: Add stack audit scripts, dependency constraint validation
    - path: turbo.json
      changes: Add documentation build tasks, stack validation pipeline
    new_files:
    - path: docs/architecture/decisions/001-frontend-framework.md
      purpose: ADR documenting React/Next.js choice with alternatives considered
    - path: docs/architecture/decisions/002-backend-framework.md
      purpose: ADR documenting Node.js/Express choice with performance considerations
    - path: docs/architecture/decisions/003-database-selection.md
      purpose: ADR documenting Supabase/PostgreSQL choice with scaling strategy
    - path: docs/architecture/decisions/004-ai-ml-stack.md
      purpose: ADR documenting OpenAI/Anthropic integration patterns
    - path: docs/architecture/decisions/005-devops-tooling.md
      purpose: ADR documenting CI/CD, monitoring, deployment strategy
    - path: docs/architecture/decisions/006-testing-strategy.md
      purpose: ADR documenting Jest/Playwright testing approach
    - path: docs/architecture/tech-stack.json
      purpose: Machine-readable tech inventory with versions, purposes, alternatives
    - path: docs/architecture/integration-patterns.md
      purpose: Document service communication patterns, event schemas, API contracts
    - path: docs/development/coding-standards.md
      purpose: Language-specific coding standards and linting rules
    - path: docs/development/dependency-management.md
      purpose: Workspace dependency policies, update procedures, security practices
    - path: docs/api/core-service.yaml
      purpose: OpenAPI specification for core backend service
    - path: docs/api/ai-service.yaml
      purpose: OpenAPI specification for AI processing service
    - path: docs/examples/service-integration.md
      purpose: Code examples for common integration patterns
    - path: tools/stack-validator.js
      purpose: Script to validate tech stack compliance and generate reports
    - path: tools/adr-template.md
      purpose: Template for future architecture decision records
    - path: .eslintrc.shared.js
      purpose: Shared ESLint configuration across all workspaces
  acceptance_criteria:
  - criterion: All major technology stack decisions are documented as ADRs in standardized
      format
    verification: Check docs/architecture/decisions/ contains ADRs for Frontend, Backend,
      Database, AI/ML, DevOps, and Testing stack decisions with consistent structure
  - criterion: Central tech stack registry with version constraints and security scanning
      is operational
    verification: Run 'npm audit' and 'turbo run security-check' successfully, verify
      tech-stack.json exists with all dependencies categorized
  - criterion: Development workflow documentation enables new developers to start
      contributing within 2 hours
    verification: Fresh checkout -> follow README -> run 'npm run dev' -> access running
      application with all services healthy
  - criterion: Inter-service communication patterns are documented with OpenAPI specs
      and examples
    verification: Check docs/api/ contains OpenAPI specs for all services, integration
      examples in docs/examples/
  - criterion: Automated tooling validates tech stack compliance and generates dependency
      reports
    verification: Run 'npm run stack-audit' generates current tech inventory, 'npm
      run lint-stack' validates compliance with documented standards
  testing:
    unit_tests:
    - file: tools/__tests__/stack-validator.test.ts
      coverage_target: 90%
      scenarios:
      - ADR format validation
      - Dependency version constraint checking
      - Tech stack inventory generation
      - Security vulnerability detection
    integration_tests:
    - file: tools/__tests__/integration/documentation-sync.test.ts
      scenarios:
      - Generated docs match actual package.json dependencies
      - OpenAPI specs validate against running services
    manual_testing:
    - step: 'New developer onboarding simulation: clone repo, follow setup docs'
      expected: All services running within 30 minutes, no missing dependencies
    - step: Tech stack decision template usage for new technology evaluation
      expected: Template guides through evaluation criteria and generates valid ADR
  estimates:
    development: 2
    code_review: 1
    testing: 0.5
    documentation: 0.5
    total: 4
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create ADR template and directory structure in docs/architecture/decisions/'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review and finalize architecture decisions for each major technology
        choice'
      done: false
      ai_friendly: false
    - task: '[AI] Generate ADR documents for Frontend (React/Next.js), Backend (Node.js),
        Database (Supabase)'
      done: false
      ai_friendly: true
    - task: '[AI] Generate ADR documents for AI/ML stack (OpenAI/Anthropic), DevOps,
        Testing frameworks'
      done: false
      ai_friendly: true
    - task: '[AI] Create tech-stack.json inventory with all dependencies categorized
        by purpose'
      done: false
      ai_friendly: true
    - task: '[AI] Generate OpenAPI specifications for core services based on existing
        code'
      done: false
      ai_friendly: true
    - task: '[AI] Create stack validation tooling and npm scripts for compliance checking'
      done: false
      ai_friendly: true
    - task: '[AI] Generate integration pattern documentation with code examples'
      done: false
      ai_friendly: true
    - task: '[AI] Update README.md with architecture overview and quick start guide'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review all documentation for accuracy, completeness, and team
        alignment'
      done: false
      ai_friendly: false
- key: T10
  title: shadcn/ui Component Audit & Verification
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nThis task involves auditing the shadcn/ui component\
      \ library integration to ensure proper setup, identify missing components needed\
      \ for the dashboard/storefront UIs, and verify consistent theming/accessibility\
      \ standards. Given that Morpheus has both a dashboard and storefront frontend,\
      \ we need a robust, accessible component foundation that can handle complex\
      \ UI patterns like comic panel layouts, user management interfaces, and e-commerce\
      \ flows. This is critical M0 infrastructure work that prevents technical debt\
      \ and ensures consistent UX.\n\n**Technical Approach:**\n- Use shadcn/ui CLI\
      \ to audit currently installed components vs. available components\n- Create\
      \ component inventory spreadsheet mapping UI requirements to shadcn components\n\
      - Implement consistent theming using CSS variables and Tailwind config\n- Set\
      \ up component testing with Storybook or similar for design system documentation\n\
      - Establish TypeScript strict typing for all component props and variants\n\
      - Configure accessibility testing with axe-core integration\n- Create component\
      \ composition patterns for complex Morpheus-specific UI (comic panels, transformation\
      \ progress, etc.)\n\n**AI Suitability Analysis:**\n- High AI effectiveness:\
      \ Component installation scripts, prop type definitions, basic component tests,\
      \ accessibility attribute additions, CSS variable extraction, Tailwind config\
      \ updates\n- Medium AI effectiveness: Component variant creation, theme customization,\
      \ integration with existing pages, Storybook stories creation\n- Low AI effectiveness:\
      \ Design system architecture decisions, accessibility UX patterns, component\
      \ API design for complex comic/transformation workflows\n\n**Dependencies:**\n\
      - External: @radix-ui/* (shadcn/ui foundation), class-variance-authority, tailwind-merge,\
      \ lucide-react icons, @storybook/nextjs, @axe-core/playwright\n- Internal: Existing\
      \ Next.js apps (dashboard/storefront), shared TypeScript configs, design tokens/brand\
      \ colors\n\n**Risks:**\n- Bundle size bloat: mitigation via tree-shaking audit\
      \ and selective component imports\n- Theme inconsistencies across apps: mitigation\
      \ via shared theme package in monorepo\n- Accessibility regressions: mitigation\
      \ via automated axe testing in CI/CD\n- Component API breaking changes: mitigation\
      \ via version pinning and gradual upgrade strategy\n- Over-engineering: mitigation\
      \ via YAGNI principle - only implement components actually needed for M0\n\n\
      **Complexity Notes:**\nInitially seems straightforward but complexity increases\
      \ with dual-app architecture (dashboard + storefront) requiring shared theming.\
      \ AI can significantly accelerate the mechanical aspects (installation, typing,\
      \ basic tests) but human judgment crucial for component selection and architectural\
      \ decisions. Estimate 40% faster with AI assistance.\n\n**Key Files:**\n- packages/ui/:\
      \ New shared component library package\n- apps/dashboard/tailwind.config.js:\
      \ Theme configuration\n- apps/storefront/tailwind.config.js: Theme configuration\
      \  \n- packages/ui/src/components/: shadcn/ui components\n- packages/ui/src/lib/utils.ts:\
      \ Utility functions\n- .storybook/: Component documentation\n"
    design_decisions:
    - decision: Create shared packages/ui workspace for shadcn/ui components
      rationale: Enables component reuse across dashboard and storefront apps while
        maintaining consistent theming and reducing bundle duplication
      alternatives_considered:
      - Duplicate components in each app
      - Use external component library
      - Build custom component system
      ai_implementation_note: AI can generate package.json, tsconfig, and basic component
        exports with high accuracy
    - decision: Use CSS variables for theming with Tailwind CSS integration
      rationale: Provides runtime theme switching capability and consistent design
        tokens across apps
      alternatives_considered:
      - Static Tailwind themes
      - Styled-components
      - CSS-in-JS solutions
      ai_implementation_note: AI excels at converting design tokens to CSS variables
        and generating Tailwind config mappings
    - decision: Implement component testing with Storybook + Playwright
      rationale: Provides visual component documentation and automated visual regression
        testing
      alternatives_considered:
      - Jest + React Testing Library only
      - Chromatic
      - No visual testing
      ai_implementation_note: AI can generate comprehensive Storybook stories and
        basic interaction tests
    researched_at: '2026-02-08T18:15:49.756466'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T18:58:41.389427'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 655cc389
    planning_hash: 70a962fa
  technical_notes:
    approach: 'Create a shared packages/ui workspace containing audited shadcn/ui
      components with consistent theming via CSS variables. Implement Storybook for
      component documentation and Playwright for visual testing. Use turborepo caching
      to optimize component builds across the dashboard and storefront applications.
      Establish accessibility testing pipeline with axe-core integration.

      '
    external_dependencies:
    - name: '@radix-ui/react-*'
      version: ^1.0.0
      reason: Foundation for shadcn/ui accessible components
    - name: class-variance-authority
      version: ^0.7.0
      reason: Type-safe component variants for shadcn/ui
    - name: '@storybook/nextjs'
      version: ^7.6.0
      reason: Component documentation and visual testing
    - name: '@axe-core/playwright'
      version: ^4.8.0
      reason: Automated accessibility testing
    - name: tailwind-merge
      version: ^2.2.0
      reason: Intelligent Tailwind class merging for component props
    files_to_modify:
    - path: package.json
      changes: Add packages/ui workspace dependency
    - path: apps/dashboard/package.json
      changes: Add @morpheus/ui dependency
    - path: apps/storefront/package.json
      changes: Add @morpheus/ui dependency
    - path: apps/dashboard/tailwind.config.js
      changes: Extend shared theme from packages/ui
    - path: apps/storefront/tailwind.config.js
      changes: Extend shared theme from packages/ui
    - path: turbo.json
      changes: Add build pipeline for packages/ui with proper caching
    new_files:
    - path: packages/ui/package.json
      purpose: UI package configuration with shadcn/ui dependencies
    - path: packages/ui/src/components/ui/button.tsx
      purpose: Button component from shadcn/ui with Morpheus theming
    - path: packages/ui/src/components/ui/card.tsx
      purpose: Card component for dashboard panels and comic displays
    - path: packages/ui/src/components/ui/input.tsx
      purpose: Input component for forms across both apps
    - path: packages/ui/src/components/ui/dialog.tsx
      purpose: Modal dialogs for user interactions
    - path: packages/ui/src/components/ui/table.tsx
      purpose: Data tables for dashboard admin interfaces
    - path: packages/ui/src/components/ui/progress.tsx
      purpose: Progress bars for transformation status
    - path: packages/ui/src/components/ui/badge.tsx
      purpose: Status badges for user roles, comic categories
    - path: packages/ui/src/lib/utils.ts
      purpose: Utility functions for class merging and theme helpers
    - path: packages/ui/tailwind.config.js
      purpose: Base Tailwind configuration with design tokens
    - path: packages/ui/src/styles/globals.css
      purpose: CSS variables and base styles for theming
    - path: packages/ui/.storybook/main.ts
      purpose: Storybook configuration for component documentation
    - path: packages/ui/src/stories/Button.stories.tsx
      purpose: Button component stories and documentation
    - path: packages/ui/tsconfig.json
      purpose: TypeScript configuration for UI package
    - path: packages/ui/src/index.ts
      purpose: Main export file for all UI components
  acceptance_criteria:
  - criterion: Shared UI package with shadcn/ui components properly configured
    verification: Run 'pnpm build' in packages/ui and verify successful build with
      no TypeScript errors
  - criterion: Consistent theming across dashboard and storefront apps
    verification: Visual comparison of identical components in both apps shows matching
      colors, spacing, and typography
  - criterion: Component library documentation accessible via Storybook
    verification: Run 'pnpm storybook' and verify all components render with proper
      controls and documentation
  - criterion: Accessibility standards met with automated testing
    verification: Run 'pnpm test:a11y' and achieve 0 critical accessibility violations
  - criterion: TypeScript strict mode compatibility for all components
    verification: Build both apps with strict TypeScript config and verify no type
      errors
  testing:
    unit_tests:
    - file: packages/ui/src/__tests__/components/button.test.tsx
      coverage_target: 90%
      scenarios:
      - Button variants render correctly
      - Click handlers fire properly
      - Disabled state prevents interaction
      - Loading state shows spinner
    - file: packages/ui/src/__tests__/lib/utils.test.ts
      coverage_target: 95%
      scenarios:
      - cn() utility merges classes correctly
      - Theme utilities return valid CSS variables
      - Variant helpers generate proper class strings
    integration_tests:
    - file: packages/ui/src/__tests__/integration/theme.test.tsx
      scenarios:
      - Components respect theme variables across light/dark modes
      - Custom theme overrides apply correctly
    - file: apps/dashboard/src/__tests__/integration/ui-integration.test.tsx
      scenarios:
      - Dashboard components import and render from packages/ui
    - file: apps/storefront/src/__tests__/integration/ui-integration.test.tsx
      scenarios:
      - Storefront components import and render from packages/ui
    visual_tests:
    - file: packages/ui/src/__tests__/visual/components.spec.ts
      scenarios:
      - Component visual regression tests
      - Cross-browser rendering consistency
    accessibility_tests:
    - file: packages/ui/src/__tests__/a11y/components.test.tsx
      scenarios:
      - All components pass axe-core audits
      - Keyboard navigation works correctly
      - Screen reader labels are present
    manual_testing:
    - step: Navigate to Storybook and interact with all component variants
      expected: No visual bugs, proper state transitions, accessible focus management
    - step: Import and use components in both dashboard and storefront pages
      expected: Components render identically with consistent theming
  estimates:
    development: 2.5
    code_review: 0.5
    testing: 0.8
    documentation: 0.2
    total: 4
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create packages/ui package structure and package.json'
      done: false
      ai_friendly: true
    - task: '[AI] Install shadcn/ui CLI and initialize component library'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Audit existing apps to identify required components'
      done: false
      ai_friendly: false
    - task: '[AI] Install identified shadcn/ui components via CLI'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define Morpheus design tokens and theme architecture'
      done: false
      ai_friendly: false
    - task: '[AI] Implement CSS variables and Tailwind theme configuration'
      done: false
      ai_friendly: true
    - task: '[AI] Configure Storybook with Next.js and create component stories'
      done: false
      ai_friendly: true
    - task: '[AI] Set up component unit tests with React Testing Library'
      done: false
      ai_friendly: true
    - task: '[AI] Configure accessibility testing with axe-core integration'
      done: false
      ai_friendly: true
    - task: '[AI] Update app package.json files to use @morpheus/ui'
      done: false
      ai_friendly: true
    - task: '[AI] Configure turbo.json build pipeline for UI package'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review component API design and accessibility patterns'
      done: false
      ai_friendly: false
    - task: '[AI] Generate TypeScript declarations and build verification'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Manual testing across both apps for theme consistency'
      done: false
      ai_friendly: false
- key: T11
  title: Test Environments Setup (Staging + Production)
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Test environments are critical for validating deployments before production,
      enabling safe CI/CD pipelines, and providing stable environments for QA testing.
      For Morpheus, this means creating isolated staging and production environments
      that mirror each other in configuration but differ in scale and data. This enables
      testing of novel-to-comic transformations, ML model integrations, and payment
      processing without risking production data or user experience.


      **Technical Approach:**

      - Use environment-specific configuration with dotenv-vault or similar for secrets
      management

      - Implement database branching/preview environments with Supabase''s branch
      feature

      - Set up CI/CD pipelines with GitHub Actions for automated deployments

      - Use Vercel for frontend deployments with preview/staging/production environments

      - Configure separate RunPod endpoints and OpenAI/Anthropic API keys with usage
      limits

      - Implement health checks and smoke tests for deployment validation

      - Use feature flags (PostHog/LaunchDarkly) for gradual rollouts


      **AI Suitability Analysis:**

      - High AI effectiveness: Environment configuration files, Docker/deployment
      scripts, health check endpoints, smoke tests, CI/CD YAML configurations

      - Medium AI effectiveness: Database migration scripts, environment-specific
      routing logic, monitoring dashboard setup

      - Low AI effectiveness: Infrastructure architecture decisions, security policy
      configuration, cost optimization strategies, disaster recovery planning


      **Dependencies:**

      - External: Vercel CLI, Supabase CLI, GitHub Actions, dotenv-vault, cross-env

      - Internal: All Morpheus services need environment-aware configuration, database
      schemas must support multi-environment, API endpoints need environment detection


      **Risks:**

      - Data leakage between environments: Use separate databases and API keys with
      strict access controls

      - Configuration drift: Implement infrastructure as code and automated configuration
      validation

      - Cost escalation: Set up billing alerts and resource limits for staging environments

      - ML model inconsistencies: Version and tag models per environment, use consistent
      inference endpoints


      **Complexity Notes:**

      More complex than initially estimated due to ML pipeline requirements. Stable
      Diffusion models and LLM integrations need careful environment isolation. AI
      agents can handle 70% of configuration boilerplate but human oversight needed
      for security and cost controls.


      **Key Files:**

      - packages/backend/src/config/environment.ts: Environment detection and configuration
      loading

      - .github/workflows/: CI/CD pipeline definitions for staging/production

      - apps/dashboard/next.config.js: Environment-specific frontend configuration

      - docker-compose.staging.yml: Staging environment container orchestration

      - packages/shared/src/constants/environments.ts: Shared environment constants

      '
    design_decisions:
    - decision: Use Supabase database branching for environment isolation
      rationale: Provides true database isolation while maintaining schema consistency
        and easy promotion path from staging to production
      alternatives_considered:
      - Single database with tenant separation
      - Completely separate Supabase projects
      ai_implementation_note: AI can generate migration scripts and environment-specific
        seeding data
    - decision: Implement staged deployment pipeline with manual production promotion
      rationale: Ensures human oversight for production deployments while automating
        staging deployments for rapid iteration
      alternatives_considered:
      - Fully automated deployments
      - Manual deployments for all environments
      ai_implementation_note: AI excellent for generating GitHub Actions workflows
        and deployment scripts
    - decision: Use environment-specific ML model endpoints with fallback chains
      rationale: Prevents staging load from affecting production ML performance while
        ensuring model consistency
      alternatives_considered:
      - Shared ML endpoints
      - Completely isolated model instances
      ai_implementation_note: AI can implement endpoint switching logic and health
        check validations
    researched_at: '2026-02-08T18:16:15.848718'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T18:59:08.221896'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 39d1d215
    planning_hash: 7aca8af7
  technical_notes:
    approach: 'Create environment-aware configuration system using TypeScript enums
      and validation schemas. Set up Supabase database branches for staging/production
      isolation with automated migrations. Implement GitHub Actions workflows for
      automated staging deployments with manual production promotion gates. Configure
      Vercel preview/production deployments with environment-specific API endpoints
      and ML model routing.

      '
    external_dependencies:
    - name: dotenv-vault
      version: ^1.25.0
      reason: Encrypted environment variable management across multiple environments
    - name: cross-env
      version: ^7.0.3
      reason: Cross-platform environment variable setting in npm scripts
    - name: '@vercel/node'
      version: ^3.0.0
      reason: Vercel-specific deployment and environment detection utilities
    - name: zod
      version: ^3.22.0
      reason: Runtime environment variable validation and type safety
    files_to_modify:
    - path: packages/backend/src/config/database.ts
      changes: Add environment-specific connection strings and migration paths
    - path: packages/backend/src/config/ml-models.ts
      changes: Add environment-specific RunPod endpoints and model versions
    - path: apps/dashboard/next.config.js
      changes: Add environment-specific API base URLs and feature flag configuration
    - path: packages/backend/src/middleware/auth.ts
      changes: Add environment-aware JWT configuration and CORS settings
    - path: packages/backend/src/services/payment.ts
      changes: Add staging/production Stripe key configuration
    new_files:
    - path: packages/backend/src/config/environment.ts
      purpose: Central environment detection and configuration management with TypeScript
        validation
    - path: packages/shared/src/constants/environments.ts
      purpose: Shared environment constants and feature flag definitions
    - path: .github/workflows/deploy-staging.yml
      purpose: Automated staging deployment on main branch updates
    - path: .github/workflows/deploy-production.yml
      purpose: Manual production deployment with approval gates
    - path: docker-compose.staging.yml
      purpose: Staging environment container orchestration with resource limits
    - path: docker-compose.production.yml
      purpose: Production environment container orchestration with scaling configuration
    - path: packages/backend/src/routes/health.ts
      purpose: Environment-aware health check endpoints for deployment validation
    - path: scripts/validate-environment.ts
      purpose: Environment configuration validation script for CI/CD
    - path: scripts/migrate-staging.ts
      purpose: Automated database migration for staging deployments
    - path: .env.staging.example
      purpose: Template for staging environment variables
    - path: .env.production.example
      purpose: Template for production environment variables
  acceptance_criteria:
  - criterion: Staging and production environments are fully isolated with separate
      databases, API keys, and ML model endpoints
    verification: Run `npm run test:environments` to verify configuration isolation
      and check database connections
  - criterion: Automated CI/CD pipeline deploys to staging on PR merge and production
      on manual approval
    verification: Merge PR to main branch, verify staging deployment succeeds, then
      manually trigger production deployment
  - criterion: Environment-specific configuration loads correctly with proper secrets
      management
    verification: Run `npm run config:validate` in each environment to verify all
      required variables are present and valid
  - criterion: Health checks and smoke tests validate deployments in both environments
    verification: Check `/api/health` endpoint returns environment-specific data and
      all services are operational
  - criterion: Feature flags system enables safe rollouts between environments
    verification: Toggle feature flag in staging, verify isolation from production,
      then promote to production
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/config/environment.test.ts
      coverage_target: 90%
      scenarios:
      - Environment detection from NODE_ENV
      - Configuration validation for each environment
      - Secrets loading and masking
      - Database connection string generation
      - API endpoint routing per environment
    - file: packages/shared/src/__tests__/constants/environments.test.ts
      coverage_target: 85%
      scenarios:
      - Environment constant validation
      - Feature flag configuration per environment
      - ML model endpoint mapping
    integration_tests:
    - file: packages/backend/src/__tests__/integration/deployment.test.ts
      scenarios:
      - End-to-end environment configuration loading
      - Database migration execution in staging
      - ML model endpoint connectivity
      - Feature flag service integration
    - file: .github/workflows/__tests__/deployment.test.ts
      scenarios:
      - CI/CD pipeline configuration validation
      - Environment variable injection
      - Deployment rollback scenarios
    manual_testing:
    - step: Deploy to staging environment and verify comic generation pipeline
      expected: Comics generate successfully with staging ML models and data isolation
    - step: Trigger production deployment and verify zero-downtime deployment
      expected: Production remains available during deployment with health checks
        passing
    - step: Test feature flag toggle between environments
      expected: Feature changes in staging don't affect production until promoted
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create environment configuration types and validation schemas'
      done: false
      ai_friendly: true
    - task: '[AI] Implement environment detection and configuration loading logic'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review security policies and access controls for environment
        isolation'
      done: false
      ai_friendly: false
    - task: '[AI] Set up GitHub Actions workflows for staging and production deployments'
      done: false
      ai_friendly: true
    - task: '[AI] Create Docker compose files for environment-specific orchestration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure Supabase database branches and migration strategies'
      done: false
      ai_friendly: false
    - task: '[AI] Implement health check endpoints and smoke tests'
      done: false
      ai_friendly: true
    - task: '[AI] Create environment validation scripts and documentation'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Set up monitoring, alerting, and cost controls'
      done: false
      ai_friendly: false
    - task: '[AI] Write comprehensive unit and integration tests'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Perform security review and penetration testing setup'
      done: false
      ai_friendly: false
- key: T12
  title: Monitoring & Observability Setup
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Monitoring & observability is critical for a production AI-powered platform
      like Morpheus. With multiple microservices (Fastapi backend, Next.js frontend,
      ML pipelines), external API dependencies (OpenAI, RunPod), and complex async
      workflows (novelâ†’comic transformation), we need comprehensive visibility into
      system health, performance bottlenecks, error tracking, and business metrics.
      This enables proactive issue detection, faster debugging, capacity planning,
      and monitoring of AI model performance/costs.


      **Technical Approach:**

      Implement a modern observability stack using OpenTelemetry for standardized
      instrumentation across all services. Use Grafana Cloud or self-hosted stack
      (Prometheus + Grafana + Loki) for metrics/logs/traces. Sentry for error tracking
      with source maps. Custom dashboards for business metrics (transformation success
      rates, AI model latency/costs, user engagement). Implement structured logging
      with correlation IDs across the request lifecycle. Health check endpoints for
      all services with dependency checks (DB, external APIs).


      **AI Suitability Analysis:**

      - High AI effectiveness: [OpenTelemetry instrumentation boilerplate, health
      check endpoints, log formatting utilities, basic Grafana dashboard configs,
      test fixtures for monitoring endpoints]

      - Medium AI effectiveness: [Custom metrics collection, alert rule configuration,
      integration with existing error boundaries, middleware setup]

      - Low AI effectiveness: [Alert threshold tuning, dashboard design for business
      metrics, observability strategy decisions, SLI/SLO definition]


      **Dependencies:**

      - External: [@opentelemetry/api, @opentelemetry/sdk-node, @sentry/nextjs, @sentry/node,
      pino (logging), prometheus client libraries]

      - Internal: [Database connection pools, external API clients (OpenAI/RunPod),
      authentication middleware, existing error handling]


      **Risks:**

      - Performance overhead: Mitigation via sampling strategies and async metric
      collection

      - Alert fatigue: Start with critical alerts only, tune thresholds based on baseline
      data

      - Data retention costs: Implement proper retention policies and sampling

      - Vendor lock-in: Use OpenTelemetry standard to maintain portability


      **Complexity Notes:**

      More complex than initially estimated due to distributed tracing across ML pipelines
      and external API calls. However, AI assistance will significantly accelerate
      the boilerplate instrumentation code. Focus human effort on defining meaningful
      SLIs/SLOs and business-specific metrics.


      **Key Files:**

      - apps/api/src/plugins/telemetry.ts: OpenTelemetry setup plugin

      - apps/dashboard/src/lib/monitoring.ts: Frontend error tracking

      - packages/shared/src/logger.ts: Structured logging utilities

      - apps/api/src/routes/health.ts: Health check endpoints

      - docker-compose.yml: Add monitoring services for local dev

      '
    design_decisions:
    - decision: Use OpenTelemetry as the instrumentation standard
      rationale: Vendor-neutral, comprehensive tracing/metrics, excellent TypeScript
        support, future-proof as industry standard
      alternatives_considered:
      - Vendor-specific SDKs (DataDog, New Relic)
      - Custom instrumentation
      - Minimal logging-only approach
      ai_implementation_note: AI can generate most OpenTelemetry boilerplate and middleware
        integration code
    - decision: Grafana Cloud for initial observability backend
      rationale: Managed service reduces ops overhead, generous free tier, integrates
        well with OpenTelemetry, can migrate to self-hosted later
      alternatives_considered:
      - Self-hosted Prometheus/Grafana
      - DataDog
      - New Relic
      ai_implementation_note: AI can help with dashboard JSON configs and PromQL queries
    - decision: Sentry for error tracking and performance monitoring
      rationale: Excellent Next.js/Node.js integration, source map support, performance
        insights, reasonable pricing
      alternatives_considered:
      - Bugsnag
      - Rollbar
      - Custom error tracking
      ai_implementation_note: AI can set up Sentry configuration and error boundary
        integration
    researched_at: '2026-02-08T18:16:44.061997'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T18:59:38.251329'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: b1ed9100
    planning_hash: 07d9e515
  technical_notes:
    approach: 'Implement OpenTelemetry instrumentation in Fastify backend and Next.js
      frontend with automatic tracing of HTTP requests, database queries, and external
      API calls. Set up structured logging with pino, ensuring correlation IDs flow
      through the entire request lifecycle. Deploy Grafana Cloud integration with
      custom dashboards for system metrics and AI-specific KPIs (transformation latency,
      model costs, success rates). Configure Sentry for error tracking with proper
      source maps and performance monitoring. Create comprehensive health check endpoints
      that verify all critical dependencies.

      '
    external_dependencies:
    - name: '@opentelemetry/api'
      version: ^1.8.0
      reason: Core OpenTelemetry API for instrumentation
    - name: '@opentelemetry/sdk-node'
      version: ^0.49.0
      reason: Node.js SDK with auto-instrumentation
    - name: '@opentelemetry/exporter-prometheus'
      version: ^0.48.0
      reason: Export metrics to Prometheus/Grafana
    - name: '@sentry/nextjs'
      version: ^7.99.0
      reason: Error tracking and performance monitoring for Next.js
    - name: '@sentry/node'
      version: ^7.99.0
      reason: Error tracking for Node.js backend
    - name: pino
      version: ^8.17.0
      reason: High-performance structured logging
    - name: prom-client
      version: ^15.1.0
      reason: Prometheus metrics collection
    files_to_modify:
    - path: apps/api/src/server.ts
      changes: Add telemetry plugin registration and graceful shutdown hooks
    - path: apps/api/src/routes/novels.ts
      changes: Add custom metrics for processing time and success rates
    - path: apps/dashboard/next.config.js
      changes: Configure Sentry webpack plugin and source maps
    - path: apps/dashboard/src/pages/_app.tsx
      changes: Initialize Sentry and performance monitoring
    - path: docker-compose.yml
      changes: Add Jaeger and Prometheus services for local development
    - path: packages/shared/src/config/index.ts
      changes: Add monitoring configuration with environment variables
    new_files:
    - path: apps/api/src/plugins/telemetry.ts
      purpose: OpenTelemetry configuration and Fastify plugin setup
    - path: apps/api/src/routes/health.ts
      purpose: Health check endpoints with dependency validation
    - path: packages/shared/src/logger.ts
      purpose: Structured logging utilities with correlation ID support
    - path: packages/shared/src/metrics.ts
      purpose: Custom business metrics collection (AI costs, success rates)
    - path: apps/dashboard/src/lib/monitoring.ts
      purpose: Frontend monitoring utilities and error boundaries
    - path: apps/api/src/middleware/correlation.ts
      purpose: Request correlation ID injection and propagation
    - path: packages/shared/src/tracing.ts
      purpose: Distributed tracing utilities and span management
    - path: monitoring/grafana/dashboards/morpheus-overview.json
      purpose: Main system health dashboard configuration
    - path: monitoring/grafana/dashboards/ai-metrics.json
      purpose: AI-specific metrics dashboard (model performance, costs)
    - path: .github/workflows/monitoring-tests.yml
      purpose: CI pipeline for monitoring infrastructure validation
  acceptance_criteria:
  - criterion: All microservices (API, dashboard, ML workers) emit OpenTelemetry traces
      with correlation IDs
    verification: Check Grafana traces view shows end-to-end request flows with consistent
      trace IDs across services
  - criterion: Health check endpoints return detailed status of all dependencies (DB,
      Redis, OpenAI, RunPod APIs)
    verification: GET /health returns 200 with dependency statuses, GET /health/ready
      passes k8s readiness checks
  - criterion: Structured logging captures all HTTP requests, errors, and business
      events with correlation IDs
    verification: Grep logs for correlation IDs, verify JSON format with required
      fields (timestamp, level, service, traceId)
  - criterion: Error tracking captures frontend and backend exceptions with source
      maps and user context
    verification: Trigger test errors, verify they appear in Sentry with proper stack
      traces and user session data
  - criterion: Custom dashboards display AI-specific metrics (transformation latency,
      model costs, success rates)
    verification: Grafana dashboards show real-time metrics for novel processing time,
      OpenAI API costs, RunPod job success rates
  testing:
    unit_tests:
    - file: apps/api/src/plugins/__tests__/telemetry.test.ts
      coverage_target: 90%
      scenarios:
      - OpenTelemetry plugin initialization
      - Trace context propagation
      - Metric collection and export
      - Error handling for telemetry failures
    - file: apps/api/src/routes/__tests__/health.test.ts
      coverage_target: 85%
      scenarios:
      - Health check with all dependencies healthy
      - Health check with database connection failure
      - Health check with external API timeout
      - Ready endpoint validation
    - file: packages/shared/src/__tests__/logger.test.ts
      coverage_target: 90%
      scenarios:
      - Structured log format validation
      - Correlation ID injection
      - Log level filtering
      - Sensitive data redaction
    integration_tests:
    - file: apps/api/src/__tests__/integration/monitoring.test.ts
      scenarios:
      - End-to-end trace generation through novel upload flow
      - Custom metrics emission during AI model calls
      - Error propagation to Sentry with proper context
    - file: apps/dashboard/src/__tests__/integration/error-tracking.test.ts
      scenarios:
      - Frontend error capture with user session
      - Performance monitoring for page loads
      - Source map resolution in production build
    manual_testing:
    - step: Deploy to staging and upload test novel
      expected: Complete trace visible in Grafana from upload to comic generation
    - step: Trigger intentional API error
      expected: Error appears in Sentry within 30 seconds with full context
    - step: Load test dashboard with 100 concurrent users
      expected: Performance metrics captured, no monitoring overhead >5%
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.55
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create OpenTelemetry plugin with auto-instrumentation for HTTP,
        DB, and external APIs'
      done: false
      ai_friendly: true
    - task: '[AI] Implement structured logger with pino and correlation ID utilities'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define SLIs/SLOs and alert thresholds for critical business metrics'
      done: false
      ai_friendly: false
    - task: '[AI] Create comprehensive health check endpoints with dependency validation'
      done: false
      ai_friendly: true
    - task: '[AI] Configure Sentry integration for both frontend and backend with
        source maps'
      done: false
      ai_friendly: true
    - task: '[AI] Implement custom metrics collection for AI model performance and
        costs'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design Grafana dashboards for business metrics and system overview'
      done: false
      ai_friendly: false
    - task: '[AI] Add correlation ID middleware and tracing utilities'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit and integration tests for monitoring components'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Performance testing and monitoring overhead validation'
      done: false
      ai_friendly: false
    - task: '[AI] Generate monitoring documentation and runbooks'
      done: false
      ai_friendly: true
- key: T13
  title: Security & Compliance Infrastructure
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 5
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nSecurity & Compliance Infrastructure is essential\
      \ for handling user data, payment processing, and AI-generated content in Morpheus.\
      \ As a platform processing creative content, user uploads, and financial transactions,\
      \ we need comprehensive security measures including authentication, authorization,\
      \ data encryption, audit logging, rate limiting, and compliance with regulations\
      \ like GDPR. This is foundational infrastructure that must be implemented before\
      \ user-facing features.\n\n**Technical Approach:**\nImplement a multi-layered\
      \ security architecture:\n- Authentication: Supabase Auth with JWT tokens, social\
      \ logins, MFA\n- Authorization: RBAC with policies (admin, creator, subscriber\
      \ roles)  \n- API Security: Rate limiting, request validation, CORS, helmet.js\n\
      - Data Protection: Field-level encryption for PII, secure file uploads\n- Audit\
      \ Logging: Comprehensive activity tracking with structured logs\n- Compliance:\
      \ GDPR data handling, retention policies, consent management\n- Infrastructure:\
      \ Security headers, CSP, secret management, vulnerability scanning\n\n**AI Suitability\
      \ Analysis:**\n- High AI effectiveness: Middleware boilerplate, validation schemas,\
      \ test cases, RBAC policy definitions, security header configurations\n- Medium\
      \ AI effectiveness: Audit logging implementation, encryption utilities, compliance\
      \ data structures\n- Low AI effectiveness: Security policy decisions, threat\
      \ modeling, compliance requirements analysis, encryption key management strategy\n\
      \n**Dependencies:**\n- External: @supabase/supabase-js, @fastify/helmet, @fastify/rate-limit,\
      \ zod, bcrypt, jsonwebtoken, winston, crypto\n- Internal: Database schemas,\
      \ authentication middleware, user service integration\n\n**Risks:**\n- Over-engineering\
      \ security: Start with essential features, iterate based on threat assessment\n\
      - Performance impact: Implement efficient caching for auth checks, optimize\
      \ database queries\n- Compliance gaps: Regular security audits, legal review\
      \ of data handling practices\n- Secret exposure: Use proper environment variable\
      \ management, rotate keys regularly\n\n**Complexity Notes:**\nMore complex than\
      \ initially estimated due to multi-service architecture (3 apps + external APIs).\
      \ However, AI can significantly accelerate implementation of repetitive security\
      \ patterns, middleware, and test coverage. Supabase handles much of the auth\
      \ complexity, reducing custom implementation needs.\n\n**Key Files:**\n- packages/backend/src/middleware/auth.ts:\
      \ JWT validation, role checking\n- packages/backend/src/middleware/security.ts:\
      \ Rate limiting, headers, validation\n- packages/backend/src/services/audit.ts:\
      \ Activity logging service\n- packages/backend/src/utils/encryption.ts: Data\
      \ encryption utilities\n- packages/shared/src/types/auth.ts: Shared authentication\
      \ types\n- apps/dashboard/src/lib/auth.ts: Frontend auth integration\n- apps/storefront/src/middleware.ts:\
      \ Next.js middleware for protected routes\n"
    design_decisions:
    - decision: Use Supabase Auth as primary authentication provider
      rationale: Leverages existing database choice, provides JWT tokens, social auth,
        and MFA out of box
      alternatives_considered:
      - Auth0
      - Custom JWT implementation
      - NextAuth.js
      ai_implementation_note: AI can generate Supabase client configurations, auth
        middleware, and integration tests
    - decision: 'Implement RBAC with three primary roles: admin, creator, subscriber'
      rationale: Covers core business model with room for expansion, maps to user
        journey and payment tiers
      alternatives_considered:
      - ABAC (Attribute-Based)
      - Simple user/admin roles
      ai_implementation_note: AI excellent for generating role-based middleware, permission
        checking functions, and policy tests
    - decision: Use Winston for structured audit logging with JSON format
      rationale: Production-ready logging with proper log levels, structured data
        for compliance audits
      alternatives_considered:
      - Pino
      - Built-in console logging
      - External service like LogRocket
      ai_implementation_note: AI can generate comprehensive logging middleware, log
        schemas, and formatting utilities
    researched_at: '2026-02-08T18:17:10.249618'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:00:08.700531'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 42c4445f
    planning_hash: f3911c9d
  technical_notes:
    approach: 'Implement security as middleware-first architecture in Fastify backend
      with corresponding client-side protection in Next.js apps. Start with Supabase
      Auth integration, then layer on RBAC, rate limiting, and audit logging. Use
      shared TypeScript types across monorepo for consistency. Implement field-level
      encryption for sensitive data and comprehensive request validation using Zod
      schemas.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.38.0
      reason: Authentication provider and database client
    - name: '@fastify/helmet'
      version: ^11.1.1
      reason: Security headers middleware for Fastify
    - name: '@fastify/rate-limit'
      version: ^9.1.0
      reason: API rate limiting to prevent abuse
    - name: zod
      version: ^3.22.4
      reason: Runtime type validation for API requests
    - name: winston
      version: ^3.11.0
      reason: Structured logging for audit trails
    - name: bcrypt
      version: ^5.1.1
      reason: Password hashing for any custom auth needs
    - name: jsonwebtoken
      version: ^9.0.2
      reason: JWT token validation utilities
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register auth, security, and audit middleware globally
    - path: packages/backend/src/routes/auth.ts
      changes: Add Supabase integration endpoints for login/logout/refresh
    - path: packages/shared/src/types/index.ts
      changes: Add UserRole enum and AuthContext interface
    - path: apps/dashboard/src/app/layout.tsx
      changes: Add AuthProvider wrapper for session management
    - path: apps/storefront/src/middleware.ts
      changes: Integrate JWT validation for protected storefront routes
    new_files:
    - path: packages/backend/src/middleware/auth.ts
      purpose: JWT validation, user context extraction, role-based route protection
    - path: packages/backend/src/middleware/security.ts
      purpose: Rate limiting, security headers, request validation, CORS handling
    - path: packages/backend/src/services/audit.ts
      purpose: Structured logging service for compliance and security monitoring
    - path: packages/backend/src/utils/encryption.ts
      purpose: Field-level encryption utilities for PII data protection
    - path: packages/backend/src/config/security.ts
      purpose: Security configuration constants and policy definitions
    - path: packages/shared/src/types/auth.ts
      purpose: Shared TypeScript interfaces for authentication across monorepo
    - path: packages/shared/src/schemas/validation.ts
      purpose: Zod schemas for request validation and type safety
    - path: apps/dashboard/src/lib/auth.ts
      purpose: Client-side auth utilities and session management
    - path: apps/dashboard/src/components/ProtectedRoute.tsx
      purpose: Role-based route protection component for dashboard
  acceptance_criteria:
  - criterion: Authentication middleware validates JWT tokens and extracts user roles
      for all protected routes
    verification: Run integration tests in apps/backend/src/__tests__/integration/auth.test.ts
      and verify 401/403 responses for invalid/missing tokens
  - criterion: Rate limiting prevents abuse with configurable limits per endpoint
      and user role
    verification: Execute load tests showing 429 responses after rate limits exceeded,
      verify different limits for authenticated vs anonymous users
  - criterion: Audit logging captures all critical user actions with structured JSON
      format
    verification: Check logs/audit.log contains user authentication, content creation,
      payment events with userId, action, timestamp, metadata fields
  - criterion: Field-level encryption protects PII data in database with proper key
      rotation support
    verification: Query database directly to verify encrypted fields are not readable,
      test decryption through API endpoints
  - criterion: RBAC system enforces creator/subscriber/admin permissions across all
      services
    verification: Manual testing of dashboard and storefront with different user roles,
      verify admin-only routes reject non-admin users
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/middleware/auth.test.ts
      coverage_target: 90%
      scenarios:
      - Valid JWT token extraction and role parsing
      - Expired token rejection with 401 response
      - Missing token handling for protected routes
      - Role-based access control validation
    - file: packages/backend/src/__tests__/middleware/security.test.ts
      coverage_target: 85%
      scenarios:
      - Rate limiting by IP and user ID
      - Security headers injection (CSP, HSTS, etc)
      - Request validation with Zod schemas
      - CORS configuration for different origins
    - file: packages/backend/src/__tests__/services/audit.test.ts
      coverage_target: 80%
      scenarios:
      - Structured log entry creation
      - Sensitive data masking in logs
      - Log rotation and retention policies
    - file: packages/backend/src/__tests__/utils/encryption.test.ts
      coverage_target: 95%
      scenarios:
      - Field-level encryption/decryption
      - Key rotation without data loss
      - Encryption performance with large payloads
    integration_tests:
    - file: packages/backend/src/__tests__/integration/auth-flow.test.ts
      scenarios:
      - End-to-end user login with Supabase Auth
      - Protected route access with different user roles
      - Token refresh flow and session management
    - file: packages/backend/src/__tests__/integration/security-headers.test.ts
      scenarios:
      - Security headers present in all responses
      - Rate limiting across multiple requests
      - CORS preflight handling for dashboard/storefront
    manual_testing:
    - step: Login to dashboard as admin user and access admin-only routes
      expected: Full access granted, audit logs created for admin actions
    - step: Attempt API requests exceeding rate limits from same IP
      expected: 429 responses after configured threshold, proper retry headers
    - step: Test file upload with malicious content types
      expected: Rejected uploads with appropriate error messages, no server errors
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup Supabase client configuration and environment variables'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define security policies, rate limits, and compliance requirements'
      done: false
      ai_friendly: false
    - task: '[AI] Implement JWT authentication middleware with role extraction'
      done: false
      ai_friendly: true
    - task: '[AI] Create security middleware with rate limiting and headers'
      done: false
      ai_friendly: true
    - task: '[AI] Build audit logging service with structured JSON output'
      done: false
      ai_friendly: true
    - task: '[AI] Implement field-level encryption utilities and key management'
      done: false
      ai_friendly: true
    - task: '[AI] Create Zod validation schemas for all API endpoints'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit and integration test suites'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Security review of encryption implementation and key storage'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Compliance audit of data handling and retention policies'
      done: false
      ai_friendly: false
- key: T14
  title: API Standards & Best Practices
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 2
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task establishes consistent API standards across Morpheus services to ensure
      maintainable, secure, and scalable APIs. With multiple services handling novel
      uploads, comic generation, payment processing, and user management, standardized
      patterns prevent technical debt and improve developer experience. This is foundational
      for M0 since all subsequent API development will follow these patterns.


      **Technical Approach:**

      - OpenAPI 3.1 specification with Swagger UI integration for documentation

      - Fastify schema validation using JSON Schema for request/response validation

      - Standardized error handling with RFC 7807 Problem Details format

      - Consistent REST conventions (resource naming, HTTP methods, status codes)

      - Rate limiting with @fastify/rate-limit for API protection

      - Request/response logging with structured JSON formatting

      - Authentication/authorization patterns using JWT tokens

      - API versioning strategy using URL path versioning (/v1/, /v2/)

      - Response pagination standards for list endpoints

      - Health check endpoints following RFC draft-inadarei-api-health-check


      **AI Suitability Analysis:**

      - High AI effectiveness: OpenAPI schema generation, error handler boilerplate,
      validation schemas, test cases for standard CRUD operations, middleware setup
      code

      - Medium AI effectiveness: Custom error types, authentication middleware integration,
      logging configuration, rate limiting rules

      - Low AI effectiveness: API versioning strategy decisions, business-specific
      error handling logic, security policy definitions, performance optimization
      patterns


      **Dependencies:**

      - External: @fastify/swagger, @fastify/swagger-ui, @fastify/rate-limit, @fastify/helmet,
      @fastify/cors, ajv for JSON schema validation, pino for structured logging

      - Internal: Supabase client configuration, authentication service integration,
      existing database models


      **Risks:**

      - Over-standardization: Could slow development if patterns are too rigid. Mitigation:
      Start with core patterns, iterate based on team feedback

      - Schema validation performance: Complex schemas can impact response times.
      Mitigation: Profile validation performance, use schema compilation

      - Breaking changes: API changes could break frontend clients. Mitigation: Implement
      proper versioning strategy and deprecation notices


      **Complexity Notes:**

      More complex than initially estimated due to need for comprehensive error handling
      and security considerations. AI can significantly accelerate boilerplate generation
      and test creation, potentially improving velocity by 40-50% for implementation
      phases.


      **Key Files:**

      - apps/backend/src/plugins/swagger.ts: OpenAPI configuration and documentation
      setup

      - apps/backend/src/plugins/validation.ts: Global schema validation configuration

      - apps/backend/src/lib/errors.ts: Standardized error classes and handlers

      - apps/backend/src/lib/responses.ts: Response formatting utilities

      - apps/backend/src/plugins/security.ts: Rate limiting and security headers

      - apps/backend/src/types/api.ts: Common API type definitions

      - apps/backend/src/routes/_schema/: Shared schema definitions

      '
    design_decisions:
    - decision: Use Fastify's built-in schema validation with JSON Schema instead
        of external validation library like Zod
      rationale: Leverages Fastify's performance optimizations and automatic OpenAPI
        generation. Reduces bundle size and maintains consistency with framework patterns.
      alternatives_considered:
      - Zod with custom integration
      - Joi validation
      - Class-validator with decorators
      ai_implementation_note: AI can generate comprehensive JSON schemas from TypeScript
        interfaces and create corresponding validation tests
    - decision: Implement RFC 7807 Problem Details for error responses
      rationale: Provides standardized, machine-readable error format that works well
        with frontend error handling and debugging
      alternatives_considered:
      - Custom error format
      - Simple string messages
      - GraphQL-style errors
      ai_implementation_note: AI can generate error classes and corresponding test
        cases following RFC 7807 structure
    - decision: Use URL path versioning (/api/v1/) rather than header-based versioning
      rationale: More explicit for debugging, easier to document, better caching behavior,
        and simpler for frontend developers
      alternatives_considered:
      - Header versioning
      - Query parameter versioning
      - Media type versioning
      ai_implementation_note: AI can generate route handlers with proper versioning
        structure and migration utilities
    researched_at: '2026-02-08T18:17:39.975132'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:00:35.742752'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 7a8b5a76
    planning_hash: ebf5fe36
  technical_notes:
    approach: 'Establish Fastify plugins for cross-cutting concerns (validation, errors,
      security) that can be registered across all route modules. Create shared schema
      definitions and response formatters that enforce consistent API contracts. Implement
      comprehensive OpenAPI documentation generation that stays synchronized with
      code through schema-first development. Set up automated API testing that validates
      both happy path and error scenarios against the established standards.

      '
    external_dependencies:
    - name: '@fastify/swagger'
      version: ^8.14.0
      reason: Auto-generates OpenAPI specifications from Fastify schemas
    - name: '@fastify/swagger-ui'
      version: ^1.10.0
      reason: Provides interactive API documentation interface
    - name: '@fastify/rate-limit'
      version: ^9.1.0
      reason: API rate limiting and abuse protection
    - name: '@fastify/helmet'
      version: ^11.1.1
      reason: Security headers and OWASP protection
    - name: '@fastify/cors'
      version: ^9.0.1
      reason: Cross-origin request handling for frontend integration
    - name: ajv-formats
      version: ^2.1.1
      reason: Additional JSON Schema format validators (email, date, etc.)
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register new plugins (swagger, validation, security) and global error
        handler
    - path: apps/backend/package.json
      changes: 'Add dependencies: @fastify/swagger, @fastify/swagger-ui, @fastify/rate-limit,
        @fastify/helmet, @fastify/cors'
    new_files:
    - path: apps/backend/src/plugins/swagger.ts
      purpose: OpenAPI 3.1 configuration, Swagger UI setup, documentation generation
    - path: apps/backend/src/plugins/validation.ts
      purpose: Global JSON Schema validation setup, custom validators, error formatting
    - path: apps/backend/src/plugins/security.ts
      purpose: Rate limiting, CORS, security headers, helmet configuration
    - path: apps/backend/src/lib/errors.ts
      purpose: Custom error classes, RFC 7807 formatting, global error handler
    - path: apps/backend/src/lib/responses.ts
      purpose: Standardized response formatting, pagination helpers, success wrappers
    - path: apps/backend/src/types/api.ts
      purpose: Common API type definitions, response interfaces, error types
    - path: apps/backend/src/routes/_schemas/common.ts
      purpose: Reusable JSON Schema definitions, validation patterns
    - path: apps/backend/src/routes/_schemas/errors.ts
      purpose: Error response schemas for OpenAPI documentation
    - path: apps/backend/src/middleware/versioning.ts
      purpose: API version detection, routing, deprecation headers
    - path: apps/backend/src/routes/v1/health.ts
      purpose: Health check endpoint implementing RFC draft-inadarei-api-health-check
  acceptance_criteria:
  - criterion: All API endpoints include OpenAPI 3.1 documentation with request/response
      schemas
    verification: Access /api/documentation in browser, verify all endpoints show
      proper schema validation examples
  - criterion: Standardized error responses follow RFC 7807 Problem Details format
      with consistent structure
    verification: curl -X POST /api/v1/test-endpoint with invalid data, verify response
      contains type, title, detail, status fields
  - criterion: Request validation automatically rejects malformed payloads with descriptive
      error messages
    verification: Send requests with missing required fields, invalid types, extra
      properties - verify 400 responses with field-specific errors
  - criterion: Rate limiting protects all endpoints with configurable limits per endpoint
      type
    verification: Make 100+ requests to /api/v1/health within 1 minute, verify 429
      Too Many Requests after configured threshold
  - criterion: API versioning supports multiple versions with proper deprecation headers
    verification: Access /api/v1/health and /api/v2/health, verify v1 includes Deprecation
      and Sunset headers
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/plugins/swagger.test.ts
      coverage_target: 90%
      scenarios:
      - OpenAPI spec generation with valid schemas
      - Swagger UI accessibility
      - Documentation route registration
    - file: apps/backend/src/__tests__/plugins/validation.test.ts
      coverage_target: 95%
      scenarios:
      - Schema validation success cases
      - Validation error formatting
      - Custom validation rules
    - file: apps/backend/src/__tests__/lib/errors.test.ts
      coverage_target: 100%
      scenarios:
      - Error class instantiation
      - RFC 7807 format compliance
      - Error serialization
      - Stack trace handling
    - file: apps/backend/src/__tests__/lib/responses.test.ts
      coverage_target: 90%
      scenarios:
      - Success response formatting
      - Pagination metadata generation
      - Response header injection
    integration_tests:
    - file: apps/backend/src/__tests__/integration/api-standards.test.ts
      scenarios:
      - End-to-end request validation flow
      - Error handling across route handlers
      - Rate limiting behavior
      - CORS and security headers
      - API versioning routing
    manual_testing:
    - step: Open /api/documentation and test interactive examples
      expected: All endpoints executable with proper request/response examples
    - step: Attempt API calls with malformed authentication headers
      expected: Consistent 401 responses with proper Problem Details format
    - step: Verify rate limiting with burst requests using Postman/curl
      expected: Progressive rate limiting with proper headers (X-RateLimit-*)
  estimates:
    development: 2.5
    code_review: 0.5
    testing: 0.8
    documentation: 0.3
    total: 4.1
    ai_acceleration_factor: 0.55
  progress:
    status: not-started
    checklist:
    - task: '[AI] Install and configure required Fastify plugins (@fastify/swagger,
        @fastify/rate-limit, etc.)'
      done: false
      ai_friendly: true
    - task: '[AI] Create OpenAPI plugin with schema generation and Swagger UI integration'
      done: false
      ai_friendly: true
    - task: '[AI] Implement JSON Schema validation plugin with custom error formatting'
      done: false
      ai_friendly: true
    - task: '[AI] Build standardized error classes following RFC 7807 Problem Details
        format'
      done: false
      ai_friendly: true
    - task: '[AI] Create response formatting utilities with pagination support'
      done: false
      ai_friendly: true
    - task: '[AI] Set up rate limiting and security headers plugin configuration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define API versioning strategy and deprecation policy'
      done: false
      ai_friendly: false
    - task: '[AI] Implement health check endpoint following RFC standards'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit tests for all utility functions'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests validating end-to-end API behavior'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review security configurations and rate limiting policies'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Validate OpenAPI documentation accuracy and completeness'
      done: false
      ai_friendly: false
- key: T15
  title: Documentation Standards & Templates
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nDocumentation standards and templates are critical\
      \ foundation work for the Morpheus project. This establishes consistent documentation\
      \ patterns across the entire codebase, making the project maintainable for future\
      \ developers and maximizing the effectiveness of AI-assisted development. Good\
      \ documentation templates enable both human developers and AI tools (like GitHub\
      \ Copilot) to understand context, generate consistent code, and maintain architectural\
      \ decisions over time. For a novel-to-comic platform with complex ML pipelines,\
      \ API integrations, and multiple frontend surfaces, standardized documentation\
      \ prevents knowledge silos and reduces onboarding friction.\n\n**Technical Approach:**\n\
      Implement a comprehensive documentation system using:\n- **README templates**\
      \ for packages/services with standardized sections (Purpose, Architecture, API,\
      \ Setup)\n- **ADR (Architecture Decision Records)** using markdown templates\
      \ in `/docs/decisions/`\n- **API documentation** with OpenAPI 3.1 specs auto-generated\
      \ from Fastify schemas\n- **Code documentation** using TSDoc comments with consistent\
      \ patterns\n- **Runbook templates** for operational procedures (deployment,\
      \ troubleshooting)\n- **Contributing guidelines** with coding standards, PR\
      \ templates, testing requirements\n- Documentation linting with markdownlint\
      \ and link checkers in CI/CD pipeline\n\n**AI Suitability Analysis:**\n- High\
      \ AI effectiveness: \n  - Generating boilerplate README content from existing\
      \ code structure\n  - Creating TSDoc comments for functions/classes based on\
      \ implementation\n  - Writing API documentation from OpenAPI schemas\n  - Generating\
      \ changelog entries from git commits\n- Medium AI effectiveness:\n  - Creating\
      \ ADR templates and initial decision records\n  - Writing troubleshooting guides\
      \ based on common error patterns\n  - Generating migration guides between versions\n\
      - Low AI effectiveness:\n  - Defining documentation strategy and information\
      \ architecture\n  - Establishing what documentation standards are needed\n \
      \ - Creating project-specific style guides and governance policies\n\n**Dependencies:**\n\
      - External: @apidevtools/swagger-jsdoc, swagger-ui-express, markdownlint-cli,\
      \ markdown-link-check\n- Internal: Existing Turborepo structure, Fastify API\
      \ routes, component libraries\n\n**Risks:**\n- Documentation debt: Templates\
      \ without enforcement become stale quickly\n- Over-documentation: Too much process\
      \ slows development velocity\n- Inconsistent adoption: Different teams/services\
      \ ignore standards\n- Maintenance overhead: Documentation gets out of sync with\
      \ code\n\n**Complexity Notes:**\nInitially appears simple but requires careful\
      \ balance between comprehensiveness and practicality. AI assistance significantly\
      \ reduces implementation time for template creation and initial documentation\
      \ generation, but human judgment crucial for determining what documentation\
      \ actually provides value. Complexity increases when integrating with existing\
      \ toolchain (Turborepo, Supabase, testing frameworks).\n\n**Key Files:**\n-\
      \ `/docs/templates/`: All documentation templates\n- `/docs/standards.md`: Documentation\
      \ standards and guidelines\n- `/.github/`: PR templates, issue templates, contributing\
      \ guidelines\n- `/packages/*/README.md`: Package-level documentation following\
      \ templates\n- `/apps/*/docs/`: Application-specific documentation\n- `turbo.json`:\
      \ Add documentation build/lint tasks\n"
    design_decisions:
    - decision: Use Architecture Decision Records (ADRs) for major technical decisions
      rationale: ADRs provide historical context for decisions, crucial for AI-assisted
        development and team knowledge sharing
      alternatives_considered:
      - Wiki-based documentation
      - Inline code comments only
      - Confluence/Notion external docs
      ai_implementation_note: AI can generate ADR templates and help populate initial
        content based on existing code patterns
    - decision: Integrate OpenAPI documentation generation directly into Fastify routes
      rationale: Keeps API documentation in sync with implementation, reduces maintenance
        burden
      alternatives_considered:
      - Separate OpenAPI spec files
      - Postman collections
      - Manual API documentation
      ai_implementation_note: AI can generate OpenAPI schemas from existing route
        handlers and validate consistency
    - decision: Use markdownlint and automated link checking in CI/CD
      rationale: Enforces documentation quality without manual review overhead, prevents
        broken links
      alternatives_considered:
      - Manual documentation review
      - No linting
      - Custom documentation validation
      ai_implementation_note: AI can fix most markdownlint issues automatically and
        suggest improvements
    researched_at: '2026-02-08T18:18:07.749200'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:01:01.645607'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 2525cb57
    planning_hash: 8e1707af
  technical_notes:
    approach: 'Create a hierarchical documentation system starting with repository-level
      standards, then cascading to package and application-specific templates. Integrate
      documentation generation into the existing Turborepo build pipeline using custom
      scripts that leverage AI for content generation. Establish documentation validation
      in GitHub Actions to ensure consistency and quality. Use Fastify''s schema-first
      approach to auto-generate API documentation that stays in sync with implementation.

      '
    external_dependencies:
    - name: '@apidevtools/swagger-jsdoc'
      version: ^3.0.0
      reason: Generate OpenAPI specs from JSDoc comments in Fastify routes
    - name: swagger-ui-express
      version: ^5.0.0
      reason: Serve interactive API documentation in development
    - name: markdownlint-cli
      version: ^0.37.0
      reason: Lint markdown files for consistency and quality
    - name: markdown-link-check
      version: ^3.11.0
      reason: Validate internal and external links in documentation
    - name: typedoc
      version: ^0.25.0
      reason: Generate API documentation from TypeScript code comments
    files_to_modify:
    - path: turbo.json
      changes: Add 'docs:build', 'docs:lint', 'docs:generate' pipeline tasks
    - path: apps/backend/src/server.ts
      changes: Register Swagger plugin with OpenAPI configuration
    - path: package.json
      changes: Add markdownlint-cli, swagger-jsdoc, swagger-ui-express dependencies
    - path: .github/workflows/ci.yml
      changes: Add documentation linting step after code linting
    new_files:
    - path: docs/templates/README-package.md
      purpose: Standard README template for packages with placeholders
    - path: docs/templates/README-app.md
      purpose: Standard README template for applications
    - path: docs/templates/ADR-template.md
      purpose: Architecture Decision Record template with MADR format
    - path: docs/decisions/0001-documentation-standards.md
      purpose: First ADR documenting this documentation decision
    - path: docs/decisions/0002-api-documentation-strategy.md
      purpose: ADR for OpenAPI/Swagger documentation approach
    - path: docs/decisions/0003-monorepo-documentation-structure.md
      purpose: ADR for how documentation is organized in Turborepo
    - path: docs/standards.md
      purpose: Master documentation standards and style guide
    - path: CONTRIBUTING.md
      purpose: Project contribution guidelines and coding standards
    - path: .github/pull_request_template.md
      purpose: Standard PR template with documentation checklist
    - path: .github/ISSUE_TEMPLATE/feature.md
      purpose: Feature request template
    - path: .github/ISSUE_TEMPLATE/bug.md
      purpose: Bug report template
    - path: scripts/generate-docs.ts
      purpose: Script to generate README files from templates
    - path: scripts/validate-docs.ts
      purpose: Script to validate documentation completeness
    - path: .markdownlint.json
      purpose: Markdown linting configuration
    - path: apps/backend/src/plugins/swagger.ts
      purpose: Fastify Swagger plugin configuration
  acceptance_criteria:
  - criterion: All repository packages and apps have README files following standard
      template
    verification: Run `find . -name 'README.md' | grep -E '(packages|apps)' | xargs
      grep -l 'Purpose\|Architecture\|API\|Setup'` returns all package/app READMEs
  - criterion: ADR system is established with at least 3 initial decision records
    verification: Check `/docs/decisions/` contains ADR template and 3+ numbered ADR
      files with consistent format
  - criterion: API documentation auto-generates from Fastify schemas
    verification: Navigate to `/api/docs` endpoint and verify OpenAPI UI displays
      all routes with schemas
  - criterion: Documentation linting passes in CI pipeline
    verification: GitHub Actions workflow runs markdownlint and link-checker successfully
      on all .md files
  - criterion: Contributing guidelines and PR templates are active
    verification: Create test PR and verify template loads; check CONTRIBUTING.md
      exists with coding standards
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/docs/swagger-generator.test.ts
      coverage_target: 90%
      scenarios:
      - OpenAPI schema generation from Fastify routes
      - API documentation endpoint availability
      - Schema validation for documented endpoints
    integration_tests:
    - file: scripts/__tests__/doc-generator.test.ts
      scenarios:
      - README template generation from package.json
      - TSDoc comment insertion into existing files
      - Documentation build pipeline execution
    manual_testing:
    - step: Run `turbo run docs:build` across all packages
      expected: All packages generate documentation without errors
    - step: Create new package using template generator
      expected: New package includes proper README with all template sections
    - step: View generated API docs at localhost:3001/api/docs
      expected: Swagger UI loads with all backend routes documented
  estimates:
    development: 2.5
    code_review: 1
    testing: 0.8
    documentation: 0.3
    total: 4.6
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[HUMAN] Define documentation strategy and required templates'
      done: false
      ai_friendly: false
    - task: '[AI] Create markdown template files with standard sections'
      done: false
      ai_friendly: true
    - task: '[AI] Generate initial ADR files using established template'
      done: false
      ai_friendly: true
    - task: '[AI] Implement Fastify Swagger plugin configuration'
      done: false
      ai_friendly: true
    - task: '[AI] Create documentation generation scripts'
      done: false
      ai_friendly: true
    - task: '[AI] Add TSDoc comments to existing service files'
      done: false
      ai_friendly: true
    - task: '[AI] Generate README files for all packages using templates'
      done: false
      ai_friendly: true
    - task: '[AI] Setup markdown linting configuration and CI integration'
      done: false
      ai_friendly: true
    - task: '[AI] Write unit tests for documentation generation utilities'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review generated documentation for accuracy and completeness'
      done: false
      ai_friendly: false
- key: T16
  title: Deployment Strategy & Procedures
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nDeployment Strategy & Procedures is foundational\
      \ for any production system. For Morpheus, this involves defining how to deploy\
      \ a multi-service platform (Fastify backend, Next.js frontend apps, ML services)\
      \ across environments while ensuring zero-downtime deployments, proper rollback\
      \ procedures, and observability. This is critical for M0 as it establishes the\
      \ foundation for all future releases and enables the team to iterate quickly\
      \ with confidence.\n\n**Technical Approach:**\n- **Infrastructure as Code**:\
      \ Terraform/CDK for AWS/Vercel resources\n- **Container Strategy**: Docker multi-stage\
      \ builds for backend, Next.js standalone output for frontend\n- **Deployment\
      \ Platforms**: Vercel for Next.js apps (dashboard/storefront), Railway/Render/AWS\
      \ ECS for Fastify backend\n- **Database Migrations**: Supabase CLI with migration\
      \ scripts, integrated into CI/CD\n- **Environment Management**: Environment-specific\
      \ configs, secret management via platform-native solutions\n- **Monitoring**:\
      \ Application-level logging, health checks, performance monitoring\n- **CI/CD\
      \ Pipeline**: GitHub Actions with Turborepo cache optimization, parallel deployment\
      \ of services\n\n**AI Suitability Analysis:**\n- High AI effectiveness: Docker\
      \ configurations, GitHub Actions workflows, environment variable templates,\
      \ health check endpoints, deployment scripts, monitoring dashboards setup\n\
      - Medium AI effectiveness: Infrastructure as Code templates, database migration\
      \ scripts, rollback procedures, integration testing for deployment pipeline\n\
      - Low AI effectiveness: Architecture decisions for deployment topology, security\
      \ policies, disaster recovery strategies, cost optimization strategies\n\n**Dependencies:**\n\
      - External: Docker, GitHub Actions, Vercel CLI, Supabase CLI, monitoring services\
      \ (DataDog/New Relic)\n- Internal: All Turborepo packages, database schema,\
      \ environment configuration system\n\n**Risks:**\n- **Deployment Complexity**:\
      \ Multi-service coordination could cause partial failures\n  - Mitigation: Health\
      \ checks, staged deployments, automated rollback triggers\n- **Environment Drift**:\
      \ Development/production inconsistencies\n  - Mitigation: Infrastructure as\
      \ Code, containerization, environment validation scripts\n- **Secret Management**:\
      \ Hardcoded secrets or insecure storage\n  - Mitigation: Platform-native secret\
      \ management, secret scanning in CI\n- **Cost Overrun**: Over-provisioned resources\
      \ or expensive monitoring\n  - Mitigation: Start with cheaper platforms (Railway/Render),\
      \ optimize based on usage\n\n**Complexity Notes:**\nInitially seems straightforward,\
      \ but Morpheus's multi-service architecture (2 frontends + backend + ML services\
      \ + database) adds significant complexity. AI can heavily accelerate the scripting/configuration\
      \ work (70% of the task), but architectural decisions around service orchestration\
      \ and deployment topology require human judgment. Turborepo's task caching can\
      \ significantly speed up deployments.\n\n**Key Files:**\n- `.github/workflows/`:\
      \ CI/CD pipeline definitions\n- `apps/*/Dockerfile`: Container configurations\
      \ for each service\n- `infrastructure/`: Terraform/CDK infrastructure definitions\n\
      - `scripts/deploy.sh`: Deployment orchestration scripts\n- `turbo.json`: Build/deploy\
      \ task definitions\n- `apps/*/next.config.js`: Next.js deployment configurations\n\
      - `apps/backend/src/health.ts`: Health check endpoints\n"
    design_decisions:
    - decision: Platform-native deployment (Vercel + Railway/Render)
      rationale: Reduces operational overhead while maintaining good performance and
        DX. Vercel excels at Next.js, Railway/Render provide excellent Fastify hosting
        with database connectivity.
      alternatives_considered:
      - Full AWS with ECS/Fargate
      - Self-managed Kubernetes
      - Fly.io full stack
      ai_implementation_note: AI can generate deployment configurations, environment
        variable mappings, and health check implementations for each platform
    - decision: Database-first deployment strategy
      rationale: Run migrations first, then deploy backend, then frontends. Ensures
        data consistency and allows for backward-compatible schema changes.
      alternatives_considered:
      - Parallel deployment
      - Frontend-first deployment
      ai_implementation_note: AI can create migration validation scripts and rollback
        procedures
    - decision: Turborepo-optimized CI/CD with selective deployment
      rationale: Only deploy services that have changed, leveraging Turborepo's dependency
        graph and caching for faster deployments.
      alternatives_considered:
      - Deploy all services always
      - Manual deployment selection
      ai_implementation_note: AI can generate GitHub Actions workflows that integrate
        with Turborepo's change detection
    researched_at: '2026-02-08T18:18:35.992247'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:01:25.347387'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 3dc6eacf
    planning_hash: 031b573d
  technical_notes:
    approach: 'Implement a multi-stage deployment pipeline using GitHub Actions with
      Turborepo integration. Create Docker containers for the Fastify backend and
      leverage Next.js standalone builds for frontends. Deploy to platform-native
      solutions (Vercel for frontends, Railway/Render for backend) with proper environment
      variable management and health checks. Implement database-first deployment with
      automated migrations and rollback capabilities.

      '
    external_dependencies:
    - name: '@vercel/cli'
      version: ^32.0.0
      reason: Programmatic Vercel deployments in CI/CD
    - name: supabase
      version: ^1.0.0
      reason: Database migration management and deployment
    - name: dotenv-cli
      version: ^7.0.0
      reason: Environment variable management across deployment stages
    - name: '@sentry/node'
      version: ^7.0.0
      reason: Production error tracking and deployment monitoring
    files_to_modify:
    - path: turbo.json
      changes: Add deployment tasks for each service with proper dependencies
    - path: package.json
      changes: Add deployment scripts and required CLI tools
    - path: apps/dashboard/next.config.js
      changes: Configure standalone output and environment variables
    - path: apps/storefront/next.config.js
      changes: Configure standalone output and environment variables
    new_files:
    - path: .github/workflows/deploy-staging.yml
      purpose: Staging deployment pipeline with Turborepo integration
    - path: .github/workflows/deploy-production.yml
      purpose: Production deployment with approval gates and rollback
    - path: apps/backend/Dockerfile
      purpose: Multi-stage Docker build for Fastify backend
    - path: apps/backend/src/routes/health.ts
      purpose: Health check endpoint for load balancer integration
    - path: apps/backend/src/middleware/deployment.ts
      purpose: Deployment-related middleware (graceful shutdown, etc.)
    - path: infrastructure/railway.json
      purpose: Railway deployment configuration for backend
    - path: scripts/deploy.sh
      purpose: Orchestration script for multi-service deployment
    - path: scripts/rollback.sh
      purpose: Automated rollback script for failed deployments
    - path: scripts/migrate.sh
      purpose: Database migration script integrated with deployment
    - path: .env.example
      purpose: Template for environment variables across all environments
    - path: docs/deployment.md
      purpose: Deployment procedures and troubleshooting guide
  acceptance_criteria:
  - criterion: All services (dashboard, storefront, backend) deploy successfully via
      GitHub Actions
    verification: Run `npm run deploy` and verify all services are accessible with
      200 status
  - criterion: Zero-downtime deployments with automatic rollback on health check failures
    verification: Deploy breaking change, confirm service rolls back automatically
      within 2 minutes
  - criterion: Environment-specific configurations work across dev/staging/production
    verification: Deploy to staging with different DB_URL, verify correct database
      connection
  - criterion: Database migrations run automatically before service deployment
    verification: Add new migration, deploy, confirm schema changes applied via Supabase
      dashboard
  - criterion: Health checks and monitoring dashboards are functional
    verification: Access /health endpoints return service status, monitoring shows
      deployment metrics
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/health.test.ts
      coverage_target: 95%
      scenarios:
      - Health endpoint returns service status
      - Database connectivity check
      - Environment variable validation
    - file: scripts/__tests__/deploy.test.ts
      coverage_target: 80%
      scenarios:
      - Deployment script argument parsing
      - Environment validation logic
      - Rollback trigger conditions
    integration_tests:
    - file: apps/backend/src/__tests__/integration/deployment.test.ts
      scenarios:
      - Service starts successfully with all dependencies
      - Database migration integration
      - Health check integration with load balancer
    - file: __tests__/e2e/deployment-pipeline.test.ts
      scenarios:
      - Full CI/CD pipeline execution
      - Multi-service deployment coordination
    manual_testing:
    - step: Trigger deployment via GitHub Actions
      expected: All services deploy without errors, accessible within 5 minutes
    - step: Force deployment failure by breaking health check
      expected: Automatic rollback triggered, previous version restored
    - step: Deploy with database migration
      expected: Migration runs first, then services deploy with new schema
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create Docker configurations for backend service'
      done: false
      ai_friendly: true
    - task: '[AI] Setup GitHub Actions workflows for CI/CD pipeline'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design deployment architecture and service orchestration strategy'
      done: false
      ai_friendly: false
    - task: '[AI] Implement health check endpoints for all services'
      done: false
      ai_friendly: true
    - task: '[AI] Create deployment and rollback scripts'
      done: false
      ai_friendly: true
    - task: '[AI] Configure Turborepo tasks for deployment pipeline'
      done: false
      ai_friendly: true
    - task: '[AI] Setup environment variable management and validation'
      done: false
      ai_friendly: true
    - task: '[AI] Implement database migration integration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure production security policies and secret management'
      done: false
      ai_friendly: false
    - task: '[AI] Write comprehensive deployment documentation'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Test deployment pipeline end-to-end and optimize'
      done: false
      ai_friendly: false
- key: T17
  title: SLA/SLO/SLI Definitions
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      SLA/SLO/SLI definitions are critical for establishing reliability standards
      for the Morpheus platform. As a novel-to-comic transformation service handling
      AI-intensive workflows (LLM processing, image generation via RunPod), users
      have expectations around processing times, availability, and quality. SLAs (Service
      Level Agreements) define commitments to users, SLOs (Service Level Objectives)
      set internal reliability targets, and SLIs (Service Level Indicators) provide
      measurable metrics. This is foundational infrastructure work needed before production
      deployment to ensure operational excellence and customer satisfaction.


      **Technical Approach:**

      Implement a multi-tiered monitoring and alerting system using Prometheus + Grafana
      for metrics collection, with custom SLI dashboards. Define SLOs using the error
      budget methodology (Google SRE practices). Create automated SLA reporting for
      customer-facing metrics. Key focus areas: API response times, comic generation
      pipeline latency, system availability, and error rates. Integrate with existing
      Fastify backend for custom metrics collection and Supabase for SLA compliance
      tracking.


      **AI Suitability Analysis:**

      - High AI effectiveness: Metrics collection middleware, dashboard configurations,
      automated report generation, test scenarios, boilerplate Prometheus/Grafana
      configs

      - Medium AI effectiveness: Alert rule definitions, SLO calculation logic, integration
      with existing Fastify routes

      - Low AI effectiveness: SLA target selection (business decisions), escalation
      procedures, incident response workflows, customer communication templates


      **Dependencies:**

      - External: [@prometheus/prom-client, grafana, alertmanager, @opentelemetry/api,
      @opentelemetry/auto-instrumentations-node]

      - Internal: Fastify backend instrumentation, Supabase metrics tables, Next.js
      admin dashboard integration, RunPod API monitoring


      **Risks:**

      - Over-aggressive SLAs: Start with conservative targets, iterate based on baseline
      measurements

      - Monitoring overhead: Use sampling and efficient metrics collection to avoid
      performance impact

      - Alert fatigue: Implement smart alerting with proper thresholds and escalation
      paths

      - Customer expectations mismatch: Clearly communicate SLA scope and limitations
      in AI-generated content scenarios


      **Complexity Notes:**

      More complex than initially estimated due to AI workflow unpredictability. Comic
      generation involves multiple async steps (text analysis, image generation, layout)
      making end-to-end SLIs challenging. AI agent effectiveness high for metrics
      infrastructure but human judgment critical for realistic target setting. Expect
      40% faster implementation with AI assistance on monitoring code.


      **Key Files:**

      - packages/backend/src/middleware/metrics.ts: Request/response time tracking

      - packages/backend/src/services/sla-monitor.ts: SLA compliance checking

      - apps/dashboard/src/pages/admin/sla-dashboard.tsx: Internal SLA monitoring
      UI

      - infrastructure/monitoring/prometheus.yml: Metrics collection config

      - infrastructure/grafana/dashboards/: SLI visualization dashboards

      '
    design_decisions:
    - decision: Use Prometheus + Grafana stack for metrics collection and visualization
      rationale: Industry standard, excellent TypeScript support, integrates well
        with containerized deployments, extensive alerting capabilities
      alternatives_considered:
      - DataDog (expensive for startup)
      - New Relic (less customizable)
      - Custom metrics solution (reinventing wheel)
      ai_implementation_note: AI can generate most Prometheus middleware and Grafana
        dashboard JSON configs from specifications
    - decision: Implement tiered SLAs based on subscription level and content complexity
      rationale: AI comic generation has variable processing times; tiered approach
        manages expectations while allowing premium service levels
      alternatives_considered:
      - Single SLA for all users (too restrictive)
      - No SLAs (poor customer experience)
      ai_implementation_note: AI excellent for implementing tier detection logic and
        automated SLA assignment
    - decision: Use error budget methodology for SLO management
      rationale: Balances reliability with development velocity, proven approach from
        Google SRE practices
      alternatives_considered:
      - Fixed uptime targets (inflexible)
      - No formal SLO framework (unmeasurable)
      ai_implementation_note: AI can implement error budget calculations and automated
        alerting when budgets are consumed
    researched_at: '2026-02-08T18:19:04.927750'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:01:58.428765'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 48f85dbd
    planning_hash: 72f04230
  technical_notes:
    approach: 'Implement comprehensive SLA/SLO/SLI framework starting with baseline
      metric collection across all Morpheus services. Deploy Prometheus for metrics
      aggregation with custom collectors for comic generation pipeline stages. Create
      Grafana dashboards for real-time SLI monitoring and historical SLA compliance
      reporting. Establish automated alerting workflows integrated with incident response
      procedures. Build customer-facing SLA dashboard in Next.js admin interface with
      transparent uptime and performance metrics.

      '
    external_dependencies:
    - name: prom-client
      version: ^15.1.0
      reason: Official Prometheus client for Node.js metrics collection
    - name: '@opentelemetry/api'
      version: ^1.7.0
      reason: Distributed tracing for comic generation pipeline monitoring
    - name: grafana
      version: ^10.2.0
      reason: Metrics visualization and alerting dashboard platform
    - name: prometheus
      version: ^2.48.0
      reason: Time-series database for metrics storage and querying
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Add Prometheus metrics middleware registration, OpenTelemetry auto-instrumentation
        setup
    - path: packages/backend/src/routes/comics.ts
      changes: Add custom metrics tracking for comic generation pipeline stages
    - path: packages/backend/src/services/runpod-client.ts
      changes: Instrument API calls with duration and error rate metrics
    - path: apps/dashboard/src/app/layout.tsx
      changes: Add navigation link to SLA dashboard for admin users
    new_files:
    - path: packages/backend/src/middleware/metrics.ts
      purpose: Prometheus metrics collection middleware for HTTP requests and custom
        business metrics
    - path: packages/backend/src/services/sla-monitor.ts
      purpose: SLA compliance calculation, error budget tracking, automated report
        generation
    - path: packages/backend/src/routes/sla.ts
      purpose: API endpoints for SLA data retrieval and admin controls
    - path: apps/dashboard/src/app/admin/sla-dashboard/page.tsx
      purpose: Customer-facing SLA dashboard with real-time metrics and historical
        data
    - path: infrastructure/monitoring/prometheus.yml
      purpose: Prometheus configuration for scraping Morpheus service metrics
    - path: infrastructure/grafana/dashboards/morpheus-sli.json
      purpose: Grafana dashboard for internal SLI monitoring and SLO tracking
    - path: infrastructure/grafana/dashboards/morpheus-sla.json
      purpose: Customer-facing SLA compliance dashboard
    - path: infrastructure/alerting/prometheus-rules.yml
      purpose: Alert rules for SLO breaches and system health monitoring
    - path: packages/shared/types/sla.ts
      purpose: TypeScript interfaces for SLA/SLO/SLI data structures
    - path: supabase/migrations/20241201000000_create_sla_tables.sql
      purpose: Database tables for SLA metrics storage and historical reporting
  acceptance_criteria:
  - criterion: SLI metrics collection active for all critical services (API endpoints,
      comic generation pipeline, RunPod integration)
    verification: Prometheus /metrics endpoint returns data for morpheus_request_duration,
      morpheus_comic_generation_duration, morpheus_error_rate with proper labels
  - criterion: SLO definitions established with error budgets for 99.0% availability
      and 95th percentile response times <30s for comic generation
    verification: Grafana dashboards show SLO compliance percentage and remaining
      error budget, alerts fire when SLO breach occurs
  - criterion: Customer-facing SLA dashboard displays real-time system status and
      30-day availability metrics
    verification: Navigate to /admin/sla-dashboard, verify uptime percentage, incident
      history, and current system status display correctly
  - criterion: Automated SLA compliance reporting generates weekly summaries stored
      in Supabase
    verification: Check sla_reports table contains weekly entries with availability_percentage,
      avg_response_time, incident_count fields
  - criterion: Alert escalation workflow triggers notifications when SLO thresholds
      breached
    verification: Simulate high error rate, verify Prometheus AlertManager sends notifications
      within 2 minutes
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/middleware/metrics.test.ts
      coverage_target: 90%
      scenarios:
      - Request duration tracking accuracy
      - Error rate calculation correctness
      - Custom label attachment for comic pipeline stages
      - Memory leak prevention in metrics collection
    - file: packages/backend/src/__tests__/services/sla-monitor.test.ts
      coverage_target: 85%
      scenarios:
      - SLA compliance calculation logic
      - Error budget consumption tracking
      - Historical data aggregation
      - Alert threshold evaluation
    integration_tests:
    - file: packages/backend/src/__tests__/integration/metrics-flow.test.ts
      scenarios:
      - End-to-end metrics collection from API request to Prometheus scrape
      - SLA report generation and Supabase storage
      - Alert firing and recovery workflow
    - file: apps/dashboard/src/__tests__/sla-dashboard.test.ts
      scenarios:
      - Dashboard data fetching and display
      - Real-time status updates via WebSocket
    manual_testing:
    - step: Generate high load on comic creation API endpoints
      expected: Prometheus metrics increase, SLO dashboard reflects load impact
    - step: Simulate RunPod API failures
      expected: Error rate SLI increases, alerts fire if threshold exceeded
    - step: Verify SLA dashboard responsiveness during system stress
      expected: Dashboard remains functional, shows accurate real-time status
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[HUMAN] Define SLA targets and SLO thresholds based on business requirements
        (99.0% availability, <30s response time)'
      done: false
      ai_friendly: false
    - task: '[AI] Create Supabase migration for SLA metrics tables (sla_reports, sli_snapshots,
        incident_log)'
      done: false
      ai_friendly: true
    - task: '[AI] Implement Prometheus metrics middleware with request duration, error
        rate, and custom comic pipeline metrics'
      done: false
      ai_friendly: true
    - task: '[AI] Build SLA monitoring service with compliance calculation and automated
        reporting logic'
      done: false
      ai_friendly: true
    - task: '[AI] Create Grafana dashboard configurations for internal SLI monitoring
        and customer SLA display'
      done: false
      ai_friendly: true
    - task: '[AI] Implement Next.js SLA dashboard component with real-time updates
        and historical charts'
      done: false
      ai_friendly: true
    - task: '[AI] Setup Prometheus AlertManager rules for SLO breach notifications'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit and integration tests for all SLA components'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure alert escalation procedures and incident response workflows'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Review SLA targets against baseline metrics and adjust thresholds
        if needed'
      done: false
      ai_friendly: false
- key: T18
  title: Performance Budgets & Optimization
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nPerformance budgets are critical for Morpheus\
      \ as it handles large image processing workloads (comic generation) and serves\
      \ both dashboard users and public storefront customers. Without proper budgets,\
      \ the platform could suffer from slow comic generation, poor user experience,\
      \ and high infrastructure costs. This task establishes measurable performance\
      \ targets and automated monitoring to catch regressions early in M0 before features\
      \ are built on top.\n\n**Technical Approach:**\n- Lighthouse CI integration\
      \ for frontend performance monitoring\n- Bundle analyzer integration (Next.js\
      \ built-in + custom metrics)\n- Backend performance monitoring with pino logging\
      \ + structured metrics\n- Database query performance tracking with Supabase\
      \ metrics\n- Real User Monitoring (RUM) with Next.js Speed Insights\n- Performance\
      \ budgets enforced in CI/CD pipeline\n- Custom performance dashboard aggregating\
      \ metrics from all sources\n\n**AI Suitability Analysis:**\n- High AI effectiveness:\
      \ \n  * Lighthouse CI configuration and workflow setup\n  * Bundle analyzer\
      \ integration and reporting scripts\n  * Basic performance monitoring middleware\n\
      \  * Test suite for performance assertions\n  * Dashboard components for metrics\
      \ visualization\n- Medium AI effectiveness:\n  * Performance optimization recommendations\
      \ based on metrics\n  * Database query optimization helpers\n  * Custom performance\
      \ monitoring utilities\n- Low AI effectiveness:\n  * Defining performance budget\
      \ thresholds (requires business context)\n  * Architecture decisions for monitoring\
      \ strategy\n  * Performance bottleneck analysis and solutions\n\n**Dependencies:**\n\
      - External: @lhci/cli, next-bundle-analyzer, clinic.js, autocannon, @vercel/speed-insights\n\
      - Internal: Requires integration with existing Fastify backend, Next.js apps,\
      \ Supabase setup\n- Infrastructure: Need CI/CD pipeline access, monitoring dashboard\
      \ deployment\n\n**Risks:**\n- Performance budgets too strict: Could block legitimate\
      \ feature development\n- Over-monitoring: Too many metrics could impact performance\
      \ itself  \n- Alert fatigue: Poor threshold tuning leads to ignored warnings\n\
      - Cost escalation: Comprehensive monitoring can be expensive at scale\n\n**Complexity\
      \ Notes:**\nMedium complexity - more involved than initial estimate due to multi-stack\
      \ monitoring (frontend, backend, database, ML pipelines). AI can significantly\
      \ accelerate the configuration and boilerplate aspects, but human judgment crucial\
      \ for meaningful budget thresholds and monitoring strategy.\n\n**Key Files:**\n\
      - .github/workflows/performance.yml: CI performance checks\n- apps/dashboard/next.config.js:\
      \ Bundle analysis configuration  \n- apps/storefront/next.config.js: Bundle\
      \ analysis configuration\n- packages/backend/src/plugins/monitoring.ts: Performance\
      \ middleware\n- packages/shared/src/monitoring/: Shared performance utilities\n\
      - tools/performance/: Custom performance testing and reporting tools\n"
    design_decisions:
    - decision: Multi-tier monitoring approach (synthetic + RUM + infrastructure)
      rationale: Comprehensive coverage needed for complex ML-driven platform with
        multiple user types
      alternatives_considered:
      - Lighthouse-only approach
      - Third-party monitoring service
      - Custom metrics only
      ai_implementation_note: AI can generate most monitoring configurations and integrations
        once strategy is defined
    - decision: Performance budgets enforced in CI with bypass mechanism
      rationale: Catch regressions early but allow emergency deployments when needed
      alternatives_considered:
      - Warning-only approach
      - Strict blocking
      - Post-deployment monitoring only
      ai_implementation_note: AI excellent for generating GitHub Actions workflows
        and test configurations
    researched_at: '2026-02-08T18:19:30.512841'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:02:24.583184'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: a5512cb9
    planning_hash: 27d85f83
  technical_notes:
    approach: 'Implement a comprehensive performance monitoring stack with Lighthouse
      CI for frontend metrics, custom Fastify middleware for backend performance,
      and Supabase query monitoring. Establish CI-enforced budgets for key metrics
      (bundle size, FCP, LCP, API response times) with automated reporting. Create
      a unified performance dashboard aggregating all metrics and integrate with alerting
      systems for regression detection.

      '
    external_dependencies:
    - name: '@lhci/cli'
      version: ^0.12.0
      reason: Lighthouse CI for automated performance testing
    - name: '@next/bundle-analyzer'
      version: ^14.0.0
      reason: Bundle size analysis for Next.js apps
    - name: '@vercel/speed-insights'
      version: ^1.0.0
      reason: Real user monitoring for production performance
    - name: clinic
      version: ^13.0.0
      reason: Node.js performance profiling for backend
    - name: autocannon
      version: ^7.12.0
      reason: HTTP load testing for API performance budgets
    - name: pino-pretty
      version: ^10.2.0
      reason: Enhanced logging for performance metrics
    files_to_modify:
    - path: apps/dashboard/next.config.js
      changes: Add bundle analyzer plugin and performance monitoring config
    - path: apps/storefront/next.config.js
      changes: Add bundle analyzer plugin and performance monitoring config
    - path: packages/backend/src/app.ts
      changes: Register performance monitoring plugin
    - path: package.json
      changes: Add performance monitoring dependencies and scripts
    new_files:
    - path: .github/workflows/performance.yml
      purpose: CI workflow for performance testing and budget enforcement
    - path: packages/shared/src/monitoring/performance-collector.ts
      purpose: Core performance metrics collection utilities
    - path: packages/shared/src/monitoring/budget-validator.ts
      purpose: Performance budget validation and threshold checking
    - path: packages/backend/src/plugins/monitoring.ts
      purpose: Fastify plugin for backend performance monitoring
    - path: tools/performance/lighthouse-ci.config.js
      purpose: Lighthouse CI configuration with custom budgets
    - path: tools/performance/bundle-analyzer.js
      purpose: Custom bundle analysis and reporting script
    - path: tools/performance/load-test.js
      purpose: Automated load testing with autocannon
    - path: apps/dashboard/src/components/PerformanceDashboard.tsx
      purpose: Real-time performance metrics visualization
    - path: apps/dashboard/src/pages/api/performance/metrics.ts
      purpose: API endpoint for performance metrics aggregation
    - path: packages/shared/src/monitoring/types.ts
      purpose: TypeScript types for performance monitoring data
    - path: docs/performance-monitoring.md
      purpose: Documentation for performance monitoring setup and usage
  acceptance_criteria:
  - criterion: Performance budgets are enforced in CI/CD pipeline with configurable
      thresholds
    verification: CI fails when bundle size exceeds 500KB or Lighthouse performance
      score drops below 90
  - criterion: Real-time performance metrics are collected from all application layers
    verification: Dashboard displays frontend (FCP, LCP), backend (response times),
      and database (query duration) metrics
  - criterion: Automated performance regression detection triggers alerts
    verification: Performance degradation >20% from baseline generates Slack/email
      notification within 5 minutes
  - criterion: Bundle analysis reports are generated and accessible for each deployment
    verification: Bundle analyzer outputs saved as CI artifacts and viewable in performance
      dashboard
  - criterion: Performance monitoring overhead stays under 5ms per request
    verification: Monitoring middleware adds <5ms latency measured via synthetic tests
  testing:
    unit_tests:
    - file: packages/shared/src/monitoring/__tests__/performance-collector.test.ts
      coverage_target: 90%
      scenarios:
      - Metric collection and aggregation
      - Budget threshold validation
      - Error handling for failed metrics
      - Memory leak prevention
    - file: packages/backend/src/plugins/__tests__/monitoring.test.ts
      coverage_target: 85%
      scenarios:
      - Request timing middleware
      - Database query tracking
      - Memory usage monitoring
    integration_tests:
    - file: tools/performance/__tests__/lighthouse-ci.test.ts
      scenarios:
      - End-to-end Lighthouse CI workflow
      - Bundle size validation pipeline
      - Performance regression detection
    - file: apps/dashboard/__tests__/performance-dashboard.test.ts
      scenarios:
      - Metrics aggregation and display
      - Real-time updates via WebSocket
    manual_testing:
    - step: Deploy with intentionally large bundle (600KB+)
      expected: CI pipeline fails with clear budget violation message
    - step: Generate load test traffic to backend
      expected: Performance metrics visible in dashboard within 30 seconds
    - step: Simulate database slowdown
      expected: Query performance alerts triggered and visible
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup performance monitoring dependencies and basic project structure'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define performance budget thresholds based on business requirements'
      done: false
      ai_friendly: false
    - task: '[AI] Implement Lighthouse CI configuration and GitHub Actions workflow'
      done: false
      ai_friendly: true
    - task: '[AI] Create bundle analyzer integration and reporting scripts'
      done: false
      ai_friendly: true
    - task: '[AI] Build backend performance monitoring middleware with pino integration'
      done: false
      ai_friendly: true
    - task: '[AI] Implement shared performance utilities and TypeScript types'
      done: false
      ai_friendly: true
    - task: '[AI] Create performance dashboard UI components with real-time updates'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive test suite for all performance monitoring components'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure alerting thresholds and notification channels'
      done: false
      ai_friendly: false
    - task: '[AI] Generate performance monitoring documentation and setup guides'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Conduct performance baseline testing and threshold validation'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Code review and integration testing with existing infrastructure'
      done: false
      ai_friendly: false
- key: T2
  title: GitHub Milestones & Issues Creation
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task involves setting up GitHub project management infrastructure by creating
      milestones and issues for the Morpheus project. This is foundational work needed
      to organize development, track progress, and provide clear deliverables for
      the team. Given the extensive use of GitHub Copilot agents, well-structured
      issues with clear descriptions will enable better AI-assisted development. This
      creates a feedback loop where good project management enhances AI effectiveness.


      **Technical Approach:**

      Use GitHub''s REST API v4 or CLI to programmatically create milestones and issues.
      Structure issues with consistent templates that include acceptance criteria,
      technical requirements, and AI-friendly descriptions. Consider using GitHub''s
      project boards (Projects v2) for kanban-style workflow management. Issues should
      be tagged with appropriate labels for area (frontend/backend/ml), priority (p0-p3),
      and type (feature/bug/task). Each issue should reference relevant parts of the
      monorepo structure and include links to related documentation.


      **AI Suitability Analysis:**

      - High AI effectiveness: [Issue template generation, bulk issue creation scripts,
      milestone scheduling logic, label standardization, GitHub API integration boilerplate]

      - Medium AI effectiveness: [Issue prioritization logic, dependency mapping between
      tasks, project timeline estimation]

      - Low AI effectiveness: [Strategic milestone planning, business priority decisions,
      architecture-level task breakdown, resource allocation decisions]


      **Dependencies:**

      - External: [@octokit/rest, github-cli, or direct REST API calls]

      - Internal: [Project roadmap decisions, team capacity planning, milestone timeline
      alignment with M0-M3 structure]


      **Risks:**

      - Over-engineering project management: Keep tooling simple, focus on GitHub
      native features rather than complex external tools

      - Issue granularity mismatch: Balance between too-granular (micromanagement)
      and too-broad (loses AI assistance value)

      - Outdated milestone tracking: Establish processes for keeping GitHub state
      synchronized with actual development progress


      **Complexity Notes:**

      Lower complexity than initially estimated. This is primarily data entry and
      API integration work. AI can significantly accelerate the bulk creation and
      template standardization aspects. The main complexity lies in the strategic
      planning phase (human-driven) rather than the execution phase (AI-friendly).


      **Key Files:**

      - .github/ISSUE_TEMPLATE/: Issue templates for different task types

      - scripts/setup-github.ts: Script to create milestones and bulk issues

      - docs/DEVELOPMENT.md: Document the issue workflow and labeling system

      '
    design_decisions:
    - decision: Use GitHub's native project management features (milestones, issues,
        labels) rather than external tools
      rationale: Keeps tooling simple, integrates naturally with GitHub Copilot workflow,
        reduces external dependencies
      alternatives_considered:
      - Linear integration
      - Jira integration
      - Custom project management dashboard
      ai_implementation_note: AI can generate comprehensive issue creation scripts
        and templates following GitHub API patterns
    - decision: Structure issues with AI-friendly descriptions including file paths,
        acceptance criteria, and technical context
      rationale: Maximizes GitHub Copilot effectiveness by providing clear context
        for code generation and task understanding
      alternatives_considered:
      - Minimal issue descriptions
      - Link-heavy external documentation approach
      ai_implementation_note: AI can generate consistent issue templates that include
        all necessary context for effective code assistance
    - decision: Create milestone structure aligned with M0-M3 roadmap and include
        estimated completion dates
      rationale: Provides clear project timeline visibility and enables progress tracking
        against business objectives
      alternatives_considered:
      - Feature-based milestones
      - Sprint-based short iterations
      ai_implementation_note: AI can calculate milestone dates based on task dependencies
        and estimated effort levels
    researched_at: '2026-02-08T18:19:55.466114'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:02:47.523856'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: bef0186e
    planning_hash: 1f7bded8
  technical_notes:
    approach: 'Create a TypeScript script using Octokit to programmatically generate
      GitHub milestones (M0-M3) and corresponding issues. Develop standardized issue
      templates that include technical context, file paths, and acceptance criteria
      optimized for AI assistance. Implement a labeling system for area, priority,
      and type classification. Set up GitHub project boards to visualize workflow
      and track progress across the monorepo structure.

      '
    external_dependencies:
    - name: '@octokit/rest'
      version: ^20.0.0
      reason: Official GitHub API client for programmatic issue and milestone creation
    - name: github-slugger
      version: ^2.0.0
      reason: Generate consistent URL-friendly slugs for issue titles and labels
    files_to_modify:
    - path: package.json
      changes: Add @octokit/rest, @octokit/types dependencies
    - path: docs/DEVELOPMENT.md
      changes: Add section on GitHub workflow and issue management
    new_files:
    - path: scripts/setup-github.ts
      purpose: Main script to create milestones, issues, and project setup
    - path: scripts/lib/github-client.ts
      purpose: GitHub API client wrapper with authentication
    - path: scripts/lib/milestone-data.ts
      purpose: Milestone definitions with dates and descriptions
    - path: scripts/lib/issue-templates.ts
      purpose: Issue data structures and bulk creation logic
    - path: scripts/config/labels.json
      purpose: Label definitions for areas, priorities, and types
    - path: .github/ISSUE_TEMPLATE/feature.yml
      purpose: Template for feature requests with AI-friendly structure
    - path: .github/ISSUE_TEMPLATE/bug.yml
      purpose: Bug report template with debugging context
    - path: .github/ISSUE_TEMPLATE/task.yml
      purpose: General task template optimized for AI assistance
    - path: scripts/__tests__/setup-github.test.ts
      purpose: Unit tests for GitHub setup functionality
    - path: scripts/__tests__/integration/github-api.test.ts
      purpose: Integration tests for GitHub API interactions
    - path: scripts/__tests__/__fixtures__/mock-issues.json
      purpose: Test data for issue creation scenarios
  acceptance_criteria:
  - criterion: All 4 milestones (M0-M3) created in GitHub with proper dates and descriptions
    verification: Check GitHub milestones page shows M0-M3 with due dates and progress
      tracking
  - criterion: At least 20 well-structured issues created across all project areas
      with proper labels
    verification: GitHub issues page shows issues with area/priority/type labels and
      AI-friendly descriptions
  - criterion: Issue templates established for consistent task creation
    verification: Verify .github/ISSUE_TEMPLATE/ contains feature.yml, bug.yml, and
      task.yml templates
  - criterion: GitHub Projects v2 board configured with proper workflow columns
    verification: Project board shows Backlog/In Progress/Review/Done columns with
      issues properly categorized
  - criterion: Setup script successfully creates all GitHub artifacts programmatically
    verification: Run scripts/setup-github.ts and verify all milestones, issues, and
      labels are created
  testing:
    unit_tests:
    - file: scripts/__tests__/setup-github.test.ts
      coverage_target: 90%
      scenarios:
      - Milestone creation with proper dates
      - Issue creation with all required fields
      - Label creation and assignment
      - Error handling for API failures
      - Duplicate milestone/issue detection
    integration_tests:
    - file: scripts/__tests__/integration/github-api.test.ts
      scenarios:
      - End-to-end GitHub API workflow
      - Template parsing and issue generation
    manual_testing:
    - step: Run setup script against test repository
      expected: All milestones and issues created without errors
    - step: Create new issue using templates
      expected: Template renders with proper sections and AI-friendly format
    - step: Verify project board functionality
      expected: Issues move between columns and update status correctly
  estimates:
    development: 2.5
    code_review: 0.5
    testing: 1
    documentation: 0.5
    total: 4.5
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup TypeScript project structure and install @octokit/rest dependencies'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define milestone timeline and strategic priorities for M0-M3'
      done: false
      ai_friendly: false
    - task: '[AI] Create GitHub API client wrapper with authentication handling'
      done: false
      ai_friendly: true
    - task: '[AI] Generate issue template YAML files with proper GitHub Actions syntax'
      done: false
      ai_friendly: true
    - task: '[AI] Implement milestone creation logic with date calculations'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review and prioritize initial issue backlog for logical dependencies'
      done: false
      ai_friendly: false
    - task: '[AI] Build bulk issue creation script with label assignment'
      done: false
      ai_friendly: true
    - task: '[AI] Create comprehensive unit tests for all GitHub operations'
      done: false
      ai_friendly: true
    - task: '[AI] Set up GitHub Projects v2 board with automation rules'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Test script execution and validate issue quality/completeness'
      done: false
      ai_friendly: false
- key: T3
  title: Database Schema v2 Migration
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 5
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Database Schema v2 Migration addresses the evolution from initial prototype
      schema to production-ready data structures for the novel-to-comic transformation
      platform. This is critical for M0 as it establishes the foundational data layer
      that all subsequent features depend on. The migration likely includes optimizations
      for comic panel relationships, user content management, AI processing workflows,
      and multi-tenant architecture for both dashboard and storefront experiences.


      **Technical Approach:**

      Use Supabase migrations with PostgreSQL-specific features (JSONB, arrays, triggers).
      Implement schema versioning with rollback capabilities, leveraging Drizzle ORM
      or Prisma for type-safe database interactions. Create migration scripts that
      handle data transformation, not just schema changes. Use PostgreSQL''s transactional
      DDL for atomic migrations and implement proper indexing strategy for comic content
      queries and AI processing workflows.


      **AI Suitability Analysis:**

      - High AI effectiveness: Migration script boilerplate, CRUD operation updates,
      test data generation, basic trigger/function creation, TypeScript type generation
      from schema

      - Medium AI effectiveness: Data transformation logic, index optimization queries,
      existing data migration scripts, API endpoint updates to match new schema

      - Low AI effectiveness: Schema design decisions, performance optimization strategy,
      complex relational modeling, business logic for data integrity constraints


      **Dependencies:**

      - External: @supabase/supabase-js, drizzle-orm or prisma, zod for validation

      - Internal: Existing database connection utilities, API route handlers, data
      access layers across backend services


      **Risks:**

      - Data loss during migration: Implement comprehensive backup strategy and test
      on staging data first

      - Downtime during deployment: Use blue-green deployment with schema compatibility
      layers

      - Type safety breakage: Update all TypeScript interfaces and validate against
      existing codebase

      - Performance regression: Benchmark queries before/after migration, especially
      for comic rendering pipelines


      **Complexity Notes:**

      Higher complexity than typical migrations due to potential AI workflow data
      structures and comic panel relationships. AI can significantly accelerate the
      mechanical aspects (script generation, type updates) but human oversight critical
      for data integrity and performance. Expect 2-3x velocity improvement with AI
      for boilerplate while requiring senior architect decisions for schema design.


      **Key Files:**

      - supabase/migrations/: New migration files with schema changes

      - packages/database/: Schema definitions and type exports

      - apps/api/src/db/: Database connection and query utilities

      - apps/dashboard/src/lib/: Client-side database interactions

      - packages/shared/types/: Shared TypeScript interfaces

      '
    design_decisions:
    - decision: Use Supabase native migrations over ORM-based migrations
      rationale: Leverages PostgreSQL-specific features, better performance, and maintains
        Supabase ecosystem consistency
      alternatives_considered:
      - Prisma migrations
      - Custom migration runner
      - Drizzle migrations
      ai_implementation_note: AI can generate migration SQL from schema descriptions
        and handle boilerplate rollback logic
    - decision: Implement schema versioning with backward compatibility layer
      rationale: Enables zero-downtime deployments and gradual API migration across
        frontend/backend
      alternatives_considered:
      - Breaking change migration
      - Dual-write pattern
      - Event sourcing
      ai_implementation_note: AI can create compatibility views and update API endpoints
        to use version-specific queries
    - decision: Use JSONB for flexible AI processing metadata
      rationale: Comic generation workflows require flexible metadata storage that
        evolves with AI model capabilities
      alternatives_considered:
      - Separate metadata tables
      - PostgreSQL arrays
      - External document store
      ai_implementation_note: AI can generate TypeScript types from JSONB schemas
        and create validation utilities
    researched_at: '2026-02-08T18:20:19.926873'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:03:11.427891'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 2ef1bf73
    planning_hash: '16487648'
  technical_notes:
    approach: 'Create comprehensive migration strategy starting with schema analysis
      and dependency mapping. Generate migration files using Supabase CLI with careful
      attention to foreign key constraints and data integrity. Implement data transformation
      scripts for existing records, ensuring type safety through generated TypeScript
      interfaces. Test migration on staging environment with production data subset,
      then deploy with rollback procedures and monitoring.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.39.0
      reason: Database client and migration utilities
    - name: drizzle-orm
      version: ^0.29.0
      reason: Type-safe database queries and schema definitions
    - name: zod
      version: ^3.22.0
      reason: Runtime schema validation for database operations
    - name: '@supabase/cli'
      version: ^1.142.0
      reason: Migration generation and database management
    files_to_modify:
    - path: packages/database/src/schema.ts
      changes: Update to v2 schema definitions with new tables and relationships
    - path: packages/database/src/types.ts
      changes: Regenerate TypeScript types from v2 schema
    - path: apps/api/src/db/queries.ts
      changes: Update queries to match v2 schema structure
    - path: apps/dashboard/src/lib/supabase.ts
      changes: Update client queries for v2 schema
    - path: packages/shared/types/database.ts
      changes: Update shared database types
    new_files:
    - path: supabase/migrations/20241201000000_schema_v2.sql
      purpose: Main migration script for v2 schema changes
    - path: supabase/migrations/20241201000001_data_transform.sql
      purpose: Data transformation script for existing records
    - path: scripts/migration/backup.ts
      purpose: Pre-migration backup script
    - path: scripts/migration/rollback.ts
      purpose: Rollback script with data restoration
    - path: scripts/migration/integrity-check.ts
      purpose: Data integrity verification script
    - path: packages/database/src/v2-adapter.ts
      purpose: Adapter layer for gradual migration
    - path: docs/database/v2-migration-guide.md
      purpose: Migration documentation and rollback procedures
  acceptance_criteria:
  - criterion: Migration successfully transforms existing data from v1 to v2 schema
      without data loss
    verification: Run `npm run db:migrate` and verify row counts match before/after
      with `npm run test:migration-integrity`
  - criterion: All TypeScript types are updated and compile without errors across
      all packages
    verification: Run `npm run type-check` in root and all apps/packages directories
      return success
  - criterion: API endpoints continue to function with new schema structure
    verification: Run integration test suite `npm run test:integration` with 100%
      pass rate
  - criterion: Migration can be rolled back without data corruption
    verification: Execute rollback script and verify data integrity with `npm run
      test:rollback-integrity`
  - criterion: Performance benchmarks meet or exceed v1 schema query times
    verification: Run `npm run benchmark:db` and verify comic panel queries <200ms,
      user queries <100ms
  testing:
    unit_tests:
    - file: packages/database/src/__tests__/schema.test.ts
      coverage_target: 90%
      scenarios:
      - Schema validation with Zod
      - Type generation accuracy
      - Constraint validation
    - file: packages/database/src/__tests__/migrations.test.ts
      coverage_target: 85%
      scenarios:
      - Migration script execution
      - Data transformation accuracy
      - Rollback functionality
    integration_tests:
    - file: apps/api/src/__tests__/integration/database-v2.test.ts
      scenarios:
      - Full CRUD operations with new schema
      - Comic panel relationship queries
      - User content management workflows
      - AI processing data structures
    - file: apps/dashboard/src/__tests__/integration/db-client.test.ts
      scenarios:
      - Dashboard data fetching with new schema
      - Real-time subscriptions functionality
    manual_testing:
    - step: Create test comic with panels using dashboard
      expected: Data persists correctly in v2 schema structure
    - step: Trigger AI processing workflow on test novel
      expected: Processing data stored in optimized v2 format
    - step: Load test with 1000+ comics
      expected: Query performance within SLA thresholds
  estimates:
    development: 2.5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 5.5
    ai_acceleration_factor: 0.55
  progress:
    status: not-started
    checklist:
    - task: '[AI] Generate comprehensive backup script for existing data'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design v2 schema with comic panel relationships and AI workflow
        optimizations'
      done: false
      ai_friendly: false
    - task: '[AI] Create Supabase migration files based on schema design'
      done: false
      ai_friendly: true
    - task: '[AI] Generate TypeScript types and Zod schemas from v2 structure'
      done: false
      ai_friendly: true
    - task: '[AI] Update all database query functions to use v2 schema'
      done: false
      ai_friendly: true
    - task: '[AI] Create data transformation scripts for existing records'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive test suite for migration functionality'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Test migration on staging with production data subset'
      done: false
      ai_friendly: false
    - task: '[AI] Create rollback procedures and integrity checking scripts'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Performance optimization and index strategy validation'
      done: false
      ai_friendly: false
- key: T4
  title: Testing Infrastructure Setup
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Testing infrastructure is the foundation for reliable development velocity in
      the Morpheus platform. With a complex multi-service architecture (Fastify backend,
      Next.js frontend, ML pipelines, Supabase integration), we need comprehensive
      testing across unit, integration, and E2E levels. This prevents regressions
      during rapid feature development and ensures the novel-to-comic transformation
      pipeline works reliably. Given the AI-assisted development approach, proper
      testing infrastructure also validates AI-generated code quality.


      **Technical Approach:**

      - **Unit Testing**: Vitest for all TypeScript services (backend APIs, shared
      utilities, React components)

      - **Integration Testing**: Vitest + Supabase test database + Docker containers
      for service interactions

      - **E2E Testing**: Playwright for critical user journeys (novel upload â†’ comic
      generation â†’ payment)

      - **API Testing**: Supertest with Fastify for REST endpoints

      - **Database Testing**: Supabase local development + test migrations

      - **ML Pipeline Testing**: Mock OpenAI/Anthropic responses, fixture-based Stable
      Diffusion outputs

      - **CI/CD Integration**: GitHub Actions with parallel test execution and coverage
      reporting


      **AI Suitability Analysis:**

      - High AI effectiveness: Test boilerplate generation, mock data creation, basic
      unit tests, API test cases, database seed scripts

      - Medium AI effectiveness: Integration test setup, Playwright selectors, test
      utility functions, CI/CD configuration

      - Low AI effectiveness: Test strategy decisions, complex E2E scenarios, flaky
      test debugging, performance test thresholds


      **Dependencies:**

      - External: Vitest, Playwright, @testcontainers/postgresql, supertest, msw (API
      mocking), @supabase/supabase-js test utilities

      - Internal: All packages need testing setup, shared test utilities workspace,
      database schema for test fixtures


      **Risks:**

      - Flaky E2E tests: Use data-testid attributes, proper wait strategies, isolated
      test environments

      - Slow test execution: Implement test parallelization, smart test selection
      based on changed files

      - ML service dependencies: Mock external APIs aggressively, use fixture responses
      for deterministic tests

      - Database state pollution: Implement proper teardown, use transactions for
      test isolation

      - CI cost explosion: Optimize test execution order, cache dependencies, use
      matrix builds wisely


      **Complexity Notes:**

      This is more complex than initial estimate due to ML pipeline testing requirements
      and multi-service integration scenarios. However, AI assistance will significantly
      accelerate test case generation and boilerplate setup. Expect 60% faster implementation
      with AI for repetitive testing patterns.


      **Key Files:**

      - packages/*/vitest.config.ts: Service-specific test configuration

      - apps/web/playwright.config.ts: E2E test configuration

      - packages/test-utils/: Shared testing utilities and fixtures

      - .github/workflows/test.yml: CI/CD pipeline configuration

      - packages/database/migrations/test/: Test-specific database setup

      '
    design_decisions:
    - decision: Use Vitest over Jest for all unit/integration testing
      rationale: Better TypeScript support, faster execution, native ESM support,
        consistent with Vite-based tooling
      alternatives_considered:
      - Jest
      - Node.js native test runner
      ai_implementation_note: AI can generate comprehensive Vitest configurations
        and test suites following established patterns
    - decision: Playwright for E2E testing over Cypress
      rationale: Better multi-browser support, faster execution, excellent CI integration,
        handles file uploads/downloads needed for novel processing
      alternatives_considered:
      - Cypress
      - Puppeteer
      ai_implementation_note: AI excels at generating Playwright page objects and
        test scenarios from user story descriptions
    - decision: Testcontainers for integration testing with real PostgreSQL
      rationale: Avoids mocking complex database behaviors, ensures Supabase compatibility,
        isolates tests properly
      alternatives_considered:
      - SQLite in-memory
      - Mock database
      - Shared test database
      ai_implementation_note: AI can generate container setup and teardown boilerplate,
        but human oversight needed for resource optimization
    researched_at: '2026-02-08T18:20:54.925742'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:03:38.238658'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 43a449a5
    planning_hash: 161673b8
  technical_notes:
    approach: 'Implement a three-tier testing strategy: Vitest for fast unit tests
      with high coverage, integration tests using Testcontainers for database interactions,
      and Playwright E2E tests for critical user workflows. Create a shared test-utils
      package for common fixtures, mocks, and utilities. Set up GitHub Actions with
      parallel execution and smart caching. Mock external ML services with realistic
      fixtures while maintaining contract testing for API boundaries.

      '
    external_dependencies:
    - name: vitest
      version: ^2.0.0
      reason: Primary unit testing framework with TypeScript support
    - name: '@playwright/test'
      version: ^1.40.0
      reason: E2E testing across browsers with excellent CI support
    - name: '@testcontainers/postgresql'
      version: ^10.0.0
      reason: Real PostgreSQL instances for integration testing
    - name: supertest
      version: ^6.3.0
      reason: HTTP testing for Fastify REST APIs
    - name: msw
      version: ^2.0.0
      reason: Mock external API calls (OpenAI, Anthropic, RunPod)
    - name: '@vitest/coverage-v8'
      version: ^2.0.0
      reason: Code coverage reporting with V8 engine
    files_to_modify:
    - path: package.json
      changes: Add root-level test scripts for unified test execution
    - path: turbo.json
      changes: Configure test pipeline with proper dependencies and caching
    new_files:
    - path: packages/test-utils/src/index.ts
      purpose: Shared testing utilities, fixtures, and mock factories
    - path: packages/test-utils/src/fixtures/novels.ts
      purpose: Sample novel data for consistent testing
    - path: packages/test-utils/src/fixtures/ml-responses.ts
      purpose: Mock OpenAI/Anthropic/Stable Diffusion responses
    - path: packages/test-utils/src/database.ts
      purpose: Database setup/teardown utilities for tests
    - path: packages/test-utils/vitest.config.ts
      purpose: Shared Vitest configuration
    - path: apps/backend/vitest.config.ts
      purpose: Backend-specific test configuration with Supabase setup
    - path: apps/web/vitest.config.ts
      purpose: Frontend unit test configuration with React Testing Library
    - path: apps/web/playwright.config.ts
      purpose: E2E test configuration with parallel browsers
    - path: packages/shared/vitest.config.ts
      purpose: Shared utilities test configuration
    - path: packages/database/src/test-helpers.ts
      purpose: Database test utilities and migrations
    - path: .github/workflows/test.yml
      purpose: CI/CD pipeline with matrix builds and caching
    - path: apps/backend/src/__tests__/setup.ts
      purpose: Global test setup for backend services
    - path: apps/web/src/__tests__/setup.ts
      purpose: Global test setup for React components
    - path: docker-compose.test.yml
      purpose: Isolated services for integration testing
  acceptance_criteria:
  - criterion: All packages have comprehensive test suites with >85% code coverage
    verification: Run `pnpm test:coverage` from root - all packages show >85% coverage
  - criterion: E2E tests cover critical user journeys (upload â†’ generation â†’ payment)
    verification: Run `pnpm test:e2e` - all Playwright tests pass in CI environment
  - criterion: CI/CD pipeline runs all tests in <10 minutes with parallel execution
    verification: GitHub Actions workflow completes successfully under time threshold
  - criterion: ML pipeline testing uses mocked responses with realistic fixtures
    verification: Tests run without external API calls - check MSW mock usage in logs
  - criterion: Database testing uses isolated environments with proper cleanup
    verification: Tests can run concurrently without state pollution - parallel execution
      passes
  testing:
    unit_tests:
    - file: packages/*/src/__tests__/**/*.test.ts
      coverage_target: 85%
      scenarios:
      - Service layer business logic
      - Utility function edge cases
      - React component rendering
      - Error boundary handling
    - file: apps/backend/src/__tests__/routes/*.test.ts
      coverage_target: 90%
      scenarios:
      - API endpoint happy paths
      - Authentication failures
      - Validation errors
      - Rate limiting
    integration_tests:
    - file: apps/backend/src/__tests__/integration/novel-processing.test.ts
      scenarios:
      - Novel upload to comic generation pipeline
      - Database transaction integrity
      - File storage operations
    - file: apps/backend/src/__tests__/integration/payment.test.ts
      scenarios:
      - Stripe webhook processing
      - Subscription state changes
    e2e_tests:
    - file: apps/web/tests/e2e/user-journey.spec.ts
      scenarios:
      - Complete novel-to-comic transformation
      - User authentication flow
      - Payment processing
      - File download/sharing
    manual_testing:
    - step: Upload various novel formats (txt, epub, pdf)
      expected: All formats processed without errors
    - step: Trigger comic generation with different style preferences
      expected: Generated comics match selected art styles
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create shared test-utils package structure and basic utilities'
      done: false
      ai_friendly: true
    - task: '[AI] Generate Vitest configurations for all packages'
      done: false
      ai_friendly: true
    - task: '[AI] Create sample fixtures for novels, users, and ML responses'
      done: false
      ai_friendly: true
    - task: '[AI] Implement database test helpers with setup/teardown'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design test strategy for ML pipeline mocking approach'
      done: false
      ai_friendly: false
    - task: '[AI] Generate unit test boilerplate for existing services'
      done: false
      ai_friendly: true
    - task: '[AI] Create Playwright E2E test structure and basic scenarios'
      done: false
      ai_friendly: true
    - task: '[AI] Implement GitHub Actions workflow with parallel execution'
      done: false
      ai_friendly: true
    - task: '[AI] Create Docker Compose setup for integration test services'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure flaky test detection and retry strategies'
      done: false
      ai_friendly: false
    - task: '[AI] Generate API test cases using Supertest for all endpoints'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review test coverage gaps and adjust thresholds'
      done: false
      ai_friendly: false
    - task: '[AI] Create comprehensive README for testing practices'
      done: false
      ai_friendly: true
- key: T5
  title: Weights & Biases Integration
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 3
  area: ml
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Weights & Biases (W&B) integration is critical for ML experiment tracking, monitoring,
      and optimization in Morpheus. As we use RunPod Stable Diffusion for image generation
      and potentially train custom models for novel-to-comic transformation, we need
      robust MLOps infrastructure to track model performance, hyperparameters, generated
      image quality metrics, and system costs. This enables data-driven optimization
      of our AI pipeline and debugging of generation quality issues.


      **Technical Approach:**

      Implement W&B SDK integration at the ML service layer with structured experiment
      tracking. Create a dedicated ML monitoring service that wraps W&B operations
      and integrates with our Fastify backend. Use W&B''s artifact system for model
      versioning, run tracking for generation experiments, and dashboard creation
      for monitoring image generation quality metrics (CLIP scores, user ratings,
      generation time/cost).


      **AI Suitability Analysis:**

      - High AI effectiveness: W&B SDK boilerplate, metric logging functions, configuration
      setup, basic API wrappers, test data generation

      - Medium AI effectiveness: Custom metric calculation logic, dashboard configuration,
      integration with existing ML pipeline, error handling patterns

      - Low AI effectiveness: Experiment design strategy, metric selection rationale,
      alerting thresholds, data privacy considerations for logged artifacts


      **Dependencies:**

      - External: wandb SDK, @wandb/sdk (if using Node.js client), potentially wandb-service
      for self-hosted

      - Internal: ML service layer, RunPod integration service, user feedback collection
      system, cost tracking utilities


      **Risks:**

      - Data privacy leakage: Ensure no user content or PII flows to W&B cloud - implement
      data sanitization

      - Performance overhead: W&B logging could slow generation pipeline - use async
      logging and sampling

      - Vendor lock-in: Heavy W&B dependence - abstract behind interface for potential
      future migration

      - Cost accumulation: W&B cloud costs scale with usage - monitor and set limits


      **Complexity Notes:**

      Medium complexity task made easier with AI assistance. The W&B SDK has excellent
      documentation and common patterns, making it very AI-friendly for implementation.
      However, designing meaningful experiments and metrics requires domain expertise.
      AI can accelerate the integration work significantly while humans focus on ML
      strategy.


      **Key Files:**

      - packages/ml-service/src/monitoring/wandb-client.ts: W&B wrapper service

      - packages/ml-service/src/services/image-generation.ts: Add experiment tracking

      - packages/backend/src/services/ml-monitoring.ts: Backend ML metrics endpoint

      - packages/shared/src/types/ml-metrics.ts: Shared metric type definitions

      '
    design_decisions:
    - decision: Use W&B cloud service with data sanitization layer
      rationale: Fastest time-to-value vs self-hosted complexity, with privacy protection
        through content filtering
      alternatives_considered:
      - MLflow self-hosted
      - Custom metrics dashboard
      - W&B self-hosted
      ai_implementation_note: AI can generate comprehensive data sanitization functions
        and W&B integration boilerplate
    - decision: Abstract W&B behind MLMonitoringService interface
      rationale: Prevents vendor lock-in and enables testing with mock implementations
      alternatives_considered:
      - Direct W&B SDK usage
      - Multiple monitoring providers simultaneously
      ai_implementation_note: AI excellent at creating interface abstractions and
        implementing wrapper patterns
    - decision: Async experiment logging with local queuing
      rationale: Prevents W&B API latency from blocking image generation pipeline
      alternatives_considered:
      - Synchronous logging
      - Fire-and-forget logging
      - Batch logging
      ai_implementation_note: AI can implement robust async patterns and queue management
        logic
    researched_at: '2026-02-08T18:21:21.515711'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:04:02.985661'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 365e4558
    planning_hash: cf14fb33
  technical_notes:
    approach: 'Create a MLMonitoringService that wraps W&B SDK with data sanitization
      and async logging. Integrate at key points in the image generation pipeline
      to track experiments, log generation metrics, and store model artifacts. Implement
      a backend endpoint to surface ML metrics to the dashboard. Use W&B''s project/entity
      structure to organize experiments by user stories and model versions.

      '
    external_dependencies:
    - name: wandb
      version: ^0.16.0
      reason: Official W&B Python SDK for experiment tracking (if using Python ML
        services)
    - name: '@wandb/sdk'
      version: ^0.3.0
      reason: W&B TypeScript SDK for Node.js integration with Fastify backend
    - name: uuid
      version: ^9.0.0
      reason: Generate unique experiment IDs and run names for W&B tracking
    files_to_modify:
    - path: packages/ml-service/src/services/image-generation.ts
      changes: Add MLMonitoringService injection and experiment tracking calls
    - path: packages/backend/src/routes/ml.ts
      changes: Add /metrics endpoint for W&B data retrieval
    - path: packages/backend/src/app.ts
      changes: Register ML monitoring service and routes
    new_files:
    - path: packages/ml-service/src/monitoring/wandb-client.ts
      purpose: W&B SDK wrapper with data sanitization and async logging
    - path: packages/ml-service/src/monitoring/interfaces.ts
      purpose: IMLMonitoringService interface for vendor abstraction
    - path: packages/ml-service/src/monitoring/data-sanitizer.ts
      purpose: Utility for removing PII and sensitive data from logs
    - path: packages/backend/src/services/ml-monitoring.ts
      purpose: Backend service for retrieving and formatting ML metrics
    - path: packages/shared/src/types/ml-metrics.ts
      purpose: Shared TypeScript types for ML experiment data
    - path: packages/ml-service/src/config/wandb.ts
      purpose: W&B configuration with environment-based settings
  acceptance_criteria:
  - criterion: W&B experiments are automatically created and tracked for each image
      generation request
    verification: Run image generation test and verify experiment appears in W&B dashboard
      with correct metadata
  - criterion: ML metrics (generation time, cost, quality scores) are logged to W&B
      without exposing user content
    verification: Check W&B runs contain sanitized metrics but no user prompts, images,
      or PII
  - criterion: W&B integration doesn't degrade generation pipeline performance by
      >100ms
    verification: Run performance benchmarks with/without W&B logging enabled
  - criterion: Backend ML metrics endpoint surfaces W&B data for dashboard consumption
    verification: GET /api/ml/metrics returns formatted experiment data from W&B API
  - criterion: W&B service is properly abstracted behind interface for future migration
      flexibility
    verification: IMLMonitoringService interface allows switching implementations
      without breaking consumers
  testing:
    unit_tests:
    - file: packages/ml-service/src/__tests__/monitoring/wandb-client.test.ts
      coverage_target: 90%
      scenarios:
      - Successful experiment logging with sanitized data
      - Error handling when W&B API unavailable
      - Async logging doesn't block operations
      - Data sanitization removes PII correctly
    - file: packages/backend/src/__tests__/services/ml-monitoring.test.ts
      coverage_target: 85%
      scenarios:
      - Metrics retrieval from W&B API
      - Error handling for W&B service failures
      - Proper data transformation for frontend
    integration_tests:
    - file: packages/ml-service/src/__tests__/integration/generation-tracking.test.ts
      scenarios:
      - End-to-end image generation creates W&B experiment
      - Multiple concurrent generations log correctly
      - Failed generations still log attempt metrics
    - file: packages/backend/src/__tests__/integration/ml-metrics-api.test.ts
      scenarios:
      - ML metrics endpoint returns real W&B data
      - API authentication and rate limiting work
    manual_testing:
    - step: Generate 5 images with different prompts and styles
      expected: 5 separate experiments in W&B with generation metrics but no user
        content
    - step: Check W&B dashboard for experiment organization and custom metrics
      expected: Experiments grouped by model version with custom quality metrics visible
    - step: Verify backend metrics endpoint performance under load
      expected: API responds within 500ms with paginated results
  estimates:
    development: 2.5
    code_review: 1
    testing: 1
    documentation: 0.5
    total: 5
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[AI] Install W&B SDK dependencies and setup basic configuration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define ML experiment strategy and key metrics to track'
      done: false
      ai_friendly: false
    - task: '[AI] Create IMLMonitoringService interface and type definitions'
      done: false
      ai_friendly: true
    - task: '[AI] Implement WandBClient class with SDK integration and error handling'
      done: false
      ai_friendly: true
    - task: '[AI] Build data sanitization utility to remove PII from logs'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review data privacy implementation and set alerting thresholds'
      done: false
      ai_friendly: false
    - task: '[AI] Integrate monitoring calls into image generation service'
      done: false
      ai_friendly: true
    - task: '[AI] Create backend ML metrics service and API endpoint'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit and integration tests'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure W&B projects and validate experiment organization'
      done: false
      ai_friendly: false
- key: T6
  title: CI/CD Workflows Setup
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn:
  - T4
  agent_notes:
    research_findings: '**Context:**

      CI/CD workflows are critical for the Morpheus platform''s development velocity
      and reliability. With a complex monorepo containing multiple applications (dashboard,
      storefront, backend API) and ML integrations, automated testing, building, and
      deployment prevents integration issues and ensures consistent environments.
      This enables the team to deploy confidently multiple times per day while maintaining
      quality standards across all workspaces.


      **Technical Approach:**

      - GitHub Actions for CI/CD (native integration with Copilot)

      - Turborepo''s built-in caching and task orchestration for monorepo builds

      - Matrix builds for different environments (staging, production)

      - Docker multi-stage builds for consistent deployments

      - Separate workflows for different triggers (PR validation, main branch deployment,
      release)

      - Integration with Supabase CLI for database migrations

      - Environment-specific secrets management via GitHub Secrets

      - Playwright tests running in CI with visual regression testing

      - Automated dependency updates via Dependabot


      **AI Suitability Analysis:**

      - High AI effectiveness: Workflow YAML generation, Docker configurations, test
      scripts, environment setup scripts, standard deployment patterns

      - Medium AI effectiveness: Turborepo task orchestration, secrets management
      setup, notification integrations

      - Low AI effectiveness: Deployment strategy decisions, security policy definitions,
      environment architecture choices


      **Dependencies:**

      - External: Docker, GitHub Actions runners, Supabase CLI, deployment platforms
      (Vercel for frontend, Railway/Render for backend)

      - Internal: All existing workspaces, existing test suites, environment configurations


      **Risks:**

      - Secret management complexity: Use GitHub''s encrypted secrets and environment-specific
      protection rules

      - Build time optimization: Leverage Turborepo caching and Docker layer caching

      - ML model deployment complexity: Separate pipeline for RunPod integrations
      with proper fallbacks

      - Database migration coordination: Implement proper migration rollback strategies

      - Monorepo deployment coordination: Use change detection to deploy only affected
      services


      **Complexity Notes:**

      Initially appears straightforward, but monorepo CI/CD is inherently complex.
      AI agents excel at generating workflow templates and Docker configurations,
      significantly reducing implementation time. The main complexity lies in orchestrating
      deployments across multiple services while maintaining data consistency.


      **Key Files:**

      - .github/workflows/: CI/CD workflow definitions

      - docker/: Multi-stage Dockerfiles for each service

      - turbo.json: Build and test task definitions

      - package.json: Workspace-level CI scripts

      '
    design_decisions:
    - decision: Use GitHub Actions with Turborepo integration
      rationale: Native GitHub integration, excellent monorepo support, and strong
        AI assistant compatibility for workflow generation
      alternatives_considered:
      - Jenkins
      - GitLab CI
      - CircleCI
      ai_implementation_note: Copilot excels at generating GitHub Actions YAML with
        proper Turborepo task orchestration
    - decision: Implement change-based deployment strategy
      rationale: Only deploy services that have actual changes to reduce deployment
        time and risk
      alternatives_considered:
      - Deploy all services always
      - Manual deployment selection
      ai_implementation_note: AI can generate change detection scripts using git diff
        and Turborepo's --filter flags
    - decision: Separate workflows for different environments and triggers
      rationale: PR validation, staging deployment, and production release have different
        requirements and risk profiles
      alternatives_considered:
      - Single mega-workflow
      - Manual environment promotion
      ai_implementation_note: AI can create workflow templates with proper environment-specific
        configurations
    researched_at: '2026-02-08T18:21:46.309144'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:04:27.473512'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 441349a7
    planning_hash: 3c5c3c92
  technical_notes:
    approach: 'Implement a multi-workflow CI/CD system using GitHub Actions integrated
      with Turborepo''s task orchestration. Create separate workflows for PR validation
      (testing + building), staging deployment (automatic on main), and production
      releases (manual trigger with approval). Use change detection to deploy only
      affected services, with proper database migration coordination and rollback
      capabilities.

      '
    external_dependencies:
    - name: '@vercel/ncc'
      version: ^0.38.0
      reason: Compile GitHub Actions for better performance
    - name: docker/build-push-action
      version: v5
      reason: Optimized Docker builds with caching
    - name: github/super-linter
      version: v5
      reason: Code quality validation across all languages
    - name: supabase/cli
      version: ^1.100.0
      reason: Database migration management in CI
    files_to_modify:
    - path: turbo.json
      changes: Add CI-specific tasks (ci:test, ci:build, ci:deploy) with proper caching
        configuration
    - path: package.json
      changes: Add workspace-level CI scripts and deployment commands
    - path: supabase/config.toml
      changes: Configure CI-friendly database settings and migration paths
    new_files:
    - path: .github/workflows/pr-validation.yml
      purpose: Run tests and builds on pull requests with change detection
    - path: .github/workflows/staging-deploy.yml
      purpose: Automatic deployment to staging on main branch merges
    - path: .github/workflows/production-deploy.yml
      purpose: Manual production deployment with approval gates
    - path: .github/workflows/dependency-updates.yml
      purpose: Automated dependency updates via Dependabot
    - path: docker/dashboard.Dockerfile
      purpose: Multi-stage Docker build for Next.js dashboard
    - path: docker/storefront.Dockerfile
      purpose: Multi-stage Docker build for storefront application
    - path: docker/backend.Dockerfile
      purpose: Multi-stage Docker build for NestJS backend API
    - path: scripts/deploy-utils.ts
      purpose: Deployment utilities for change detection and coordination
    - path: scripts/migration-runner.ts
      purpose: Database migration coordination and rollback logic
    - path: .github/environments/staging.yml
      purpose: Staging environment configuration and secrets
    - path: .github/environments/production.yml
      purpose: Production environment with approval requirements
  acceptance_criteria:
  - criterion: PR validation workflow runs automatically on pull requests with parallel
      testing across all workspaces
    verification: Create test PR and verify workflow runs tests for dashboard, storefront,
      and backend with proper status checks
  - criterion: Staging deployment automatically deploys affected services when changes
      merge to main branch
    verification: Merge change to main and verify only affected services deploy to
      staging environment within 10 minutes
  - criterion: Production deployment workflow requires manual approval and includes
      rollback capability
    verification: Trigger production deployment, verify approval gate works, and test
      rollback functionality
  - criterion: Database migrations run safely with proper coordination across environments
    verification: Deploy schema change and verify migration runs before application
      deployment with proper error handling
  - criterion: Build performance utilizes Turborepo caching to reduce CI times by
      60%+
    verification: Compare initial vs cached build times, verify cache hit rates in
      workflow logs
  testing:
    unit_tests:
    - file: scripts/__tests__/deploy-utils.test.ts
      coverage_target: 85%
      scenarios:
      - Change detection logic
      - Environment validation
      - Migration coordination
      - Rollback procedures
    integration_tests:
    - file: .github/workflows/__tests__/workflow-validation.yml
      scenarios:
      - Full CI pipeline with all workspaces
      - Selective deployment based on changes
      - Database migration integration
      - Secret management across environments
    manual_testing:
    - step: Create PR with frontend-only changes
      expected: Only frontend tests run, backend deployment skipped
    - step: Trigger production deployment with approval
      expected: Workflow waits for approval, deploys with zero downtime
    - step: Test rollback scenario
      expected: Previous version restored within 2 minutes
  estimates:
    development: 4
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 7.5
    ai_acceleration_factor: 0.5
  progress:
    status: not-started
    checklist:
    - task: '[AI] Generate base GitHub Actions workflow templates for PR validation,
        staging, and production'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define deployment strategy and environment architecture decisions'
      done: false
      ai_friendly: false
    - task: '[AI] Create multi-stage Dockerfiles for dashboard, storefront, and backend
        services'
      done: false
      ai_friendly: true
    - task: '[AI] Configure Turborepo tasks for CI/CD with proper caching and task
        dependencies'
      done: false
      ai_friendly: true
    - task: '[AI] Implement change detection logic to deploy only affected services'
      done: false
      ai_friendly: true
    - task: '[AI] Create database migration coordination scripts with Supabase CLI
        integration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure GitHub environments, secrets, and approval policies'
      done: false
      ai_friendly: false
    - task: '[AI] Generate deployment utility scripts for service orchestration and
        rollbacks'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive tests for deployment utilities and workflow
        validation'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Security review of secrets management and deployment permissions'
      done: false
      ai_friendly: false
    - task: '[HUMAN] End-to-end testing of complete deployment pipeline'
      done: false
      ai_friendly: false
- key: T7
  title: Mock Mode for External Services
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Mock mode for external services is critical for development velocity, testing
      reliability, and cost management. In Morpheus, we have several expensive/rate-limited
      external services (OpenAI/Anthropic LLMs, RunPod Stable Diffusion, Supabase
      storage) that need to be mocked during development and testing. This prevents
      developers from hitting API limits, incurring costs, or dealing with flaky network
      conditions while building features. It also enables deterministic testing and
      offline development.


      **Technical Approach:**

      Implement a service layer abstraction with mock implementations using dependency
      injection. Create a configuration-driven approach where services can be switched
      between real and mock implementations via environment variables. Use factory
      pattern for service instantiation and MSW (Mock Service Worker) for API mocking.
      Store mock responses as fixtures for consistent testing. Implement realistic
      delay simulation and error scenarios.


      **AI Suitability Analysis:**

      - High AI effectiveness: Mock service implementations, fixture data generation,
      test scenarios, configuration boilerplate, type definitions

      - Medium AI effectiveness: Service factory patterns, MSW handler setup, environment
      variable parsing

      - Low AI effectiveness: Architectural decisions on which services to mock, mock
      data strategy, determining realistic response patterns


      **Dependencies:**

      - External: msw, faker.js, dotenv, joi/zod for config validation

      - Internal: Service layer architecture, configuration system, existing API clients
      for OpenAI/Anthropic/RunPod


      **Risks:**

      - Mock drift: Mock responses diverging from real API responses over time - mitigate
      with integration tests and periodic real API validation

      - Incomplete mocking: Missing edge cases or error scenarios - mitigate with
      comprehensive test scenarios

      - Performance assumptions: Mocks being too fast/perfect - mitigate with realistic
      delay and error simulation

      - Configuration complexity: Too many mock configuration options - mitigate with
      sensible defaults and clear documentation


      **Complexity Notes:**

      Initially appears straightforward but complexity increases with the number of
      external services and interaction patterns. AI can significantly accelerate
      implementation of mock services and test fixtures, but architectural decisions
      about abstraction layers require human oversight. The configuration management
      system adds moderate complexity.


      **Key Files:**

      - packages/core/src/services/: Service interfaces and implementations

      - packages/core/src/config/mock.config.ts: Mock configuration management

      - packages/api/src/lib/service-factory.ts: Service instantiation logic

      - tests/fixtures/: Mock response data

      - vitest.setup.ts: Test environment mock configuration

      '
    design_decisions:
    - decision: Use dependency injection with factory pattern for service instantiation
      rationale: Enables clean switching between mock and real implementations without
        code changes, supports different mock levels per environment
      alternatives_considered:
      - Direct environment checks in service calls
      - Monkey patching
      - Separate mock servers
      ai_implementation_note: AI can generate factory boilerplate and service interfaces
        once pattern is established
    - decision: Implement MSW for HTTP-level mocking combined with service-level mocks
      rationale: Provides both unit-level mocking for fast tests and integration-level
        mocking for realistic scenarios
      alternatives_considered:
      - Only service mocks
      - Only HTTP mocks
      - Jest mocks
      ai_implementation_note: AI excels at generating MSW handlers and realistic mock
        responses from API documentation
    - decision: Configuration-driven mock behavior with fixture-based responses
      rationale: Enables different mock scenarios for testing, predictable responses
        for development, easy maintenance
      alternatives_considered:
      - Hard-coded mock responses
      - Dynamic mock generation
      - Record/replay approach
      ai_implementation_note: AI can generate fixture data and configuration schemas
        efficiently
    researched_at: '2026-02-08T18:22:12.356111'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:04:54.006327'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 5475aa0c
    planning_hash: 7f296f69
  technical_notes:
    approach: 'Create abstract service interfaces for all external APIs (LLM, image
      generation, storage). Implement concrete classes for both real and mock versions
      using a factory pattern driven by environment configuration. Use MSW for HTTP-level
      mocking in tests and development. Store mock responses as JSON fixtures with
      realistic data generated using faker.js. Implement a configuration system that
      allows granular control over which services are mocked and what behavior they
      exhibit (success/failure rates, response times, etc.).

      '
    external_dependencies:
    - name: msw
      version: ^2.0.0
      reason: HTTP request mocking for realistic API simulation
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate realistic mock data for fixtures
    - name: zod
      version: ^3.22.0
      reason: Validate mock configuration and ensure type safety
    - name: node-cache
      version: ^5.1.0
      reason: Cache mock responses to simulate API rate limiting and improve performance
    files_to_modify:
    - path: packages/core/src/config/index.ts
      changes: Add mock mode configuration schema and validation
    - path: packages/api/src/lib/services.ts
      changes: Replace direct service instantiation with factory pattern
    - path: apps/web/vitest.config.ts
      changes: Configure MSW for test environment
    - path: apps/api/vitest.config.ts
      changes: Configure MSW for API test environment
    new_files:
    - path: packages/core/src/services/interfaces/llm.interface.ts
      purpose: Define common interface for LLM services (OpenAI, Anthropic)
    - path: packages/core/src/services/interfaces/image-gen.interface.ts
      purpose: Define interface for image generation services (RunPod)
    - path: packages/core/src/services/interfaces/storage.interface.ts
      purpose: Define interface for storage services (Supabase)
    - path: packages/core/src/services/mock/mock-llm.service.ts
      purpose: Mock implementation of LLM services
    - path: packages/core/src/services/mock/mock-image-gen.service.ts
      purpose: Mock implementation of image generation
    - path: packages/core/src/services/mock/mock-storage.service.ts
      purpose: Mock implementation of storage operations
    - path: packages/core/src/services/factory/service-factory.ts
      purpose: Factory for instantiating real vs mock services
    - path: packages/core/src/config/mock.config.ts
      purpose: Mock-specific configuration management
    - path: tests/fixtures/llm-responses.json
      purpose: Realistic mock response data for LLM services
    - path: tests/fixtures/image-gen-responses.json
      purpose: Mock response data for image generation
    - path: tests/fixtures/storage-responses.json
      purpose: Mock response data for storage operations
    - path: packages/core/src/services/mock/msw-handlers.ts
      purpose: MSW request handlers for HTTP-level mocking
    - path: packages/core/src/utils/mock-delays.ts
      purpose: Utility for simulating realistic API response delays
  acceptance_criteria:
  - criterion: All external services (OpenAI, Anthropic, RunPod, Supabase) can be
      switched to mock mode via environment variables
    verification: Set MOCK_MODE=true and verify all external API calls return mock
      data without network requests
  - criterion: Mock services return realistic response data with configurable delays
      and error scenarios
    verification: Run test suite with mock services and verify response structure
      matches real APIs, delays are simulated
  - criterion: Development environment can run completely offline with mock services
    verification: Disconnect from internet, start dev server with MOCK_MODE=true,
      verify all features work
  - criterion: Mock responses are deterministic and consistent for the same inputs
    verification: Execute same mock API calls multiple times and verify identical
      responses
  - criterion: Service factory correctly instantiates real vs mock implementations
      based on configuration
    verification: Unit tests verify factory returns correct service type for each
      configuration
  testing:
    unit_tests:
    - file: packages/core/src/services/__tests__/service-factory.test.ts
      coverage_target: 90%
      scenarios:
      - Factory returns mock services when MOCK_MODE=true
      - Factory returns real services when MOCK_MODE=false
      - Invalid configuration throws appropriate errors
      - Service interface compliance for all implementations
    - file: packages/core/src/services/__tests__/mock-services.test.ts
      coverage_target: 85%
      scenarios:
      - Mock LLM service returns valid responses
      - Mock image generation simulates processing time
      - Mock storage operations succeed/fail as configured
      - Error scenarios are properly simulated
    integration_tests:
    - file: packages/api/src/__tests__/integration/mock-mode.test.ts
      scenarios:
      - API endpoints work with mock services end-to-end
      - No external network calls made in mock mode
      - Real and mock service responses have compatible schemas
    manual_testing:
    - step: Start development server with MOCK_MODE=true
      expected: Server starts without external API credentials
    - step: Trigger LLM completion request
      expected: Returns mock response within configured delay
    - step: Generate image through RunPod mock
      expected: Returns placeholder image URL or base64 data
  estimates:
    development: 2.5
    code_review: 0.5
    testing: 0.8
    documentation: 0.3
    total: 4.1
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create service interface definitions for all external APIs'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review service interfaces and approve abstraction level'
      done: false
      ai_friendly: false
    - task: '[AI] Generate mock response fixtures using faker.js'
      done: false
      ai_friendly: true
    - task: '[AI] Implement mock service classes with realistic behavior'
      done: false
      ai_friendly: true
    - task: '[AI] Create service factory with environment-based instantiation'
      done: false
      ai_friendly: true
    - task: '[AI] Setup MSW handlers for HTTP-level mocking'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure mock behavior patterns and error scenarios'
      done: false
      ai_friendly: false
    - task: '[AI] Update configuration system to support mock mode'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit tests for all mock services'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for mock mode workflows'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Manual testing and validation of mock behavior accuracy'
      done: false
      ai_friendly: false
    - task: '[AI] Generate documentation for mock mode usage'
      done: false
      ai_friendly: true
- key: T8
  title: Project Timeline & Cost Estimates
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Project timeline and cost estimation is critical for M0 milestone planning and
      stakeholder communication. This task involves creating realistic development
      schedules, resource allocation, and budget projections for the Morpheus platform.
      Given the AI-assisted development approach with GitHub Copilot, traditional
      estimation models need adjustment to account for accelerated development cycles
      in certain areas while maintaining realistic timelines for architecture and
      design work.


      **Technical Approach:**

      - Create estimation framework using story points adjusted for AI assistance
      multipliers

      - Implement project tracking dashboard using Next.js components with real-time
      progress visualization

      - Use PostgreSQL views in Supabase for aggregating task completion metrics

      - Build estimation templates for common patterns (CRUD operations, API endpoints,
      UI components)

      - Integrate with GitHub API for actual velocity tracking vs estimates

      - Create cost models factoring in AI tool subscriptions, cloud resources, and
      development time


      **AI Suitability Analysis:**

      - High AI effectiveness: Estimation templates generation, dashboard boilerplate,
      data aggregation queries, progress tracking components, cost calculation formulas

      - Medium AI effectiveness: Integration with project management APIs, historical
      data analysis, report generation

      - Low AI effectiveness: Initial estimation methodology design, stakeholder requirement
      gathering, risk assessment frameworks, strategic timeline decisions


      **Dependencies:**

      - External: @supabase/supabase-js for data queries, recharts for visualization,
      date-fns for timeline calculations, @octokit/rest for GitHub integration

      - Internal: Shared UI components from design system, authentication middleware,
      database schemas for project tracking


      **Risks:**

      - Over-optimistic AI productivity assumptions: Mitigate with conservative multipliers
      and regular velocity reviews

      - Scope creep in estimation granularity: Focus on milestone-level estimates
      initially

      - Integration complexity with existing project tools: Use webhook-based loose
      coupling

      - Inaccurate cost projections for ML/AI services: Build in 30% buffer for usage
      spikes


      **Complexity Notes:**

      Initially appears straightforward but becomes complex when accounting for AI-assisted
      development variables. Traditional estimation models don''t account for the
      productivity variance between AI-friendly tasks (2-3x faster) and human-required
      tasks (unchanged velocity). This creates a bi-modal estimation challenge requiring
      custom frameworks.


      **Key Files:**

      - apps/dashboard/src/components/ProjectTimeline.tsx: Timeline visualization
      component

      - apps/dashboard/src/pages/estimates/index.tsx: Main estimation interface

      - packages/database/migrations/: Tables for project tracking and estimates

      - apps/api/src/routes/estimates/: Backend APIs for timeline calculations

      - packages/shared/src/estimation-utils.ts: Shared estimation logic

      '
    design_decisions:
    - decision: AI-adjusted story point estimation framework
      rationale: 'Traditional story points don''t account for AI assistance variance.
        Need multipliers: 0.3x for boilerplate, 0.5x for CRUD, 1.0x for architecture
        work'
      alternatives_considered:
      - Time-based estimates
      - T-shirt sizing
      - Feature-based estimation
      ai_implementation_note: Copilot can generate the calculation logic and estimation
        templates once framework is defined
    - decision: Embedded estimation dashboard in main app vs standalone tool
      rationale: Embedding in dashboard provides better integration with existing
        auth/UI and reduces maintenance overhead
      alternatives_considered:
      - Standalone estimation app
      - Google Sheets integration
      - Third-party PM tools
      ai_implementation_note: Dashboard components and data fetching logic are highly
        AI-suitable for rapid development
    researched_at: '2026-02-08T18:22:37.072116'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:05:20.549207'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 249f583f
    planning_hash: fd3977fd
  technical_notes:
    approach: 'Build an integrated estimation and tracking system within the existing
      dashboard using a story point framework adjusted for AI assistance. Create database
      models for storing estimates, actuals, and velocity metrics. Develop visualization
      components for timeline and cost projections. Implement GitHub API integration
      for automatic progress tracking against estimates.

      '
    external_dependencies:
    - name: recharts
      version: ^2.8.0
      reason: Timeline and cost visualization charts
    - name: date-fns
      version: ^3.0.0
      reason: Timeline calculations and date manipulations
    - name: '@octokit/rest'
      version: ^20.0.0
      reason: GitHub API integration for velocity tracking
    - name: zod
      version: ^3.22.0
      reason: Estimation data validation schemas
    files_to_modify:
    - path: packages/database/supabase/migrations/20240101000000_project_estimates.sql
      changes: Add tables for estimates, actuals, velocity_metrics, cost_projections
    - path: apps/dashboard/src/components/Sidebar.tsx
      changes: Add navigation link to /estimates page
    - path: packages/shared/src/types/index.ts
      changes: Add estimation-related TypeScript interfaces
    new_files:
    - path: packages/shared/src/estimation-utils.ts
      purpose: Core estimation logic, AI multipliers, cost calculations
    - path: apps/dashboard/src/components/ProjectTimeline.tsx
      purpose: Interactive timeline visualization with milestones
    - path: apps/dashboard/src/components/VelocityChart.tsx
      purpose: Historical velocity tracking and projections
    - path: apps/dashboard/src/components/CostBreakdown.tsx
      purpose: Cost visualization by category and time period
    - path: apps/dashboard/src/pages/estimates/index.tsx
      purpose: Main estimation dashboard interface
    - path: apps/dashboard/src/pages/estimates/[projectId].tsx
      purpose: Detailed project estimation and tracking page
    - path: apps/api/src/routes/estimates/index.ts
      purpose: CRUD APIs for estimation data
    - path: apps/api/src/routes/estimates/github.ts
      purpose: GitHub webhook handlers for progress tracking
    - path: apps/api/src/services/estimation-service.ts
      purpose: Business logic for calculations and aggregations
    - path: apps/api/src/services/github-service.ts
      purpose: GitHub API integration for velocity tracking
    - path: packages/database/src/queries/estimates.ts
      purpose: Supabase queries for estimation data
  acceptance_criteria:
  - criterion: Estimation framework calculates story points with AI assistance multipliers
      (0.3x-0.4x for AI-friendly tasks, 1.0x for human-required tasks)
    verification: Run estimation calculation tests and verify multipliers are applied
      correctly based on task type classification
  - criterion: Project timeline dashboard displays milestone progress, velocity metrics,
      and cost projections with real-time updates
    verification: Navigate to /estimates page, verify charts render with sample data,
      and check auto-refresh functionality
  - criterion: GitHub API integration tracks actual completion times vs estimates
      with 85%+ accuracy
    verification: Compare GitHub issue/PR completion data against stored estimates
      in database queries
  - criterion: Cost estimation includes AI tool subscriptions, cloud resources, and
      adjusted development time with 30% buffer
    verification: Generate cost report and verify all cost categories are included
      with proper buffer calculations
  - criterion: Historical velocity data enables accurate future sprint planning with
      confidence intervals
    verification: Generate velocity report spanning 3+ sprints and verify confidence
      interval calculations
  testing:
    unit_tests:
    - file: packages/shared/src/__tests__/estimation-utils.test.ts
      coverage_target: 90%
      scenarios:
      - Story point calculation with AI multipliers
      - Cost estimation with different resource types
      - Timeline calculation edge cases
      - Velocity metric calculations
    - file: apps/api/src/__tests__/estimates/routes.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations for estimates
      - GitHub API integration error handling
      - Data aggregation queries
    integration_tests:
    - file: apps/dashboard/src/__tests__/integration/project-timeline.test.ts
      scenarios:
      - Timeline component with real Supabase data
      - GitHub API webhook processing
      - Real-time dashboard updates
    manual_testing:
    - step: Create new project estimate with mixed AI/human tasks
      expected: System applies correct multipliers and generates realistic timeline
    - step: Simulate GitHub webhook for task completion
      expected: Dashboard updates velocity metrics and remaining estimates
  estimates:
    development: 4
    code_review: 0.5
    testing: 1.2
    documentation: 0.3
    total: 6
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create database migration for estimation tables (projects, estimates,
        actuals, velocity_metrics, cost_projections)'
      done: false
      ai_friendly: true
    - task: '[AI] Generate TypeScript interfaces for estimation data models'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design estimation methodology framework and AI task classification
        criteria'
      done: false
      ai_friendly: false
    - task: '[AI] Implement core estimation utilities with story point calculations
        and AI multipliers'
      done: false
      ai_friendly: true
    - task: '[AI] Build timeline visualization component using recharts with milestone
        markers'
      done: false
      ai_friendly: true
    - task: '[AI] Create velocity tracking chart component with historical data and
        trend lines'
      done: false
      ai_friendly: true
    - task: '[AI] Implement cost breakdown component with category filtering'
      done: false
      ai_friendly: true
    - task: '[AI] Build estimation dashboard pages with data fetching and real-time
        updates'
      done: false
      ai_friendly: true
    - task: '[AI] Create API routes for CRUD operations on estimation data'
      done: false
      ai_friendly: true
    - task: '[AI] Implement GitHub API integration service for progress tracking'
      done: false
      ai_friendly: true
    - task: '[AI] Build GitHub webhook handlers for automatic progress updates'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit tests for all utility functions and
        components'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Calibrate AI assistance multipliers based on initial development
        velocity'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Review estimation accuracy and adjust methodology based on early
        results'
      done: false
      ai_friendly: false
- key: T9
  title: Storybook 8 Setup for Component Development
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Storybook 8 is essential for Morpheus''s component-driven development approach.
      Given the platform''s complex UI needs (novel reader, comic panel editor, admin
      dashboard), having isolated component development and visual regression testing
      is critical. Storybook enables parallel frontend development while backend APIs
      are being built, visual QA for comic presentation components, and serves as
      living documentation for the design system. This is particularly valuable for
      Morpheus since comic layouts require precise visual validation.


      **Technical Approach:**

      - Use Storybook 8 with Vite builder for fast HMR and modern tooling compatibility

      - Configure for Next.js 16 compatibility with App Router support

      - Implement Component Story Format (CSF) 3.0 for type-safe stories

      - Set up essential addons: Controls, Actions, Viewport, A11y, and Docs

      - Configure visual regression testing with Chromatic or Percy integration

      - Establish story structure: Basic, Interactive, Edge Cases, and Visual variants

      - Use Mock Service Worker (MSW) addon for API mocking during component development

      - Configure Tailwind CSS integration for consistent styling


      **AI Suitability Analysis:**

      - High AI effectiveness: Initial Storybook configuration files, basic story
      templates, addon setup boilerplate, TypeScript configurations, build scripts

      - Medium AI effectiveness: Complex story scenarios, component prop documentation,
      integration with existing Next.js setup, MSW handler creation

      - Low AI effectiveness: Visual design decisions, component interaction patterns,
      story organization strategy, performance optimization decisions


      **Dependencies:**

      - External: @storybook/nextjs, @storybook/addon-essentials, @storybook/addon-interactions,
      @storybook/test, msw-storybook-addon, chromatic

      - Internal: Requires shared UI components to be extracted, needs design tokens/theme
      configuration, depends on component library structure


      **Risks:**

      - Build performance degradation: Use Vite builder and optimize story imports

      - Next.js 16 compatibility issues: Pin compatible Storybook version, test App
      Router integration

      - Monorepo complexity: Configure proper workspace resolution and shared dependencies

      - Visual regression test flakiness: Implement proper loading states and deterministic
      data


      **Complexity Notes:**

      Moderate complexity due to Next.js 16 + Turborepo integration challenges. Storybook
      8 has improved DX but requires careful configuration for monorepo setups. AI
      can significantly accelerate initial setup and story creation, reducing typical
      2-3 day setup to 1 day with human oversight.


      **Key Files:**

      - .storybook/main.ts: Core Storybook configuration

      - .storybook/preview.ts: Global decorators and parameters

      - .storybook/middleware.js: MSW integration

      - package.json: Dependencies and scripts

      - turbo.json: Build pipeline integration

      - apps/dashboard/stories/: Story files for dashboard components

      - apps/storefront/stories/: Story files for storefront components

      '
    design_decisions:
    - decision: Use Storybook 8 with Vite builder instead of Webpack
      rationale: Faster build times, better Next.js 16 compatibility, improved HMR
        performance critical for visual component iteration
      alternatives_considered:
      - Webpack builder
      - Custom dev environment
      - Ladle
      ai_implementation_note: AI can generate most Vite configuration boilerplate
        and migration scripts
    - decision: Implement shared Storybook instance for both apps with workspace-aware
        story organization
      rationale: Enables component reuse visualization, consistent design system documentation,
        single visual regression pipeline
      alternatives_considered:
      - Separate Storybook per app
      - Component library only approach
      ai_implementation_note: AI excellent at generating workspace configuration and
        story structure templates
    - decision: Integrate MSW for API mocking with realistic comic/novel data fixtures
      rationale: Enables realistic component states without backend dependency, critical
        for comic panel and reading components
      alternatives_considered:
      - Static mock data
      - Backend dev environment dependency
      ai_implementation_note: AI can generate comprehensive mock handlers and realistic
        fixture data
    researched_at: '2026-02-08T18:23:05.578981'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:05:42.943161'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 24044f96
    planning_hash: e847d6aa
  technical_notes:
    approach: 'Set up Storybook 8 with Next.js framework support in the monorepo root,
      configuring it to discover stories from both dashboard and storefront apps.
      Use Vite builder for performance, integrate essential addons for component development,
      and establish MSW for realistic API mocking. Create a structured approach to
      story organization with clear naming conventions and comprehensive coverage
      of component states, particularly focusing on comic/novel display components
      that require visual precision.

      '
    external_dependencies:
    - name: '@storybook/nextjs'
      version: ^8.0.0
      reason: Official Next.js framework integration with App Router support
    - name: '@storybook/addon-essentials'
      version: ^8.0.0
      reason: Core addons bundle (controls, actions, viewport, docs)
    - name: '@storybook/test'
      version: ^8.0.0
      reason: Built-in testing utilities and interaction testing
    - name: msw-storybook-addon
      version: ^2.0.0
      reason: Mock Service Worker integration for API mocking
    - name: chromatic
      version: ^10.0.0
      reason: Visual regression testing and component review workflow
    files_to_modify:
    - path: package.json
      changes: Add Storybook 8 dependencies and scripts
    - path: turbo.json
      changes: Add storybook build pipeline and dependencies
    - path: apps/dashboard/package.json
      changes: Add story-specific dependencies and scripts
    - path: apps/storefront/package.json
      changes: Add story-specific dependencies and scripts
    new_files:
    - path: .storybook/main.ts
      purpose: Core Storybook configuration with Next.js 16 and monorepo support
    - path: .storybook/preview.ts
      purpose: Global decorators, parameters, and theme configuration
    - path: .storybook/middleware.js
      purpose: MSW integration for API mocking
    - path: .storybook/manager.js
      purpose: Storybook UI customization and branding
    - path: apps/dashboard/stories/components/Button.stories.ts
      purpose: Example story template for dashboard components
    - path: apps/storefront/stories/components/BookCard.stories.ts
      purpose: Example story template for storefront components
    - path: .storybook/mocks/handlers.ts
      purpose: MSW request handlers for component API mocking
    - path: .storybook/decorators/index.ts
      purpose: Reusable story decorators for themes, layouts, etc.
  acceptance_criteria:
  - criterion: Storybook 8 builds and serves successfully for both dashboard and storefront
      components
    verification: Run `npm run storybook` and verify http://localhost:6006 loads with
      stories from both apps
  - criterion: Essential addons (Controls, Actions, Viewport, A11y, Docs) are functional
    verification: Navigate to any component story and verify all addon panels are
      present and interactive
  - criterion: MSW integration provides realistic API mocking for components
    verification: Check component stories that depend on API data render with mock
      responses
  - criterion: Visual regression testing is configured and operational
    verification: Run `npm run chromatic` or equivalent and verify snapshots are captured
  - criterion: TypeScript support with CSF 3.0 provides type safety in stories
    verification: Stories have proper TypeScript intellisense and compile without
      errors
  testing:
    unit_tests:
    - file: .storybook/__tests__/config.test.ts
      coverage_target: 90%
      scenarios:
      - Storybook configuration loads correctly
      - Addon registration works
      - Story discovery from multiple apps
    - file: apps/dashboard/stories/__tests__/story-validation.test.ts
      coverage_target: 85%
      scenarios:
      - All dashboard components have stories
      - Stories render without errors
      - Required story variants exist
    integration_tests:
    - file: .storybook/__tests__/integration/build.test.ts
      scenarios:
      - Storybook builds successfully for production
      - Static build generates all story files
      - MSW handlers integrate properly with stories
    manual_testing:
    - step: Navigate to component stories and interact with Controls addon
      expected: Component props update in real-time
    - step: Test responsive behavior using Viewport addon
      expected: Components adapt to different screen sizes
    - step: Verify accessibility warnings in A11y addon
      expected: Accessibility issues are highlighted and actionable
  estimates:
    development: 1.5
    code_review: 0.5
    testing: 0.8
    documentation: 0.2
    total: 3
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Install Storybook 8 dependencies and initialize configuration'
      done: false
      ai_friendly: true
    - task: '[AI] Configure main.ts with Next.js framework and addon setup'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define story organization strategy and naming conventions'
      done: false
      ai_friendly: false
    - task: '[AI] Set up preview.ts with global decorators and Tailwind integration'
      done: false
      ai_friendly: true
    - task: '[AI] Configure MSW integration and create basic API handlers'
      done: false
      ai_friendly: true
    - task: '[AI] Create example stories for key components (Button, BookCard, etc.)'
      done: false
      ai_friendly: true
    - task: '[AI] Set up Chromatic or Percy for visual regression testing'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review story coverage and component interaction patterns'
      done: false
      ai_friendly: false
    - task: '[AI] Configure turbo.json pipeline integration and build optimization'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive test suite for Storybook configuration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Validate visual design consistency and component behavior'
      done: false
      ai_friendly: false
