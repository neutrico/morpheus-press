milestone: M1 - Backend Services
task_count: 19
issues:
- key: T23
  title: Backend TypeScript Port
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 8
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task appears to involve porting an existing backend from JavaScript to
      TypeScript, or establishing a TypeScript-first backend architecture for Morpheus.
      Given the project uses Fastify 5 + TypeScript, this likely involves ensuring
      full type safety across the backend services, implementing proper TypeScript
      configurations, and establishing patterns for type-safe API development. This
      is foundational for the novel-to-comic transformation platform''s reliability
      and maintainability.


      **Technical Approach:**

      - Use Fastify 5 with TypeScript for type-safe route definitions and schema validation

      - Implement strict TypeScript configuration with noImplicitAny, strictNullChecks

      - Use Supabase TypeScript client for database operations with generated types

      - Establish type-safe API contracts using JSON Schema with TypeScript inference

      - Implement proper error handling with typed error responses

      - Use dependency injection patterns for testability


      **AI Suitability Analysis:**

      - High AI effectiveness: Type definitions, schema generation, CRUD operations,
      test boilerplate, migration scripts, validation logic

      - Medium AI effectiveness: Service layer refactoring, API route implementations,
      database query optimization

      - Low AI effectiveness: Architecture decisions for service boundaries, complex
      business logic patterns, performance optimization strategies


      **Dependencies:**

      - External: @fastify/type-provider-typebox, @supabase/supabase-js, zod, @types/node

      - Internal: Database schema types, shared validation schemas, authentication
      middleware


      **Risks:**

      - Type complexity explosion: Mitigate with utility types and modular type definitions

      - Performance overhead from excessive type checking: Use build-time validation
      where possible

      - Breaking changes during migration: Implement gradual migration with hybrid
      JS/TS support

      - AI generating overly complex types: Establish clear typing patterns and guidelines


      **Complexity Notes:**

      This task is moderately complex due to the need for architectural consistency
      across the platform. AI can significantly accelerate implementation velocity
      for boilerplate and type definitions, but human oversight is critical for establishing
      maintainable patterns. The complexity increases if migrating existing JavaScript
      code vs. greenfield TypeScript development.


      **Key Files:**

      - packages/api/src/types/: Core type definitions

      - packages/api/src/routes/: Typed route handlers

      - packages/api/src/services/: Business logic services

      - packages/api/tsconfig.json: TypeScript configuration

      - packages/shared/src/types/: Cross-package type definitions

      '
    design_decisions:
    - decision: Use TypeBox for runtime schema validation with TypeScript inference
      rationale: Provides single source of truth for validation and types, integrates
        well with Fastify
      alternatives_considered:
      - Zod validation
      - JSON Schema only
      - Joi validation
      ai_implementation_note: AI can generate comprehensive schema definitions and
        corresponding types efficiently
    - decision: Generate Supabase types from database schema
      rationale: Ensures type safety for database operations and catches schema changes
        at compile time
      alternatives_considered:
      - Manual type definitions
      - ORM with code-first approach
      ai_implementation_note: AI can help with type generation scripts and database
        query implementations
    - decision: Implement strict TypeScript configuration with incremental adoption
      rationale: Balances type safety with migration practicality for existing codebase
      alternatives_considered:
      - Loose TypeScript config
      - All-or-nothing migration
      ai_implementation_note: AI can assist with gradual type annotation and error
        resolution
    researched_at: '2026-02-08T18:23:28.262525'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:06:12.244589'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 16fa10dc
    planning_hash: 03f0d5a7
  technical_notes:
    approach: 'Establish a type-safe Fastify backend using TypeBox for schema validation
      and type inference. Generate database types from Supabase schema and implement
      service layers with proper dependency injection. Create shared type definitions
      for cross-package consistency. Use AI for boilerplate generation while maintaining
      human oversight for architectural decisions and complex business logic patterns.

      '
    external_dependencies:
    - name: '@fastify/type-provider-typebox'
      version: ^4.0.0
      reason: Runtime schema validation with TypeScript type inference
    - name: '@sinclair/typebox'
      version: ^0.32.0
      reason: JSON Schema with TypeScript support for Fastify integration
    - name: supabase
      version: ^2.38.0
      reason: Type-safe database client with generated types
    - name: '@types/node'
      version: ^20.0.0
      reason: Node.js type definitions for TypeScript
    files_to_modify:
    - path: packages/api/tsconfig.json
      changes: Configure strict TypeScript settings with noImplicitAny, strictNullChecks,
        noImplicitReturns
    - path: packages/api/package.json
      changes: Add TypeBox, Supabase TypeScript client, and type generation scripts
    - path: packages/api/src/app.ts
      changes: Configure Fastify with TypeBox type provider and error serialization
    new_files:
    - path: packages/api/src/types/database.ts
      purpose: Generated database types from Supabase schema
    - path: packages/api/src/types/api.ts
      purpose: API request/response type definitions and schemas
    - path: packages/api/src/types/errors.ts
      purpose: Standardized error response types and error codes
    - path: packages/api/src/schemas/novel.schema.ts
      purpose: TypeBox schemas for novel-related API endpoints
    - path: packages/api/src/schemas/user.schema.ts
      purpose: TypeBox schemas for user authentication and profile endpoints
    - path: packages/api/src/services/base.service.ts
      purpose: Generic typed service base class with common CRUD operations
    - path: packages/api/src/routes/novels.ts
      purpose: Type-safe novel management API routes
    - path: packages/api/src/routes/users.ts
      purpose: Type-safe user management API routes
    - path: packages/api/src/middleware/validation.ts
      purpose: Request validation middleware with TypeBox integration
    - path: packages/api/src/utils/type-helpers.ts
      purpose: Utility types and type guards for common patterns
    - path: packages/shared/src/types/index.ts
      purpose: Cross-package shared type definitions
    - path: packages/api/scripts/generate-db-types.ts
      purpose: Script to generate TypeScript types from Supabase schema
  acceptance_criteria:
  - criterion: All backend API routes are fully type-safe with TypeScript interfaces
      and schema validation
    verification: Run `npm run type-check` in backend package with zero TypeScript
      errors and test API endpoints with invalid payloads
  - criterion: Database operations use generated TypeScript types from Supabase schema
      with compile-time safety
    verification: Execute `npm run db:generate-types` and verify no type errors in
      service layer database calls
  - criterion: Shared type definitions are available across packages with proper import/export
      structure
    verification: Import shared types in frontend package and verify IntelliSense
      and compilation works without errors
  - criterion: Error handling follows typed error response patterns with consistent
      HTTP status codes
    verification: Test error scenarios return properly typed error objects matching
      defined error schema interfaces
  - criterion: Development experience includes auto-completion, type hints, and catch
      compile-time errors
    verification: Open VS Code, modify API route parameters, and verify TypeScript
      errors appear immediately with helpful suggestions
  testing:
    unit_tests:
    - file: packages/api/src/__tests__/types/schemas.test.ts
      coverage_target: 90%
      scenarios:
      - Schema validation passes for valid inputs
      - Schema validation rejects invalid inputs with descriptive errors
      - Type inference works correctly from schemas
    - file: packages/api/src/__tests__/services/user.service.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations with proper type safety
      - Database constraint violations handled with typed errors
      - Service methods return correct TypeScript types
    - file: packages/api/src/__tests__/routes/novels.test.ts
      coverage_target: 85%
      scenarios:
      - Route handlers accept typed request bodies
      - Response bodies match defined schemas
      - Authentication middleware preserves type information
    integration_tests:
    - file: packages/api/src/__tests__/integration/api-contracts.test.ts
      scenarios:
      - End-to-end API calls maintain type safety from request to database
      - Cross-service communication uses shared types correctly
      - Error propagation maintains type information through layers
    manual_testing:
    - step: Start development server and make API calls with Postman/curl using invalid
        payloads
      expected: Receive properly formatted error responses with type-safe error codes
        and messages
    - step: Modify a type definition and observe compile-time errors in dependent
        files
      expected: TypeScript compiler catches breaking changes immediately with helpful
        error messages
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.55
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup TypeScript configuration with strict settings and required
        dependencies'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define architecture patterns for service layers and dependency
        injection'
      done: false
      ai_friendly: false
    - task: '[AI] Generate database types from Supabase schema and create type generation
        script'
      done: false
      ai_friendly: true
    - task: '[AI] Create TypeBox schemas for API request/response validation'
      done: false
      ai_friendly: true
    - task: '[AI] Implement typed service base classes with generic CRUD operations'
      done: false
      ai_friendly: true
    - task: '[AI] Convert existing API routes to use TypeBox type provider and schemas'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review type complexity and establish conventions for maintainable
        type definitions'
      done: false
      ai_friendly: false
    - task: '[AI] Create comprehensive error handling with typed error responses'
      done: false
      ai_friendly: true
    - task: '[AI] Generate unit tests for all typed services and route handlers'
      done: false
      ai_friendly: true
    - task: '[AI] Write integration tests for end-to-end type safety'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Code review focusing on type safety patterns and performance
        implications'
      done: false
      ai_friendly: false
- key: T24
  title: Supabase Database Setup with RLS
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 5
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task establishes the foundational database layer for Morpheus, implementing
      Supabase with PostgreSQL and Row Level Security (RLS). This is critical for
      M1 as it provides secure, multi-tenant data access patterns that will support
      user authentication, novel management, comic generation tracking, and billing.
      RLS is essential for a SaaS platform where users must only access their own
      data, and it reduces backend security complexity by enforcing data isolation
      at the database level.


      **Technical Approach:**

      - Use Supabase CLI for local development and migration management

      - Implement RLS policies for core entities: users, novels, comics, generations,
      subscriptions

      - Create database migrations with proper indexing for performance

      - Set up TypeScript types generation from database schema

      - Implement connection pooling and prepared statements via Supabase client

      - Use Supabase''s built-in auth integration with RLS user context

      - Create seed data scripts for development/testing environments


      **AI Suitability Analysis:**

      - High AI effectiveness: Migration SQL files, RLS policy syntax, TypeScript
      type definitions, seed data scripts, basic CRUD operations, test fixtures

      - Medium AI effectiveness: Performance optimization queries, complex join policies,
      integration with Fastify middleware

      - Low AI effectiveness: Database schema design decisions, RLS policy architecture,
      security model design, performance bottleneck identification


      **Dependencies:**

      - External: @supabase/supabase-js (^2.38.0), supabase CLI, pg types

      - Internal: Backend authentication middleware, user session management, API
      route handlers

      - Environment: Supabase project setup, environment variables configuration


      **Risks:**

      - RLS policy complexity: Start simple, iterate with automated tests for policy
      enforcement

      - Performance with RLS overhead: Implement proper indexing strategy, monitor
      query performance

      - Migration rollback complexity: Use transactional migrations, test rollbacks
      in staging

      - Type safety drift: Automate type generation in CI/CD pipeline


      **Complexity Notes:**

      This is foundational work that''s moderately complex due to security requirements
      but highly AI-assistable for implementation details. RLS policies require careful
      human design but AI can generate the SQL syntax efficiently. The task should
      move faster than estimated due to AI assistance with boilerplate and Supabase''s
      excellent tooling.


      **Key Files:**

      - supabase/migrations/: All database schema and RLS policy files

      - packages/database/: Database client wrapper and types

      - packages/backend/src/lib/supabase.ts: Supabase client configuration

      - packages/backend/src/middleware/auth.ts: RLS context middleware

      '
    design_decisions:
    - decision: Use Supabase hosted service over self-hosted PostgreSQL
      rationale: Reduces operational overhead, provides built-in auth integration,
        excellent RLS tooling, and real-time capabilities for future features
      alternatives_considered:
      - Self-hosted PostgreSQL
      - PlanetScale
      - Neon
      ai_implementation_note: AI can generate all Supabase client configuration and
        connection handling code
    - decision: Implement RLS at database level rather than application middleware
      rationale: Provides defense in depth, reduces code complexity, leverages PostgreSQL's
        battle-tested security model
      alternatives_considered:
      - Application-level authorization
      - API Gateway policies
      ai_implementation_note: AI can generate RLS policies from human-defined access
        patterns and test cases
    - decision: Generate TypeScript types from database schema automatically
      rationale: Ensures type safety, reduces manual maintenance, catches schema changes
        early
      alternatives_considered:
      - Manual type definitions
      - Runtime validation only
      ai_implementation_note: AI can set up type generation pipeline and create utility
        types for common patterns
    researched_at: '2026-02-08T18:23:55.096399'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:06:37.836198'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: '25592410'
    planning_hash: 1c6cf854
  technical_notes:
    approach: 'Set up Supabase project with local development environment using Docker.
      Create migration files for core schema (users, novels, comics, generations)
      with appropriate indexes. Implement RLS policies that use auth.uid() for user
      isolation and role-based access for admin functions. Configure TypeScript client
      with generated types and connection pooling. Create comprehensive test suite
      for RLS policies and database operations.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.38.0
      reason: Official Supabase client for TypeScript with full feature support
    - name: supabase
      version: ^1.110.0
      reason: CLI tool for local development, migrations, and type generation
    - name: '@types/pg'
      version: ^8.10.0
      reason: PostgreSQL type definitions for enhanced type safety
    files_to_modify:
    - path: packages/backend/src/lib/supabase.ts
      changes: Add Supabase client configuration with connection pooling
    - path: packages/backend/src/middleware/auth.ts
      changes: Integrate Supabase auth context for RLS
    - path: package.json
      changes: Add Supabase dependencies and database scripts
    new_files:
    - path: supabase/config.toml
      purpose: Supabase project configuration for local development
    - path: supabase/migrations/20240101000001_initial_schema.sql
      purpose: Core database schema with users, novels, comics, generations tables
    - path: supabase/migrations/20240101000002_rls_policies.sql
      purpose: Row Level Security policies for all tables
    - path: supabase/migrations/20240101000003_indexes.sql
      purpose: Performance indexes for common query patterns
    - path: supabase/seed.sql
      purpose: Development seed data for testing
    - path: packages/database/src/client.ts
      purpose: Supabase client wrapper with TypeScript types
    - path: packages/database/src/types.ts
      purpose: Generated TypeScript types from database schema
    - path: packages/database/src/operations/users.ts
      purpose: User CRUD operations with RLS
    - path: packages/database/src/operations/novels.ts
      purpose: Novel CRUD operations with user context
    - path: packages/database/src/operations/comics.ts
      purpose: Comic CRUD operations with user context
    - path: packages/database/src/operations/generations.ts
      purpose: Generation tracking operations
    - path: packages/database/package.json
      purpose: Database package dependencies and scripts
    - path: scripts/db-setup.sh
      purpose: Database setup and migration scripts
  acceptance_criteria:
  - criterion: Supabase project configured with local development environment and
      all core tables created
    verification: Run `supabase status` shows all services running, migrations applied
      successfully
  - criterion: RLS policies enforce user data isolation - users can only access their
      own novels, comics, and generations
    verification: Execute RLS policy test suite with multiple test users, verify cross-user
      data access is blocked
  - criterion: TypeScript types auto-generated from database schema and integrated
      with backend services
    verification: Run `npm run db:types` generates types, backend compiles without
      type errors
  - criterion: Database operations achieve <100ms response time for single-record
      queries
    verification: Load test with 100 concurrent users, measure query performance in
      Supabase dashboard
  - criterion: Complete database API layer with CRUD operations for all core entities
    verification: Integration tests pass for users, novels, comics, generations endpoints
  testing:
    unit_tests:
    - file: packages/database/src/__tests__/client.test.ts
      coverage_target: 90%
      scenarios:
      - Database client initialization
      - Connection pooling configuration
      - Error handling for connection failures
      - Type safety validation
    - file: packages/database/src/__tests__/rls-policies.test.ts
      coverage_target: 95%
      scenarios:
      - User isolation policies work correctly
      - Admin access policies function properly
      - Unauthorized access is blocked
      - Policy performance under load
    integration_tests:
    - file: packages/backend/src/__tests__/integration/database.test.ts
      scenarios:
      - End-to-end CRUD operations through API
      - Multi-user data isolation verification
      - Authentication context propagation to RLS
      - Migration rollback and recovery
    manual_testing:
    - step: Create test users and verify data isolation in Supabase dashboard
      expected: Users can only see their own records in all tables
    - step: Monitor query performance during load testing
      expected: All queries complete within performance targets
  estimates:
    development: 2.5
    code_review: 1
    testing: 0.8
    documentation: 0.3
    total: 4.6
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Initialize Supabase project and generate config.toml with standard
        settings'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design database schema and RLS policy architecture for multi-tenant
        SaaS'
      done: false
      ai_friendly: false
    - task: '[AI] Generate initial migration SQL files for core tables (users, novels,
        comics, generations)'
      done: false
      ai_friendly: true
    - task: '[AI] Implement RLS policies SQL based on approved schema design'
      done: false
      ai_friendly: true
    - task: '[AI] Create performance indexes migration for common query patterns'
      done: false
      ai_friendly: true
    - task: '[AI] Generate TypeScript database client wrapper with connection pooling'
      done: false
      ai_friendly: true
    - task: '[AI] Create CRUD operation modules for each entity with proper typing'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive test suites for RLS policies and database
        operations'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review RLS policies for security vulnerabilities and performance
        impact'
      done: false
      ai_friendly: false
    - task: '[AI] Create seed data scripts and development setup automation'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Validate end-to-end security and performance under realistic
        load'
      done: false
      ai_friendly: false
- key: T25
  title: API Routes Implementation
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 5
  area: backend
  dependsOn:
  - T23
  - T24
  agent_notes:
    research_findings: '**Context:**

      This task involves implementing the core API routes for the Morpheus backend
      service using Fastify 5. These routes will handle novel-to-comic transformation
      workflows, user management, project CRUD operations, and integration with ML
      services (OpenAI/Anthropic for text processing, RunPod for image generation).
      This is foundational infrastructure that enables the frontend dashboard and
      storefront to function, making it a critical M1 deliverable.


      **Technical Approach:**

      - Use Fastify 5''s native TypeScript support with strict type definitions

      - Implement route schemas using Fastify''s JSON Schema validation

      - Follow RESTful conventions with proper HTTP status codes

      - Use Fastify plugins for cross-cutting concerns (auth, CORS, rate limiting)

      - Implement request/response DTOs with Zod for runtime validation

      - Use Fastify''s built-in serialization for consistent JSON responses

      - Implement OpenAPI/Swagger documentation generation

      - Follow repository pattern for database operations with Supabase client

      - Use Fastify hooks for logging, error handling, and request preprocessing


      **AI Suitability Analysis:**

      - High AI effectiveness: CRUD route implementations, request/response schemas,
      basic validation logic, OpenAPI documentation generation, test case creation

      - Medium AI effectiveness: Authentication middleware integration, error handling
      patterns, database query optimization

      - Low AI effectiveness: API design decisions, security strategy, ML service
      integration patterns, performance optimization strategies


      **Dependencies:**

      - External: @fastify/swagger, @fastify/swagger-ui, @fastify/cors, @fastify/rate-limit,
      @fastify/jwt, zod, @supabase/supabase-js

      - Internal: Database schema definitions, authentication service, ML service
      adapters, shared TypeScript types


      **Risks:**

      - API versioning strategy: Implement v1 prefix and plan for backwards compatibility

      - Rate limiting configuration: Start conservative, monitor usage patterns

      - Schema evolution: Use additive changes only, plan migration strategy for breaking
      changes

      - Error handling consistency: Define standard error response format early

      - Authentication token management: Implement proper JWT refresh strategy


      **Complexity Notes:**

      Initially seems straightforward, but complexity increases with ML service integrations
      and async job processing for comic generation. AI agents excel at generating
      boilerplate routes but require careful human oversight for security and performance
      considerations. Estimate 30% velocity increase with AI assistance for route
      generation and testing.


      **Key Files:**

      - apps/backend/src/routes/: Main route definitions

      - apps/backend/src/schemas/: Request/response schemas

      - apps/backend/src/plugins/: Fastify plugins for auth, validation

      - apps/backend/src/types/: Shared TypeScript interfaces

      - apps/backend/src/services/: Business logic layer

      '
    design_decisions:
    - decision: Use Fastify's built-in JSON Schema validation over external libraries
      rationale: Better performance, native TypeScript support, automatic OpenAPI
        generation
      alternatives_considered:
      - Joi validation
      - Yup schemas
      - Pure Zod validation
      ai_implementation_note: AI can generate comprehensive JSON schemas from TypeScript
        interfaces
    - decision: Implement async job processing for ML operations with immediate response
        + webhook/polling
      rationale: ML operations (comic generation) take 30+ seconds, need non-blocking
        API design
      alternatives_considered:
      - Synchronous long-polling
      - Server-sent events
      - WebSocket connections
      ai_implementation_note: AI can generate job queue integration code and status
        tracking endpoints
    - decision: Use repository pattern with Supabase client abstraction
      rationale: Separates database logic, enables easier testing, maintains clean
        architecture
      alternatives_considered:
      - Direct Supabase client usage
      - ORM layer
      - Query builder pattern
      ai_implementation_note: AI excels at generating repository implementations from
        database schemas
    researched_at: '2026-02-08T18:24:19.933059'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:07:06.658141'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: fa50458f
    planning_hash: 5557da09
  technical_notes:
    approach: 'Implement RESTful API routes using Fastify 5 with TypeScript, focusing
      on novel processing (/novels), comic generation (/comics), user management (/users),
      and project operations (/projects). Use JSON Schema validation for all endpoints,
      implement JWT authentication middleware, and create async job processing for
      ML operations. Structure routes as Fastify plugins for modularity and testability.

      '
    external_dependencies:
    - name: '@fastify/swagger'
      version: ^8.0.0
      reason: OpenAPI documentation generation
    - name: '@fastify/swagger-ui'
      version: ^2.0.0
      reason: Interactive API documentation interface
    - name: '@fastify/cors'
      version: ^9.0.0
      reason: Cross-origin resource sharing for frontend integration
    - name: '@fastify/rate-limit'
      version: ^9.0.0
      reason: API rate limiting to prevent abuse
    - name: '@fastify/jwt'
      version: ^7.0.0
      reason: JWT token handling for authentication
    - name: zod
      version: ^3.22.0
      reason: Runtime type validation and DTO definitions
    - name: bullmq
      version: ^4.0.0
      reason: Job queue for async ML processing tasks
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register route plugins and global error handler
    - path: apps/backend/package.json
      changes: Add dependencies for Fastify plugins and validation
    new_files:
    - path: apps/backend/src/routes/novels.ts
      purpose: Novel upload, processing, and management endpoints
    - path: apps/backend/src/routes/comics.ts
      purpose: Comic generation, status, and retrieval endpoints
    - path: apps/backend/src/routes/users.ts
      purpose: User profile and preference management
    - path: apps/backend/src/routes/projects.ts
      purpose: Project CRUD operations and collaboration
    - path: apps/backend/src/routes/health.ts
      purpose: Health check and system status endpoints
    - path: apps/backend/src/schemas/novels.ts
      purpose: Novel request/response Zod schemas
    - path: apps/backend/src/schemas/comics.ts
      purpose: Comic generation Zod schemas
    - path: apps/backend/src/schemas/users.ts
      purpose: User management Zod schemas
    - path: apps/backend/src/schemas/projects.ts
      purpose: Project operation Zod schemas
    - path: apps/backend/src/schemas/common.ts
      purpose: Shared validation schemas and error types
    - path: apps/backend/src/plugins/auth.ts
      purpose: JWT authentication Fastify plugin
    - path: apps/backend/src/plugins/validation.ts
      purpose: Request/response validation plugin
    - path: apps/backend/src/plugins/swagger.ts
      purpose: OpenAPI documentation configuration
    - path: apps/backend/src/plugins/cors.ts
      purpose: CORS configuration for frontend access
    - path: apps/backend/src/plugins/rate-limit.ts
      purpose: API rate limiting configuration
    - path: apps/backend/src/types/api.ts
      purpose: API-specific TypeScript interfaces
    - path: apps/backend/src/types/jobs.ts
      purpose: Async job processing types
    - path: apps/backend/src/services/novel-processor.ts
      purpose: Novel text processing business logic
    - path: apps/backend/src/services/comic-generator.ts
      purpose: Comic generation workflow orchestration
    - path: apps/backend/src/services/job-queue.ts
      purpose: Async job management service
    - path: apps/backend/src/utils/error-handler.ts
      purpose: Centralized error handling utilities
    - path: apps/backend/src/utils/response-formatter.ts
      purpose: Consistent API response formatting
  acceptance_criteria:
  - criterion: All API routes return proper HTTP status codes and JSON responses with
      consistent error format
    verification: curl commands for each endpoint verify 200/201/400/401/404/500 responses
      match schema
  - criterion: Request/response validation works with proper error messages for invalid
      data
    verification: Postman collection tests invalid payloads return 400 with descriptive
      error messages
  - criterion: JWT authentication protects secured endpoints and allows public access
      to health/docs
    verification: Test suite verifies 401 for missing/invalid tokens, 200 for valid
      tokens
  - criterion: OpenAPI documentation is auto-generated and matches actual route implementations
    verification: Visit /docs endpoint, verify all routes documented with correct
      schemas
  - criterion: Comic generation workflow handles async processing with job status
      tracking
    verification: POST /comics returns job ID, GET /comics/{id}/status shows progress/completion
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/routes/novels.test.ts
      coverage_target: 90%
      scenarios:
      - Novel upload and text extraction
      - Invalid file format handling
      - Authentication required scenarios
    - file: apps/backend/src/__tests__/routes/comics.test.ts
      coverage_target: 90%
      scenarios:
      - Comic creation job initiation
      - Status tracking
      - Generation completion webhook
    - file: apps/backend/src/__tests__/routes/users.test.ts
      coverage_target: 85%
      scenarios:
      - User profile CRUD operations
      - Authorization checks
      - Data validation
    - file: apps/backend/src/__tests__/schemas/validation.test.ts
      coverage_target: 95%
      scenarios:
      - Request schema validation
      - Response serialization
      - Edge case data types
    integration_tests:
    - file: apps/backend/src/__tests__/integration/auth-flow.test.ts
      scenarios:
      - Full authentication workflow with JWT
    - file: apps/backend/src/__tests__/integration/comic-pipeline.test.ts
      scenarios:
      - Novel upload to comic generation complete flow
    manual_testing:
    - step: Import Postman collection and run full test suite
      expected: All tests pass with proper response schemas
    - step: Load /docs page and verify API documentation completeness
      expected: All routes documented with examples and schemas
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup Fastify plugins and dependencies in package.json'
      done: false
      ai_friendly: true
    - task: '[AI] Create common schemas and TypeScript types for API contracts'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design authentication strategy and JWT token structure'
      done: false
      ai_friendly: false
    - task: '[AI] Implement authentication plugin with JWT validation'
      done: false
      ai_friendly: true
    - task: '[AI] Generate CRUD routes for novels with full validation schemas'
      done: false
      ai_friendly: true
    - task: '[AI] Generate CRUD routes for comics with async job handling'
      done: false
      ai_friendly: true
    - task: '[AI] Generate user management and project routes'
      done: false
      ai_friendly: true
    - task: '[AI] Create comprehensive unit tests for all route handlers'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure rate limiting and security middleware settings'
      done: false
      ai_friendly: false
    - task: '[AI] Setup OpenAPI documentation with Swagger UI integration'
      done: false
      ai_friendly: true
    - task: '[AI] Create Postman collection for manual testing'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review security implementation and error handling patterns'
      done: false
      ai_friendly: false
    - task: '[AI] Write integration tests for complete workflows'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Performance testing and optimization review'
      done: false
      ai_friendly: false
- key: T26
  title: Authentication Integration
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 3
  area: backend
  dependsOn:
  - T24
  agent_notes:
    research_findings: '**Context:**

      Authentication integration is the foundational security layer for Morpheus,
      enabling user registration, login, session management, and role-based access
      control across the platform. This is critical for M1 as it gates access to novel-to-comic
      transformation features, user-generated content, and premium services. Without
      robust auth, we cannot implement user-specific features like saved projects,
      payment processing, or content ownership tracking.


      **Technical Approach:**

      Leverage Supabase Auth as the primary authentication provider, integrating with
      our Fastify backend through @supabase/supabase-js. Implement JWT-based session
      management with role-based access control (RBAC) supporting roles like ''user'',
      ''premium'', ''admin''. Create auth middleware for route protection, user context
      injection, and automated session refresh. Design auth schemas in PostgreSQL
      for user profiles, roles, and permissions that extend Supabase''s auth.users
      table.


      **AI Suitability Analysis:**

      - High AI effectiveness: Auth middleware boilerplate, JWT validation logic,
      CRUD operations for user profiles, test suites for auth flows, TypeScript interfaces
      for auth payloads

      - Medium AI effectiveness: Supabase client configuration, password reset flows,
      role-based middleware, integration with existing routes

      - Low AI effectiveness: Security architecture decisions, auth flow design, token
      expiration strategies, RBAC permission matrix design


      **Dependencies:**

      - External: @supabase/supabase-js, @fastify/jwt, @fastify/cookie, bcrypt, zod
      (validation)

      - Internal: Database schemas, user service, middleware system, error handling
      utilities


      **Risks:**

      - Token security: Implement proper JWT storage, rotation, and httpOnly cookies
      for web clients

      - Session management: Design robust refresh token flows to prevent auth state
      inconsistencies

      - Rate limiting: Protect auth endpoints from brute force attacks with @fastify/rate-limit

      - CORS configuration: Ensure proper cross-origin setup between Next.js frontend
      and Fastify backend

      - Role escalation: Carefully validate role assignments and prevent unauthorized
      privilege escalation


      **Complexity Notes:**

      Medium complexity task that becomes highly AI-accelerated once architecture
      decisions are made. AI can generate 70-80% of implementation code including
      middleware, validation schemas, and comprehensive test coverage. Human oversight
      critical for security review and flow design.


      **Key Files:**

      - packages/backend/src/plugins/auth.ts: Supabase client setup and JWT configuration

      - packages/backend/src/middleware/auth.ts: Authentication middleware for route
      protection

      - packages/backend/src/routes/auth.ts: Auth endpoints (login, register, refresh,
      logout)

      - packages/backend/src/types/auth.ts: TypeScript interfaces for auth payloads

      - packages/backend/src/services/user.ts: User profile management service

      - supabase/migrations/: Database schemas for user profiles and roles

      '
    design_decisions:
    - decision: Use Supabase Auth as primary authentication provider
      rationale: Already committed to Supabase ecosystem, provides enterprise-grade
        security, handles OAuth providers, reduces custom auth complexity
      alternatives_considered:
      - Auth0
      - Firebase Auth
      - Custom JWT implementation
      ai_implementation_note: AI can generate Supabase client configuration, wrapper
        functions, and integration patterns from documentation
    - decision: Implement JWT-based session management with refresh tokens
      rationale: Stateless authentication scales well, integrates cleanly with Next.js
        frontend, supports mobile clients for future expansion
      alternatives_considered:
      - Server-side sessions
      - Supabase session management only
      ai_implementation_note: AI excellent at generating JWT middleware, token validation
        logic, and refresh flows following established patterns
    - decision: Role-Based Access Control (RBAC) with PostgreSQL role storage
      rationale: Flexible permission system for different user tiers (free, premium,
        admin), extensible for future features like content creator roles
      alternatives_considered:
      - Simple boolean flags
      - Attribute-based access control
      ai_implementation_note: AI can generate role checking middleware and database
        queries, but requires human design of permission matrix
    researched_at: '2026-02-08T18:24:47.909598'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:07:32.484128'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: d97bfc9e
    planning_hash: 1b601310
  technical_notes:
    approach: 'Set up Supabase client in Fastify with JWT plugin configuration for
      token validation. Create authentication middleware that extracts and validates
      JWTs, injects user context into request objects, and handles refresh token rotation.
      Implement auth routes for registration, login, logout, and profile management
      that integrate with Supabase Auth APIs. Design PostgreSQL schemas extending
      Supabase''s auth.users with application-specific user profiles and role assignments.
      Build comprehensive test coverage for all auth flows including edge cases and
      security scenarios.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.38.0
      reason: Primary Supabase client for auth operations and database access
    - name: '@fastify/jwt'
      version: ^8.0.0
      reason: JWT token generation, validation, and middleware for Fastify
    - name: '@fastify/cookie'
      version: ^9.2.0
      reason: Secure cookie handling for refresh tokens and session management
    - name: '@fastify/rate-limit'
      version: ^9.1.0
      reason: Rate limiting for auth endpoints to prevent brute force attacks
    - name: zod
      version: ^3.22.0
      reason: Runtime validation for auth payloads and user input sanitization
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register auth plugin, JWT plugin, and auth middleware
    - path: packages/backend/src/routes/index.ts
      changes: Mount auth routes and apply auth middleware to protected routes
    new_files:
    - path: packages/backend/src/plugins/auth.ts
      purpose: Supabase client setup, JWT configuration, and auth plugin registration
    - path: packages/backend/src/middleware/auth.ts
      purpose: JWT validation, user context injection, role-based access control middleware
    - path: packages/backend/src/routes/auth.ts
      purpose: 'Authentication endpoints: register, login, logout, refresh, profile'
    - path: packages/backend/src/types/auth.ts
      purpose: TypeScript interfaces for auth payloads, user context, JWT claims
    - path: packages/backend/src/services/user.ts
      purpose: User profile management, role assignment, profile CRUD operations
    - path: packages/backend/src/utils/auth-helpers.ts
      purpose: JWT token utilities, password hashing, validation helpers
    - path: supabase/migrations/20241201000001_user_profiles.sql
      purpose: User profiles table extending Supabase auth.users
    - path: supabase/migrations/20241201000002_user_roles.sql
      purpose: Role-based access control tables and policies
    - path: packages/backend/src/config/supabase.ts
      purpose: Supabase client configuration and connection setup
  acceptance_criteria:
  - criterion: Users can register, login, and logout with JWT-based session management
    verification: POST /api/auth/register, POST /api/auth/login, POST /api/auth/logout
      return proper status codes and JWT tokens
  - criterion: Protected routes enforce authentication and inject user context
    verification: Requests to protected endpoints without valid JWT return 401, authenticated
      requests include user object in request context
  - criterion: Role-based access control prevents unauthorized access to admin/premium
      features
    verification: Users with 'user' role cannot access /api/admin/* endpoints, premium
      features require 'premium' or 'admin' role
  - criterion: Token refresh mechanism maintains session continuity without forced
      re-login
    verification: POST /api/auth/refresh with valid refresh token returns new access
      token, expired tokens trigger automatic refresh
  - criterion: Auth system integrates with Supabase and maintains user profile consistency
    verification: User registration creates records in both Supabase auth.users and
      app user_profiles table, profile updates sync correctly
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/middleware/auth.test.ts
      coverage_target: 90%
      scenarios:
      - JWT token validation success/failure
      - User context injection
      - Role-based access control
      - Token expiration handling
      - Malformed token rejection
    - file: packages/backend/src/__tests__/services/user.test.ts
      coverage_target: 85%
      scenarios:
      - User profile CRUD operations
      - Role assignment/validation
      - Profile data sanitization
    - file: packages/backend/src/__tests__/routes/auth.test.ts
      coverage_target: 85%
      scenarios:
      - Registration flow validation
      - Login success/failure cases
      - Logout and session cleanup
      - Token refresh flows
    integration_tests:
    - file: packages/backend/src/__tests__/integration/auth-flow.test.ts
      scenarios:
      - Complete registration-to-authenticated-request flow
      - Login-to-protected-resource access flow
      - Token refresh during active session
      - Cross-role permission enforcement
      - Supabase integration end-to-end
    - file: packages/backend/src/__tests__/integration/auth-supabase.test.ts
      scenarios:
      - Supabase auth service integration
      - Database profile sync operations
    manual_testing:
    - step: Register new user via frontend and verify database records
      expected: User appears in both Supabase auth panel and user_profiles table
    - step: Test session persistence across browser refresh
      expected: User remains logged in, protected routes accessible
    - step: Verify admin panel inaccessible to regular users
      expected: 403 Forbidden response for non-admin users
  estimates:
    development: 2.5
    code_review: 1
    testing: 1
    documentation: 0.5
    total: 5
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[HUMAN] Design auth architecture and security model (JWT strategy, refresh
        flow, RBAC design)'
      done: false
      ai_friendly: false
    - task: '[AI] Create database migration files for user profiles and roles schema'
      done: false
      ai_friendly: true
    - task: '[AI] Implement Supabase client configuration and auth plugin setup'
      done: false
      ai_friendly: true
    - task: '[AI] Build authentication middleware with JWT validation and user context
        injection'
      done: false
      ai_friendly: true
    - task: '[AI] Create auth routes (register, login, logout, refresh, profile endpoints)'
      done: false
      ai_friendly: true
    - task: '[AI] Implement user service with profile management and role handling'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit test suites for all auth components'
      done: false
      ai_friendly: true
    - task: '[AI] Build integration tests for complete auth flows and Supabase integration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Security review of auth implementation and token handling'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Manual testing of auth flows and cross-browser session handling'
      done: false
      ai_friendly: false
- key: T27
  title: Backend Unit Tests (Vitest)
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 5
  area: backend
  dependsOn:
  - T25
  agent_notes:
    research_findings: "**Context:**\nBackend unit tests are foundational for the\
      \ Morpheus platform's reliability and maintainability. Given that Morpheus handles\
      \ critical operations like novel parsing, AI-generated comic panels, user payments,\
      \ and content delivery, comprehensive unit testing ensures each service component\
      \ behaves correctly in isolation. This is especially crucial for a startup where\
      \ rapid iteration needs to be balanced with system stability. Unit tests also\
      \ serve as living documentation for the codebase and enable confident refactoring\
      \ as the platform evolves.\n\n**Technical Approach:**\n- **Vitest Configuration**:\
      \ Leverage Vitest's native TypeScript support and ESM compatibility with Fastify\
      \ 5\n- **Test Structure**: Follow AAA pattern (Arrange, Act, Assert) with descriptive\
      \ test names\n- **Mocking Strategy**: Use Vitest's vi.mock() for external dependencies\
      \ (Supabase, OpenAI, RunPod)\n- **Test Categories**: \n  - Service layer tests\
      \ (business logic)\n  - Repository/data layer tests (database operations)\n\
      \  - Utility function tests (parsing, validation, transformations)\n  - API\
      \ route handler tests (request/response validation)\n- **Coverage**: Target\
      \ 80%+ code coverage with focus on critical paths\n- **Test Data**: Use factories/builders\
      \ for consistent test data generation\n\n**AI Suitability Analysis:**\n- High\
      \ AI effectiveness: Test boilerplate generation, mock setup, CRUD operation\
      \ tests, utility function tests, repetitive assertion patterns\n- Medium AI\
      \ effectiveness: Complex business logic tests, integration between services,\
      \ error handling scenarios\n- Low AI effectiveness: Test strategy decisions,\
      \ coverage analysis, complex async flow testing, performance test design\n\n\
      **Dependencies:**\n- External: vitest, @vitest/ui, c8 (coverage), supertest\
      \ (HTTP testing), msw (API mocking)\n- Internal: All backend services, database\
      \ schemas, shared types, authentication middleware\n\n**Risks:**\n- **Flaky\
      \ Tests**: Async operations and timing issues; mitigation: proper async/await\
      \ usage, deterministic test data\n- **Over-mocking**: Tests that don't catch\
      \ real integration issues; mitigation: balance unit vs integration tests\n-\
      \ **Maintenance Overhead**: Tests breaking frequently during development; mitigation:\
      \ focus on testing behavior over implementation\n- **Coverage Gaming**: High\
      \ coverage but poor quality tests; mitigation: review test quality, not just\
      \ metrics\n\n**Complexity Notes:**\nInitially seems straightforward, but complexity\
      \ increases with Morpheus's AI integration points and async workflows. AI agents\
      \ can significantly accelerate test writing velocity (3-4x faster for boilerplate),\
      \ allowing focus on test strategy and edge cases. The monorepo structure adds\
      \ complexity for test configuration sharing.\n\n**Key Files:**\n- vitest.config.ts:\
      \ Test configuration and environment setup\n- packages/backend/src/**/*.test.ts:\
      \ Individual service unit tests\n- packages/backend/tests/: Shared test utilities\
      \ and fixtures\n- packages/shared/: Type definitions needed for tests\n- turbo.json:\
      \ Test pipeline configuration\n"
    design_decisions:
    - decision: Use Vitest over Jest for testing framework
      rationale: Native ESM support, faster execution, better TypeScript integration,
        and seamless Vite ecosystem compatibility
      alternatives_considered:
      - Jest with babel transformation
      - Node.js built-in test runner
      ai_implementation_note: AI excels at converting existing patterns to Vitest
        syntax and generating test suites from function signatures
    - decision: Implement test factories for data generation
      rationale: Consistent test data creation, reduced boilerplate, easier maintenance
        when schemas change
      alternatives_considered:
      - Inline test data
      - JSON fixtures
      - Database seeding
      ai_implementation_note: AI can generate comprehensive factory functions from
        TypeScript interfaces and database schemas
    - decision: Mock external services at service boundary
      rationale: True unit isolation while maintaining realistic service interfaces
        for AI/database operations
      alternatives_considered:
      - Integration tests only
      - Mock at HTTP layer
      - Test doubles
      ai_implementation_note: AI can generate mock implementations from API documentation
        and TypeScript interfaces
    researched_at: '2026-02-08T18:25:16.173897'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:08:00.103199'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: d38eb260
    planning_hash: 0361fd1c
  technical_notes:
    approach: 'Establish Vitest configuration with TypeScript support and coverage
      reporting. Create test utilities and factories for consistent data generation.
      Implement comprehensive test suites for each service layer, focusing on business
      logic validation, error handling, and edge cases. Use strategic mocking for
      external dependencies while maintaining realistic service contracts. Set up
      automated coverage reporting and integrate with CI/CD pipeline.

      '
    external_dependencies:
    - name: vitest
      version: ^1.0.0
      reason: Primary testing framework with native TypeScript and ESM support
    - name: '@vitest/ui'
      version: ^1.0.0
      reason: Visual test runner interface for development
    - name: c8
      version: ^8.0.0
      reason: Code coverage reporting with V8 engine integration
    - name: supertest
      version: ^6.3.0
      reason: HTTP assertion testing for Fastify route handlers
    - name: msw
      version: ^2.0.0
      reason: Mock Service Worker for intercepting external API calls
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate realistic test data for factories
    files_to_modify:
    - path: packages/backend/package.json
      changes: Add vitest, @vitest/ui, c8, supertest, msw dependencies and test scripts
    - path: turbo.json
      changes: Add test and test:coverage pipeline configurations for backend package
    new_files:
    - path: packages/backend/vitest.config.ts
      purpose: Vitest configuration with TypeScript, ESM, coverage, and mock setup
    - path: packages/backend/tests/setup.ts
      purpose: Global test setup, mock configurations, and test environment initialization
    - path: packages/backend/tests/fixtures/index.ts
      purpose: Test data factories for users, novels, comics, payments
    - path: packages/backend/tests/mocks/external-apis.ts
      purpose: Mock implementations for Supabase, OpenAI, RunPod, payment providers
    - path: packages/backend/src/services/__tests__/novel-parser.test.ts
      purpose: Unit tests for novel parsing service with file handling edge cases
    - path: packages/backend/src/services/__tests__/ai-comic-generator.test.ts
      purpose: Unit tests for AI comic generation with API integration mocking
    - path: packages/backend/src/services/__tests__/user-service.test.ts
      purpose: Unit tests for user management including authentication flows
    - path: packages/backend/src/services/__tests__/payment-service.test.ts
      purpose: Unit tests for payment processing with critical business logic coverage
    - path: packages/backend/src/routes/__tests__/api-routes.test.ts
      purpose: Unit tests for API route handlers with request/response validation
    - path: packages/backend/src/utils/__tests__/validators.test.ts
      purpose: Unit tests for utility functions, validators, and transformers
    - path: .github/workflows/backend-tests.yml
      purpose: CI/CD workflow for automated test execution and coverage reporting
  acceptance_criteria:
  - criterion: Vitest configuration supports TypeScript, ESM, and monorepo structure
      with 80%+ code coverage reporting
    verification: Run `pnpm test:coverage` from backend package - coverage report
      shows >80% for all service modules
  - criterion: All backend services have comprehensive unit tests covering business
      logic, error handling, and edge cases
    verification: Each service file in src/services/ has corresponding .test.ts file
      with AAA pattern tests
  - criterion: External dependencies (Supabase, OpenAI, RunPod) are properly mocked
      with realistic contracts
    verification: Tests run in isolation without network calls - check test output
      for mock usage logs
  - criterion: Test utilities and factories enable consistent test data generation
      across all test suites
    verification: Run test suite - verify consistent data structures using shared
      factories in tests/fixtures/
  - criterion: CI/CD pipeline integration with automated test execution and coverage
      reporting
    verification: GitHub Actions workflow runs tests on PR - coverage report posted
      as comment
  testing:
    unit_tests:
    - file: packages/backend/src/services/__tests__/novel-parser.test.ts
      coverage_target: 90%
      scenarios:
      - Parse EPUB file successfully
      - Handle corrupted file formats
      - Extract metadata correctly
      - Chapter segmentation logic
    - file: packages/backend/src/services/__tests__/ai-comic-generator.test.ts
      coverage_target: 85%
      scenarios:
      - Generate comic panels from text
      - Handle OpenAI API failures
      - Validate panel configurations
      - Process generation queue
    - file: packages/backend/src/services/__tests__/user-service.test.ts
      coverage_target: 85%
      scenarios:
      - User registration flow
      - Authentication validation
      - Subscription management
      - Profile updates
    - file: packages/backend/src/services/__tests__/payment-service.test.ts
      coverage_target: 95%
      scenarios:
      - Process subscription payments
      - Handle payment failures
      - Webhook processing
      - Refund operations
    integration_tests:
    - file: packages/backend/src/__tests__/integration/novel-to-comic-flow.test.ts
      scenarios:
      - Complete novel upload to comic generation pipeline
      - User payment to content unlock flow
    manual_testing:
    - step: Run full test suite with coverage
      expected: All tests pass, coverage report generated, no external API calls made
    - step: Verify mock contracts match real API responses
      expected: Mock data structures identical to actual service responses
  estimates:
    development: 2.5
    code_review: 1
    testing: 0.8
    documentation: 0.3
    total: 4.6
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup Vitest configuration with TypeScript, ESM, and coverage reporting'
      done: false
      ai_friendly: true
    - task: '[AI] Create test utilities, fixtures, and mock factories for consistent
        data generation'
      done: false
      ai_friendly: true
    - task: '[AI] Implement external API mocks (Supabase, OpenAI, RunPod) with realistic
        contracts'
      done: false
      ai_friendly: true
    - task: '[AI] Generate unit tests for novel parser service with file handling
        scenarios'
      done: false
      ai_friendly: true
    - task: '[AI] Generate unit tests for AI comic generation service with error handling'
      done: false
      ai_friendly: true
    - task: '[AI] Generate unit tests for user service covering authentication and
        profile management'
      done: false
      ai_friendly: true
    - task: '[AI] Generate unit tests for payment service with comprehensive business
        logic coverage'
      done: false
      ai_friendly: true
    - task: '[AI] Generate unit tests for API route handlers and utility functions'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review test quality, coverage gaps, and mock contract accuracy'
      done: false
      ai_friendly: false
    - task: '[AI] Setup CI/CD integration with automated test execution and reporting'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Validate end-to-end test scenarios and integration points'
      done: false
      ai_friendly: false
- key: T28
  title: Mock Mode for Backend Services
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 2
  area: backend
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      Mock Mode is essential for development and testing environments where external
      dependencies (OpenAI, Anthropic, RunPod, payment processors) should be bypassed.
      This enables faster development cycles, reliable CI/CD without API costs, and
      offline development. For Morpheus, this means developers can test novel-to-comic
      workflows without consuming expensive ML API credits or waiting for real image
      generation.


      **Technical Approach:**

      Implement an environment-driven mock system using the Strategy pattern. Create
      mock implementations for each external service that return deterministic responses.
      Use dependency injection through a service registry that switches between real
      and mock implementations based on NODE_ENV or MOCK_MODE flag. Store mock responses
      as static JSON/image files in a `__mocks__` directory. Implement response delays
      to simulate real API latency.


      **AI Suitability Analysis:**

      - High AI effectiveness: Mock response generation, test data creation, CRUD
      mock implementations, JSON schema validation, basic service switching logic

      - Medium AI effectiveness: Service registry setup, dependency injection patterns,
      mock response matching logic

      - Low AI effectiveness: Architecture decisions on what to mock, business logic
      for realistic mock scenarios, integration testing strategy


      **Dependencies:**

      - External: None required (intentionally avoiding external dependencies for
      mocks)

      - Internal: Existing service layer (LLM services, image generation, payment
      processing), configuration management, logging system


      **Risks:**

      - Mock drift: Mock responses diverge from real API responses over time. Mitigation:
      Regular mock validation against real APIs

      - Over-mocking: Mocking too much business logic instead of just external services.
      Mitigation: Only mock at service boundaries

      - Test reliability: Tests pass in mock mode but fail in production. Mitigation:
      Separate integration test suite with real APIs


      **Complexity Notes:**

      Medium complexity - requires careful architecture to avoid tight coupling. AI
      can significantly accelerate implementation of mock data and boilerplate service
      implementations, reducing estimated effort by ~40%.


      **Key Files:**

      - apps/api/src/services/mock/: Mock service implementations

      - apps/api/src/config/service-registry.ts: Dependency injection container

      - apps/api/src/types/mock-responses.ts: TypeScript interfaces for mock data

      - apps/api/__mocks__/: Static mock response files

      '
    design_decisions:
    - decision: Use Strategy Pattern with Service Registry for Mock/Real Service Switching
      rationale: Enables clean separation between mock and real implementations, supports
        runtime switching, and maintains type safety
      alternatives_considered:
      - Environment-based imports
      - Conditional instantiation
      - Proxy-based mocking
      ai_implementation_note: AI can generate service interfaces and mock implementations
        following established patterns
    - decision: Store Mock Responses as Static Files with Versioning
      rationale: Provides deterministic responses, enables version control of test
        data, and supports easy updates from real API responses
      alternatives_considered:
      - In-memory hardcoded responses
      - Dynamic mock generation
      - External mock server
      ai_implementation_note: AI can generate comprehensive mock datasets based on
        API documentation and existing response schemas
    - decision: Implement Configurable Response Delays for Realistic Testing
      rationale: Simulates real API latency to catch timing-related bugs and provides
        more realistic development experience
      alternatives_considered:
      - Instant responses
      - Fixed delays
      - Network simulation
      ai_implementation_note: AI can implement delay logic and configuration management
        with proper async/await patterns
    researched_at: '2026-02-08T18:25:38.929980'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:08:32.663392'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 9d6a1b10
    planning_hash: 90e6c5bf
  technical_notes:
    approach: 'Create a service registry that instantiates mock or real service implementations
      based on environment configuration. Implement mock versions of LLM, image generation,
      and payment services that return static responses from JSON files. Add middleware
      to introduce realistic delays and error simulation. Ensure mock services implement
      the same interfaces as real services to maintain type safety and allow seamless
      switching.

      '
    external_dependencies:
    - name: None required
      version: N/A
      reason: Mock mode should be self-contained to avoid external dependencies that
        could break offline development
    files_to_modify:
    - path: apps/api/src/config/environment.ts
      changes: Add MOCK_MODE boolean configuration variable
    - path: apps/api/src/server.ts
      changes: Initialize service registry based on mock mode configuration
    - path: apps/api/src/routes/stories.ts
      changes: Use dependency-injected LLM service instead of direct imports
    - path: apps/api/src/routes/images.ts
      changes: Use dependency-injected image service instead of direct imports
    new_files:
    - path: apps/api/src/config/service-registry.ts
      purpose: Dependency injection container for service management
    - path: apps/api/src/services/mock/mock-llm.service.ts
      purpose: Mock implementation of LLM service interface
    - path: apps/api/src/services/mock/mock-image.service.ts
      purpose: Mock implementation of image generation service interface
    - path: apps/api/src/services/mock/mock-payment.service.ts
      purpose: Mock implementation of payment processing service interface
    - path: apps/api/src/types/mock-responses.ts
      purpose: TypeScript interfaces for mock response data structures
    - path: apps/api/src/services/interfaces/llm.interface.ts
      purpose: Common interface for LLM service implementations
    - path: apps/api/src/services/interfaces/image.interface.ts
      purpose: Common interface for image generation service implementations
    - path: apps/api/src/services/interfaces/payment.interface.ts
      purpose: Common interface for payment service implementations
    - path: apps/api/__mocks__/llm/story-outlines.json
      purpose: Static mock data for story generation responses
    - path: apps/api/__mocks__/llm/character-descriptions.json
      purpose: Static mock data for character generation responses
    - path: apps/api/__mocks__/images/characters/
      purpose: Directory containing mock character image files
    - path: apps/api/__mocks__/images/scenes/
      purpose: Directory containing mock scene image files
    - path: apps/api/__mocks__/payments/responses.json
      purpose: Static mock data for payment processing responses
  acceptance_criteria:
  - criterion: Mock mode activated via NODE_ENV=test or MOCK_MODE=true environment
      variables switches all external services to mock implementations
    verification: Start API server with MOCK_MODE=true, verify logs show 'Mock services
      initialized', check service registry returns mock instances
  - criterion: Mock LLM service returns deterministic story outlines and character
      descriptions from static JSON files within 100-500ms simulated latency
    verification: POST /api/stories/generate with test prompt, verify response matches
      apps/api/__mocks__/llm/story-outline.json and response time is 100-500ms
  - criterion: Mock image generation service returns predefined base64 images for
      character and scene generation without external API calls
    verification: POST /api/images/generate, verify returns mock image from __mocks__/images/
      directory and no network requests to RunPod/external APIs
  - criterion: Mock payment service simulates successful/failed transactions based
      on test card numbers without Stripe API calls
    verification: Process payment with test card 4242424242424242 (success) and 4000000000000002
      (decline), verify appropriate responses and no Stripe network calls
  - criterion: All mock services implement identical interfaces to real services enabling
      seamless switching without code changes
    verification: TypeScript compilation passes, unit tests run against both mock
      and real service interfaces, no type errors in service registry
  testing:
    unit_tests:
    - file: apps/api/src/services/mock/__tests__/mock-llm.test.ts
      coverage_target: 90%
      scenarios:
      - Returns story outline from mock data
      - Simulates API latency correctly
      - Handles different prompt types
      - Maintains response format consistency
    - file: apps/api/src/services/mock/__tests__/mock-image.test.ts
      coverage_target: 90%
      scenarios:
      - Returns base64 image data
      - Cycles through available mock images
      - Simulates generation delays
      - Handles different image types (character/scene)
    - file: apps/api/src/config/__tests__/service-registry.test.ts
      coverage_target: 85%
      scenarios:
      - Returns mock services when MOCK_MODE=true
      - Returns real services when MOCK_MODE=false
      - Handles missing environment variables
      - Maintains singleton instances
    integration_tests:
    - file: apps/api/src/__tests__/integration/mock-workflow.test.ts
      scenarios:
      - Complete novel-to-comic workflow using only mock services
      - Service switching between mock and real modes
      - Mock error simulation and recovery
    manual_testing:
    - step: Start API with MOCK_MODE=true and generate a complete comic
      expected: All external service calls return mock data, no real API charges incurred
    - step: Toggle MOCK_MODE=false and verify real services are used
      expected: Service registry switches to real implementations, logs indicate real
        API usage
  estimates:
    development: 2.5
    code_review: 0.5
    testing: 0.8
    documentation: 0.3
    total: 4.1
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create service interfaces for LLM, image generation, and payment
        services with comprehensive TypeScript types'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive mock response data (JSON files) for all service
        types including edge cases'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design service registry architecture and dependency injection
        strategy'
      done: false
      ai_friendly: false
    - task: '[AI] Implement service registry with environment-based service instantiation
        logic'
      done: false
      ai_friendly: true
    - task: '[AI] Create mock service implementations that conform to interfaces and
        include latency simulation'
      done: false
      ai_friendly: true
    - task: '[AI] Update environment configuration to support MOCK_MODE flag with
        proper validation'
      done: false
      ai_friendly: true
    - task: '[AI] Refactor existing route handlers to use dependency-injected services
        instead of direct imports'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit tests for all mock services and service
        registry'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for complete mock workflow scenarios'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Code review focusing on architecture patterns, error handling,
        and mock data realism'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Validate mock responses match real API response formats through
        spot testing'
      done: false
      ai_friendly: false
- key: T29
  title: Error Handling Strategy
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 2
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Error handling strategy is crucial for the Morpheus backend as it deals with
      complex ML operations (OpenAI/Anthropic LLMs, RunPod Stable Diffusion), database
      operations, and user content processing. Without proper error handling, users
      will experience unclear failures during comic generation, and developers will
      struggle with debugging. This is P0 because all subsequent backend work depends
      on consistent error patterns.


      **Technical Approach:**

      - Implement structured error classes extending native Error with error codes,
      HTTP status mapping, and context

      - Use Fastify''s built-in error handling with custom error serializers

      - Create async error boundaries for ML pipeline operations

      - Implement circuit breaker pattern for external ML services

      - Add request correlation IDs for tracing across microservices

      - Structured logging with Pino (Fastify''s default) for error aggregation

      - Implement retry mechanisms with exponential backoff for transient failures


      **AI Suitability Analysis:**

      - High AI effectiveness: Error class boilerplate, test scenarios, validation
      schemas, error message templates, logging utilities

      - Medium AI effectiveness: Fastify plugin integration, error serializers, retry
      logic implementation

      - Low AI effectiveness: Error classification strategy, business logic error
      boundaries, ML-specific error handling patterns


      **Dependencies:**

      - External: @fastify/error, pino, p-retry, p-timeout, zod (validation errors)

      - Internal: All backend services, database layer, ML service integrations


      **Risks:**

      - Over-engineering: Keep error classes simple, avoid deep inheritance hierarchies

      - Performance impact: Ensure error handling doesn''t add significant latency
      to happy path

      - Information leakage: Sanitize errors in production to avoid exposing internal
      details

      - ML service failures: Design graceful degradation when external AI services
      are down


      **Complexity Notes:**

      Medium complexity due to ML integration requirements. AI agents excel at generating
      error handling boilerplate but human judgment needed for error classification
      and recovery strategies. Expect 40% faster implementation with AI assistance.


      **Key Files:**

      - packages/backend/src/lib/errors/: Core error classes and utilities

      - packages/backend/src/plugins/errorHandler.ts: Fastify error handling plugin

      - packages/backend/src/middleware/errorBoundary.ts: Async error boundaries

      - packages/backend/src/services/ml/errorHandling.ts: ML-specific error handling

      '
    design_decisions:
    - decision: Use custom error classes with error codes instead of generic HTTP
        errors
      rationale: Provides better debugging, consistent API responses, and allows for
        specific handling of ML/DB errors
      alternatives_considered:
      - Generic HTTP errors
      - Error objects without classes
      - Third-party error libraries
      ai_implementation_note: AI can generate error class boilerplate, HTTP status
        mappings, and serialization methods
    - decision: Implement circuit breaker pattern for external ML services
      rationale: Prevents cascading failures when OpenAI/RunPod services are down,
        provides graceful degradation
      alternatives_considered:
      - Simple retry logic
      - Manual service health checks
      - No circuit breaking
      ai_implementation_note: AI can implement standard circuit breaker logic but
        human input needed for threshold configuration
    - decision: Use correlation IDs for request tracing across services
      rationale: Essential for debugging complex ML pipelines where requests span
        multiple services and async operations
      alternatives_considered:
      - Service-specific logging
      - Database-only error tracking
      - No correlation tracking
      ai_implementation_note: AI excellent for implementing correlation ID middleware
        and logging integration
    researched_at: '2026-02-08T18:26:03.523648'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:08:58.355995'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: fff52931
    planning_hash: 8e7d8053
  technical_notes:
    approach: 'Create a hierarchical error system with base MorpheusError class containing
      error codes, HTTP status mappings, and context data. Implement Fastify plugin
      for centralized error handling with sanitization for production. Add circuit
      breakers for ML services with configurable thresholds and fallback responses.
      Use correlation IDs for request tracing and structured logging with Pino for
      error aggregation and monitoring.

      '
    external_dependencies:
    - name: '@fastify/error'
      version: ^4.0.0
      reason: Fastify's official error handling utilities and serialization
    - name: pino
      version: ^8.17.0
      reason: High-performance structured logging (Fastify default)
    - name: p-retry
      version: ^6.2.0
      reason: Robust retry mechanism with exponential backoff for ML service calls
    - name: p-timeout
      version: ^6.1.2
      reason: Timeout handling for long-running ML operations
    - name: opossum
      version: ^8.0.0
      reason: Circuit breaker implementation for ML service resilience
    files_to_modify:
    - path: packages/backend/src/index.ts
      changes: Register errorHandler plugin and correlation middleware
    - path: packages/backend/src/services/ml/openai.ts
      changes: Wrap API calls with circuit breaker and retry logic
    - path: packages/backend/src/services/ml/runpod.ts
      changes: Add ML-specific error handling and timeouts
    - path: packages/backend/package.json
      changes: 'Add dependencies: p-retry, p-timeout, @fastify/error'
    new_files:
    - path: packages/backend/src/lib/errors/MorpheusError.ts
      purpose: Base error class with code, httpStatus, context properties
    - path: packages/backend/src/lib/errors/ValidationError.ts
      purpose: Form/API validation errors with field-level details
    - path: packages/backend/src/lib/errors/MLServiceError.ts
      purpose: OpenAI/Anthropic/RunPod specific errors with retry context
    - path: packages/backend/src/lib/errors/DatabaseError.ts
      purpose: Database operation errors with transaction context
    - path: packages/backend/src/lib/errors/index.ts
      purpose: Export all error classes and utilities
    - path: packages/backend/src/lib/circuitBreaker.ts
      purpose: Circuit breaker implementation for external services
    - path: packages/backend/src/lib/retry.ts
      purpose: Exponential backoff retry utilities
    - path: packages/backend/src/plugins/errorHandler.ts
      purpose: Fastify plugin for centralized error handling
    - path: packages/backend/src/plugins/correlationId.ts
      purpose: Request correlation ID generation and propagation
    - path: packages/backend/src/middleware/errorBoundary.ts
      purpose: Async error boundaries for ML pipeline operations
    - path: packages/backend/src/services/ml/errorHandling.ts
      purpose: ML-specific error classification and recovery strategies
    - path: packages/backend/src/types/errors.ts
      purpose: TypeScript interfaces for error structures
  acceptance_criteria:
  - criterion: All errors inherit from MorpheusError with consistent structure (code,
      message, context, httpStatus)
    verification: Run `npm test -- --testNamePattern='error structure'` - all error
      classes extend MorpheusError
  - criterion: ML service failures trigger circuit breaker with graceful degradation
    verification: Simulate OpenAI/RunPod downtime - verify circuit opens after 5 failures
      and returns fallback responses
  - criterion: Request correlation IDs trace through entire ML pipeline
    verification: Check logs during comic generation - same correlationId appears
      across all service calls
  - criterion: Error responses are sanitized in production (no stack traces/internal
      details)
    verification: Set NODE_ENV=production, trigger error - response contains only
      safe error code/message
  - criterion: Retry logic handles transient failures with exponential backoff
    verification: Mock intermittent network failures - verify 3 retries with 1s, 2s,
      4s delays before failing
  testing:
    unit_tests:
    - file: packages/backend/src/lib/errors/__tests__/MorpheusError.test.ts
      coverage_target: 95%
      scenarios:
      - Error class hierarchy and inheritance
      - HTTP status code mapping
      - Context data serialization
      - Error code validation
    - file: packages/backend/src/lib/errors/__tests__/circuitBreaker.test.ts
      coverage_target: 90%
      scenarios:
      - Circuit opens after failure threshold
      - Circuit closes after recovery period
      - Fallback response handling
    - file: packages/backend/src/plugins/__tests__/errorHandler.test.ts
      coverage_target: 85%
      scenarios:
      - Production error sanitization
      - Development error details
      - Correlation ID preservation
    integration_tests:
    - file: packages/backend/src/__tests__/integration/errorHandling.test.ts
      scenarios:
      - End-to-end comic generation with ML service failures
      - Database connection errors with retry logic
      - Request tracing through multiple services
    manual_testing:
    - step: Kill OpenAI mock service during comic generation
      expected: Circuit breaker activates, returns 'AI service temporarily unavailable'
        message
    - step: Generate comic with invalid parameters
      expected: ValidationError returned with clear field-level messages
  estimates:
    development: 2.5
    code_review: 0.5
    testing: 0.8
    documentation: 0.2
    total: 4
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create base MorpheusError class with TypeScript interfaces'
      done: false
      ai_friendly: true
    - task: '[AI] Generate specific error classes (ValidationError, MLServiceError,
        DatabaseError)'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define error classification strategy and business logic boundaries'
      done: false
      ai_friendly: false
    - task: '[AI] Implement circuit breaker with configurable thresholds'
      done: false
      ai_friendly: true
    - task: '[AI] Create retry utilities with exponential backoff'
      done: false
      ai_friendly: true
    - task: '[AI] Build Fastify error handler plugin with sanitization'
      done: false
      ai_friendly: true
    - task: '[AI] Implement correlation ID middleware'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design ML service error recovery strategies and fallbacks'
      done: false
      ai_friendly: false
    - task: '[AI] Generate comprehensive unit tests for all error classes'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for error handling flows'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review error classification and ensure no information leakage'
      done: false
      ai_friendly: false
    - task: '[AI] Generate API documentation for error responses'
      done: false
      ai_friendly: true
- key: T30
  title: Logging & Observability Setup
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 2
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Logging & Observability is critical for the Morpheus platform because we''re
      dealing with complex, long-running AI workflows (novel analysis, comic generation,
      image processing) that can fail at multiple points. Without proper observability,
      debugging production issues, monitoring performance bottlenecks, and tracking
      user journeys through the novel-to-comic pipeline becomes nearly impossible.
      This is especially crucial given our multi-service architecture (Fastify backend,
      ML services, Supabase) and external dependencies (OpenAI, RunPod).


      **Technical Approach:**

      Implement structured logging with Pino (Fastify''s default) as the core logger,
      enhanced with OpenTelemetry for distributed tracing. Use a three-tier approach:
      1) Application logs (business logic, errors, performance), 2) Request/response
      logging with correlation IDs, 3) Distributed tracing for cross-service calls
      (ML pipelines, external APIs). For observability, integrate with a monitoring
      stack (Grafana/Prometheus for metrics, structured log aggregation). Include
      custom metrics for comic generation success rates, processing times, and user
      funnel analytics.


      **AI Suitability Analysis:**

      - High AI effectiveness: Logger configuration files, middleware boilerplate,
      test cases for logging functions, OpenTelemetry instrumentation setup, metric
      collection utilities

      - Medium AI effectiveness: Custom log formatters, error boundary implementations,
      performance monitoring decorators, dashboard configuration files

      - Low AI effectiveness: Determining what to log and at what levels, designing
      correlation ID strategies, choosing appropriate metric dimensions, defining
      alerting thresholds


      **Dependencies:**

      - External: pino, pino-pretty, @opentelemetry/api, @opentelemetry/sdk-node,
      @opentelemetry/instrumentation-fastify, @opentelemetry/instrumentation-http,
      prom-client, @supabase/supabase-js (for log storage)

      - Internal: Authentication middleware (for user context), Comic generation service,
      ML pipeline orchestrator, Error handling utilities


      **Risks:**

      - Performance overhead: Excessive logging can impact API response times - mitigation:
      Use async logging and appropriate log levels in production

      - Log volume explosion: AI workflows generate massive logs - mitigation: Implement
      log sampling and retention policies

      - Sensitive data leakage: Novel content or user data in logs - mitigation: Strict
      PII filtering and log sanitization

      - External service dependency: If using hosted observability - mitigation: Graceful
      degradation when logging services are unavailable


      **Complexity Notes:**

      Initially seems straightforward but complexity increases significantly due to:
      1) Cross-service correlation across ML pipelines, 2) Handling high-volume, long-running
      comic generation processes, 3) Balancing detail vs performance in production.
      AI can accelerate implementation by 60-70% for configuration and middleware,
      but architectural decisions around what to observe require human insight into
      the comic generation business logic.


      **Key Files:**

      - packages/backend/src/utils/logger.ts: Core logger configuration and utilities

      - packages/backend/src/middleware/logging.ts: Request/response logging middleware

      - packages/backend/src/middleware/tracing.ts: OpenTelemetry setup and correlation
      ID handling

      - packages/backend/src/services/comic-generation/logger.ts: Domain-specific
      logging for ML workflows

      - packages/backend/src/utils/metrics.ts: Custom business metrics (generation
      success rates, processing times)

      - packages/backend/src/config/observability.ts: Environment-specific logging
      configuration

      '
    design_decisions:
    - decision: Use Pino as the primary logger with structured JSON output
      rationale: Native Fastify integration, excellent performance, structured logging
        supports better querying and analysis
      alternatives_considered:
      - Winston (more features but heavier)
      - Console logging (insufficient for production)
      ai_implementation_note: AI can generate comprehensive Pino configuration, custom
        serializers, and log level management utilities
    - decision: Implement OpenTelemetry for distributed tracing across ML services
      rationale: Standard observability protocol, handles complex async workflows
        like comic generation pipelines, vendor-neutral
      alternatives_considered:
      - Custom correlation IDs only
      - Jaeger directly
      ai_implementation_note: AI excels at OpenTelemetry boilerplate setup, span creation
        patterns, and instrumentation decorators
    - decision: Create domain-specific loggers for comic generation workflows
      rationale: Novel-to-comic transformation has unique observability needs (progress
        tracking, quality metrics, user journey analysis)
      alternatives_considered:
      - Generic application logging only
      - Separate analytics service
      ai_implementation_note: AI can implement logger factories and workflow-specific
        log formatters, but requires human input on what comic generation events to
        track
    researched_at: '2026-02-08T18:26:37.046692'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:09:22.699865'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 719329ca
    planning_hash: 5e3bab4b
  technical_notes:
    approach: 'Start with Pino logger configuration and Fastify integration, then
      layer on request correlation middleware to track user sessions through comic
      generation. Implement OpenTelemetry instrumentation to trace cross-service calls
      to ML services and external APIs. Create specialized logging utilities for comic
      generation workflows with progress tracking and quality metrics. Finally, expose
      key business metrics via Prometheus endpoints for monitoring dashboard integration.

      '
    external_dependencies:
    - name: pino
      version: ^8.0.0
      reason: High-performance structured logger with native Fastify integration
    - name: pino-pretty
      version: ^10.0.0
      reason: Development-friendly log formatting for local debugging
    - name: '@opentelemetry/sdk-node'
      version: ^0.45.0
      reason: Distributed tracing for complex ML workflows and external API calls
    - name: '@opentelemetry/instrumentation-fastify'
      version: ^0.32.0
      reason: Automatic HTTP request/response tracing for Fastify backend
    - name: prom-client
      version: ^15.0.0
      reason: Prometheus metrics collection for business KPIs and performance monitoring
    - name: nanoid
      version: ^5.0.0
      reason: Generate correlation IDs for request tracking across services
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Add logger initialization, OpenTelemetry setup, and logging middleware
        registration
    - path: packages/backend/package.json
      changes: Add pino, pino-pretty, @opentelemetry/* packages, prom-client dependencies
    new_files:
    - path: packages/backend/src/utils/logger.ts
      purpose: Core Pino logger configuration with environment-based settings and
        PII filtering
    - path: packages/backend/src/middleware/logging.ts
      purpose: Fastify request/response logging middleware with correlation ID generation
    - path: packages/backend/src/middleware/tracing.ts
      purpose: OpenTelemetry instrumentation setup for distributed tracing
    - path: packages/backend/src/services/comic-generation/logger.ts
      purpose: Domain-specific logging utilities for ML workflow progress tracking
    - path: packages/backend/src/utils/metrics.ts
      purpose: Prometheus metrics collection for business KPIs and performance monitoring
    - path: packages/backend/src/config/observability.ts
      purpose: Environment-specific configuration for logging levels, sampling rates,
        and external integrations
    - path: packages/backend/src/routes/metrics.ts
      purpose: Prometheus metrics endpoint handler
  acceptance_criteria:
  - criterion: Core logger configuration is properly initialized with environment-specific
      settings
    verification: Run app in dev/prod modes and verify log format/levels via `npm
      run dev` and check console output
  - criterion: Request correlation IDs are tracked across all HTTP requests and passed
      to downstream services
    verification: Make API request, verify correlation ID appears in all log entries
      for that request chain
  - criterion: Comic generation workflows log progress milestones and performance
      metrics
    verification: 'Trigger comic generation, verify logs show: start, novel analysis
      complete, image generation progress, completion with timing'
  - criterion: OpenTelemetry tracing captures spans for external API calls (OpenAI,
      RunPod)
    verification: Generate comic, verify trace data includes spans for each external
      service call with latency metrics
  - criterion: Business metrics endpoint exposes key performance indicators
    verification: GET /metrics returns Prometheus format with comic_generation_success_rate,
      processing_time_seconds metrics
  testing:
    unit_tests:
    - file: packages/backend/src/utils/__tests__/logger.test.ts
      coverage_target: 90%
      scenarios:
      - Logger initialization with different environments
      - Log level filtering in production
      - PII sanitization in log messages
      - Correlation ID injection
    - file: packages/backend/src/middleware/__tests__/logging.test.ts
      coverage_target: 85%
      scenarios:
      - Request/response logging with correlation ID
      - Error request logging with stack traces
      - Performance timing capture
    - file: packages/backend/src/utils/__tests__/metrics.test.ts
      coverage_target: 85%
      scenarios:
      - Counter increment for comic generation events
      - Histogram recording for processing times
      - Gauge updates for active users
    integration_tests:
    - file: packages/backend/src/__tests__/integration/observability.test.ts
      scenarios:
      - End-to-end comic generation with full tracing
      - Error propagation through logging middleware
      - Metrics collection during API workflows
    manual_testing:
    - step: Start backend with LOG_LEVEL=debug and make several API requests
      expected: Structured JSON logs with correlation IDs, request timings, and business
        context
    - step: Access /metrics endpoint
      expected: Prometheus format metrics including custom comic generation metrics
  estimates:
    development: 2.5
    code_review: 1
    testing: 1
    documentation: 0.5
    total: 5
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Install required dependencies (pino, OpenTelemetry packages, prom-client)'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define logging strategy and what business events to capture for
        comic generation workflows'
      done: false
      ai_friendly: false
    - task: '[AI] Create core logger configuration with environment-based settings'
      done: false
      ai_friendly: true
    - task: '[AI] Implement request/response logging middleware with correlation ID
        generation'
      done: false
      ai_friendly: true
    - task: '[AI] Set up OpenTelemetry instrumentation for HTTP and external service
        calls'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design custom metrics schema for comic generation success rates
        and processing times'
      done: false
      ai_friendly: false
    - task: '[AI] Implement Prometheus metrics collection utilities and /metrics endpoint'
      done: false
      ai_friendly: true
    - task: '[AI] Create domain-specific logging for comic generation service with
        progress tracking'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit tests for all logging and metrics utilities'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Code review focusing on PII handling, performance impact, and
        observability completeness'
      done: false
      ai_friendly: false
- key: T31
  title: Query Performance Optimization
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 3
  area: backend
  dependsOn:
  - T24
  agent_notes:
    research_findings: '**Context:**

      Query performance optimization is critical for Morpheus as the platform scales
      to handle multiple users transforming novels to comics simultaneously. The backend
      will face complex queries involving novel text processing, comic panel generation
      tracking, user progress, and ML job status monitoring. Without proper optimization,
      response times will degrade, affecting user experience and potentially causing
      timeouts during resource-intensive operations like Stable Diffusion image generation
      coordination.


      **Technical Approach:**

      - Database indexing strategy for frequently queried fields (user_id, novel_id,
      processing_status, created_at)

      - Query analysis using Supabase''s built-in PostgreSQL EXPLAIN ANALYZE

      - Connection pooling configuration for Fastify 5 + Supabase integration

      - Implement query caching layer using Redis for expensive aggregations

      - Database pagination patterns for large result sets (novel chapters, generated
      panels)

      - N+1 query prevention in Fastify route handlers

      - Batch operations for bulk ML job status updates


      **AI Suitability Analysis:**

      - High AI effectiveness: Index creation scripts, connection pool configuration,
      basic caching middleware, test query generators, performance monitoring utilities

      - Medium AI effectiveness: Query refactoring for existing routes, pagination
      implementation, batch operation patterns

      - Low AI effectiveness: Index strategy decisions, cache invalidation logic,
      performance bottleneck identification, query optimization trade-offs


      **Dependencies:**

      - External: @supabase/supabase-js (already in use), ioredis for caching, @fastify/rate-limit
      for query protection

      - Internal: Database schema from user management, novel processing pipeline,
      ML job queue system


      **Risks:**

      - Over-indexing: Too many indexes slow down writes; mitigation: profile write
      vs read patterns first

      - Cache invalidation complexity: Stale data in comic generation pipeline; mitigation:
      implement cache tags and TTL strategies

      - Connection pool exhaustion: Under high load with ML operations; mitigation:
      proper pool sizing and monitoring


      **Complexity Notes:**

      More complex than initially estimated due to Morpheus''s unique workflow of
      coordinating long-running ML tasks with real-time user interactions. AI can
      significantly accelerate the implementation of standard optimization patterns,
      but human judgment crucial for Morpheus-specific caching strategies around novel-to-comic
      transformation states.


      **Key Files:**

      - packages/backend/src/config/database.ts: Connection pool configuration

      - packages/backend/src/middleware/cache.ts: Redis caching middleware

      - packages/backend/src/routes/novels/*.ts: Query optimization for novel routes

      - packages/backend/src/routes/comics/*.ts: Comic generation query patterns

      - packages/database/migrations/: Index creation migrations

      - packages/backend/src/utils/query-helpers.ts: Reusable query optimization utilities

      '
    design_decisions:
    - decision: Implement Redis-based query result caching with TTL and tag-based
        invalidation
      rationale: Supabase PostgreSQL queries for novel analysis and comic generation
        status will be expensive. Redis provides fast access and flexible invalidation
        patterns for Morpheus's complex state transitions.
      alternatives_considered:
      - In-memory caching (limited scalability)
      - PostgreSQL materialized views (less flexible)
      - No caching (poor performance)
      ai_implementation_note: AI can generate Redis connection setup, basic get/set
        patterns, and TTL configurations. Human needed for cache invalidation strategy
        around novel processing states.
    - decision: Create composite indexes on (user_id, novel_id, status, created_at)
        for main entity queries
      rationale: Most Morpheus queries filter by user ownership, then drill down to
        specific novels/comics and their processing status with time-based ordering
        for progress tracking.
      alternatives_considered:
      - Single column indexes (less efficient for complex queries)
      - Full-text search indexes only (missing common filter patterns)
      ai_implementation_note: AI excellent for generating index creation SQL and analyzing
        EXPLAIN plans, but human judgment needed for identifying optimal index combinations
        based on actual query patterns.
    - decision: Implement connection pooling with separate pools for read-heavy vs
        write-heavy operations
      rationale: Comic generation involves both heavy reads (novel content) and frequent
        status updates. Separating pools prevents write operations from blocking reads
        during ML processing coordination.
      alternatives_considered:
      - Single connection pool (potential blocking)
      - Per-route connections (inefficient)
      - Read replicas (overkill for M1)
      ai_implementation_note: AI can implement pool configuration and connection management
        boilerplate. Human oversight needed for pool sizing based on Morpheus's ML
        workload patterns.
    researched_at: '2026-02-08T18:27:07.381394'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:09:51.575584'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 1f2ba105
    planning_hash: f6968065
  technical_notes:
    approach: 'Start with query profiling using Supabase''s PostgreSQL EXPLAIN ANALYZE
      on key Morpheus routes (novel upload, comic generation status, user dashboard).
      Create targeted indexes based on actual query patterns, focusing on user_id
      + status combinations. Implement Redis caching middleware in Fastify for expensive
      aggregations like user progress calculations. Configure connection pooling optimized
      for the mixed read/write patterns of novel processing workflows. Add comprehensive
      performance monitoring to track query execution times across the comic generation
      pipeline.

      '
    external_dependencies:
    - name: ioredis
      version: ^5.3.2
      reason: High-performance Redis client for query result caching with pipeline
        support for batch operations
    - name: '@fastify/redis'
      version: ^6.1.1
      reason: Official Fastify Redis plugin for seamless integration with existing
        route handlers
    - name: pg-query-analyzer
      version: ^2.1.0
      reason: Automated query performance analysis and bottleneck identification for
        PostgreSQL
    files_to_modify:
    - path: packages/backend/src/config/database.ts
      changes: Add Supabase connection pool configuration, query timeout settings
    - path: packages/backend/src/routes/novels/list.ts
      changes: Implement pagination, add caching middleware, optimize user novel queries
    - path: packages/backend/src/routes/novels/detail.ts
      changes: Add caching for novel metadata, optimize chapter loading
    - path: packages/backend/src/routes/comics/status.ts
      changes: Cache ML job aggregations, batch status updates
    - path: packages/backend/src/routes/users/dashboard.ts
      changes: Cache user progress calculations, optimize multi-table joins
    - path: packages/backend/package.json
      changes: Add ioredis dependency for Redis caching
    new_files:
    - path: packages/backend/src/middleware/cache.ts
      purpose: Redis caching middleware with TTL and invalidation strategies
    - path: packages/backend/src/middleware/performance-monitor.ts
      purpose: Query execution time tracking and logging
    - path: packages/backend/src/utils/query-helpers.ts
      purpose: Reusable pagination, batching, and query optimization utilities
    - path: packages/backend/src/config/redis.ts
      purpose: Redis client configuration and connection management
    - path: packages/database/migrations/20240115_add_performance_indexes.sql
      purpose: Create indexes for user_id, novel_id, status, created_at combinations
    - path: packages/backend/src/services/cache-service.ts
      purpose: Cache management service with invalidation logic
    - path: packages/backend/src/types/performance.ts
      purpose: TypeScript types for performance monitoring and caching
  acceptance_criteria:
  - criterion: Database queries execute within performance thresholds (novels <200ms,
      comics <500ms, dashboards <300ms)
    verification: Performance monitoring middleware logs response times; run `npm
      run test:performance` benchmark suite
  - criterion: Redis caching reduces database load by 70% for repeated queries
    verification: Cache hit/miss metrics in monitoring dashboard; compare query counts
      before/after implementation
  - criterion: Connection pool maintains <80% utilization under peak load (100 concurrent
      users)
    verification: Supabase dashboard shows connection metrics; load test with `npm
      run test:load`
  - criterion: All high-traffic routes have appropriate indexes (user_id, novel_id,
      status combinations)
    verification: EXPLAIN ANALYZE output shows index usage; migration files contain
      required indexes
  - criterion: N+1 queries eliminated in novel and comic routes
    verification: Database query logs show single queries for bulk operations; integration
      tests verify
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/middleware/cache.test.ts
      coverage_target: 90%
      scenarios:
      - Cache hit returns cached data
      - Cache miss fetches and stores data
      - Cache invalidation works correctly
      - Redis connection failure graceful handling
    - file: packages/backend/src/__tests__/utils/query-helpers.test.ts
      coverage_target: 85%
      scenarios:
      - Pagination helper generates correct LIMIT/OFFSET
      - Batch operations chunk data properly
      - Query builder creates optimized queries
    integration_tests:
    - file: packages/backend/src/__tests__/integration/query-performance.test.ts
      scenarios:
      - Novel list with pagination performs within limits
      - Comic generation status aggregation uses cache
      - User dashboard loads efficiently
      - Bulk ML job status updates complete quickly
    - file: packages/backend/src/__tests__/integration/database-indexes.test.ts
      scenarios:
      - Index usage verified with EXPLAIN ANALYZE
      - Query plans show expected performance
    performance_tests:
    - file: packages/backend/src/__tests__/performance/load.test.ts
      scenarios:
      - 100 concurrent users browsing novels
      - 50 concurrent comic generation requests
      - Connection pool stress test
    manual_testing:
    - step: Monitor Supabase dashboard during peak usage simulation
      expected: Connection count <80% of pool, query times within thresholds
    - step: Check Redis cache metrics in monitoring dashboard
      expected: Cache hit ratio >70% for repeated queries
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup Redis configuration and connection client'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Analyze existing queries and determine indexing strategy'
      done: false
      ai_friendly: false
    - task: '[AI] Create database migration files for performance indexes'
      done: false
      ai_friendly: true
    - task: '[AI] Implement Redis caching middleware with TTL configuration'
      done: false
      ai_friendly: true
    - task: '[AI] Create query helper utilities for pagination and batching'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design cache invalidation strategy for comic generation workflow'
      done: false
      ai_friendly: false
    - task: '[AI] Refactor novel routes to use caching and pagination'
      done: false
      ai_friendly: true
    - task: '[AI] Refactor comic routes to use batch operations and caching'
      done: false
      ai_friendly: true
    - task: '[AI] Implement performance monitoring middleware'
      done: false
      ai_friendly: true
    - task: '[AI] Configure Supabase connection pool optimization'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit tests for all new components'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for optimized query flows'
      done: false
      ai_friendly: true
    - task: '[AI] Generate performance and load testing suite'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review cache strategies and validate performance improvements'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Conduct load testing and tune parameters based on results'
      done: false
      ai_friendly: false
- key: T32
  title: API Documentation & OpenAPI
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 2
  area: backend
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      API documentation is critical for the Morpheus platform''s success, enabling
      frontend teams to efficiently integrate with backend services, third-party developers
      to build integrations, and AI coding assistants to generate accurate API client
      code. With Morpheus being a complex transformation platform involving novel
      uploads, comic generation, and user management, comprehensive OpenAPI documentation
      will reduce integration time, prevent API misuse, and serve as a contract between
      backend and frontend teams. This is especially valuable given the team''s heavy
      reliance on GitHub Copilot, which performs significantly better with well-documented
      APIs.


      **Technical Approach:**

      Implement OpenAPI 3.1 specification using Fastify''s native swagger plugin ecosystem.
      Use `@fastify/swagger` for OpenAPI generation and `@fastify/swagger-ui` for
      interactive documentation. Leverage JSON Schema validation that Fastify already
      provides to automatically generate accurate request/response schemas. Implement
      schema-first approach where TypeScript types are derived from JSON schemas,
      ensuring documentation stays in sync with actual API behavior. Add automated
      testing to validate OpenAPI spec completeness and accuracy.


      **AI Suitability Analysis:**

      - High AI effectiveness: Schema definitions, example payloads, basic CRUD endpoint
      documentation, test cases for API spec validation, boilerplate Fastify plugin
      setup

      - Medium AI effectiveness: Complex schema relationships, authentication documentation,
      error response mapping, integration with existing route handlers

      - Low AI effectiveness: API design decisions, information architecture for documentation
      site, business logic documentation, deciding what to expose publicly vs internally


      **Dependencies:**

      - External: @fastify/swagger, @fastify/swagger-ui, @apidevtools/swagger-jsdoc,
      openapi-types

      - Internal: Existing Fastify route handlers, authentication middleware, database
      schema types, error handling patterns


      **Risks:**

      - Schema drift: Documentation becomes outdated as API evolves. Mitigation: Automated
      validation in CI/CD pipeline

      - Over-documentation: Too much detail slows development. Mitigation: Start with
      core endpoints, expand iteratively

      - Security exposure: Accidentally documenting internal endpoints. Mitigation:
      Separate public/private documentation builds

      - Performance impact: Swagger UI adds overhead. Mitigation: Disable in production,
      serve static docs


      **Complexity Notes:**

      Initially seems straightforward, but complexity increases significantly when
      ensuring documentation accuracy and maintaining it over time. AI can accelerate
      initial setup and boilerplate generation by 60-70%, but the architectural decisions
      around what to document and how to structure it require human judgment. The
      integration with Fastify''s plugin system is well-established, reducing implementation
      complexity.


      **Key Files:**

      - `packages/backend/src/plugins/swagger.ts`: Main swagger configuration plugin

      - `packages/backend/src/schemas/`: JSON schemas for request/response validation

      - `packages/backend/src/routes/`: Add schema annotations to route handlers

      - `packages/backend/src/types/api.ts`: Generated TypeScript types from schemas

      - `packages/backend/docs/`: Static documentation assets

      - `turbo.json`: Add docs generation to build pipeline

      '
    design_decisions:
    - decision: Use Fastify's native swagger ecosystem instead of external tools like
        Redoc or standalone OpenAPI generators
      rationale: Tight integration with existing Fastify infrastructure, automatic
        schema generation from route validation, maintains single source of truth
      alternatives_considered:
      - Standalone OpenAPI with Redoc
      - Manual documentation with GitBook
      - Postman collections
      ai_implementation_note: AI can generate comprehensive route schemas and plugin
        configurations following Fastify patterns
    - decision: Implement schema-first approach with JSON Schema as source of truth
      rationale: Ensures documentation accuracy, enables automatic TypeScript generation,
        leverages Fastify's validation system
      alternatives_considered:
      - Code-first with decorators
      - Separate documentation files
      - TypeScript-first approach
      ai_implementation_note: AI excels at converting existing TypeScript interfaces
        to JSON Schema format and generating validation schemas
    - decision: Separate public and internal API documentation
      rationale: Security isolation, different audiences have different needs, allows
        for internal-only endpoints
      alternatives_considered:
      - Single documentation with access controls
      - Tag-based filtering
      - Multiple API versions
      ai_implementation_note: AI can help categorize endpoints and generate separate
        documentation builds based on route tags
    researched_at: '2026-02-08T18:27:36.414126'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:10:18.341448'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: afaeafea
    planning_hash: 5acd0446
  technical_notes:
    approach: 'Implement OpenAPI documentation by integrating @fastify/swagger into
      the existing Fastify application, converting current TypeScript interfaces to
      JSON Schema format, and annotating route handlers with comprehensive schema
      definitions. Create automated validation to ensure documentation completeness
      and accuracy. Set up separate documentation builds for public and internal APIs,
      with CI/CD integration to generate static documentation sites.

      '
    external_dependencies:
    - name: '@fastify/swagger'
      version: ^8.0.0
      reason: Official Fastify plugin for OpenAPI generation and route schema integration
    - name: '@fastify/swagger-ui'
      version: ^2.0.0
      reason: Interactive Swagger UI for API exploration and testing
    - name: openapi-types
      version: ^12.0.0
      reason: TypeScript type definitions for OpenAPI specification
    - name: '@apidevtools/swagger-jsdoc'
      version: ^6.2.8
      reason: Generate OpenAPI specs from JSDoc comments for complex documentation
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register swagger plugin with configuration
    - path: packages/backend/src/routes/auth.ts
      changes: Add schema annotations to login/register endpoints
    - path: packages/backend/src/routes/comics.ts
      changes: Add comprehensive schemas for upload, generate, list endpoints
    - path: packages/backend/src/routes/users.ts
      changes: Add schemas for profile, preferences endpoints
    - path: packages/backend/package.json
      changes: Add swagger dependencies and docs build scripts
    - path: turbo.json
      changes: Add docs:build and docs:validate pipeline tasks
    new_files:
    - path: packages/backend/src/plugins/swagger.ts
      purpose: Main swagger plugin configuration and OpenAPI setup
    - path: packages/backend/src/schemas/index.ts
      purpose: Central export for all JSON schemas
    - path: packages/backend/src/schemas/auth.ts
      purpose: Authentication request/response schemas
    - path: packages/backend/src/schemas/comics.ts
      purpose: Comic-related API schemas including upload/generation
    - path: packages/backend/src/schemas/users.ts
      purpose: User profile and preferences schemas
    - path: packages/backend/src/schemas/common.ts
      purpose: Shared schema components (pagination, errors, etc.)
    - path: packages/backend/src/types/api.ts
      purpose: Generated TypeScript types from JSON schemas
    - path: packages/backend/docs/api-guide.md
      purpose: Human-readable API integration guide
    - path: packages/backend/scripts/generate-types.ts
      purpose: Script to generate TypeScript types from schemas
    - path: packages/backend/scripts/validate-docs.ts
      purpose: Validate OpenAPI spec completeness and accuracy
  acceptance_criteria:
  - criterion: OpenAPI 3.1 specification is automatically generated for all API endpoints
    verification: GET /documentation/json returns valid OpenAPI spec, validated by
      openapi-validator
  - criterion: Interactive Swagger UI is accessible at /documentation with all endpoints
      testable
    verification: Navigate to /documentation, verify all routes visible, test sample
      requests return expected responses
  - criterion: All request/response schemas are validated and documented with examples
    verification: Each endpoint in spec has request/response schemas, examples, and
      validation works in runtime
  - criterion: TypeScript types are auto-generated from schemas maintaining type safety
    verification: npm run generate:types produces valid TypeScript interfaces matching
      API schemas
  - criterion: Documentation build integrates with CI/CD pipeline and stays current
    verification: turbo run docs:build generates static site, CI fails if schema validation
      fails
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/plugins/swagger.test.ts
      coverage_target: 90%
      scenarios:
      - Swagger plugin registration
      - Schema validation for valid/invalid requests
      - OpenAPI spec generation accuracy
      - Type generation from schemas
    - file: packages/backend/src/__tests__/schemas/validation.test.ts
      coverage_target: 85%
      scenarios:
      - Request schema validation success/failure
      - Response schema compliance
      - Schema composition and references
    integration_tests:
    - file: packages/backend/src/__tests__/integration/api-docs.test.ts
      scenarios:
      - Full OpenAPI spec generation from live routes
      - Swagger UI serves correctly
      - Schema validation on actual HTTP requests
      - Generated types match runtime behavior
    manual_testing:
    - step: Access /documentation and test user registration endpoint
      expected: Swagger UI loads, shows user schema, request succeeds with valid data
    - step: Verify schema validation rejects invalid comic upload request
      expected: API returns 400 with detailed validation errors matching schema
    - step: Check generated TypeScript types compile without errors
      expected: npm run type-check passes after schema changes
  estimates:
    development: 2.5
    code_review: 1
    testing: 0.8
    documentation: 0.4
    total: 4.7
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Install and configure @fastify/swagger and @fastify/swagger-ui dependencies'
      done: false
      ai_friendly: true
    - task: '[AI] Create swagger plugin with basic OpenAPI 3.1 configuration'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design schema architecture and decide public vs internal API
        exposure'
      done: false
      ai_friendly: false
    - task: '[AI] Convert existing TypeScript interfaces to JSON Schema format'
      done: false
      ai_friendly: true
    - task: '[AI] Create comprehensive schemas for auth, comics, and users endpoints'
      done: false
      ai_friendly: true
    - task: '[AI] Annotate existing route handlers with schema definitions'
      done: false
      ai_friendly: true
    - task: '[AI] Implement type generation script from JSON schemas'
      done: false
      ai_friendly: true
    - task: '[AI] Create validation script for documentation completeness'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive test suite for schema validation and docs generation'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review schema design, security implications, and documentation
        architecture'
      done: false
      ai_friendly: false
    - task: '[AI] Integrate docs build into turbo pipeline with CI/CD validation'
      done: false
      ai_friendly: true
    - task: '[AI] Create API integration guide and examples documentation'
      done: false
      ai_friendly: true
- key: T33
  title: Code Review & Standards Process
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 1
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Code review and standards processes are critical for a team using AI-assisted
      development extensively. With GitHub Copilot generating significant portions
      of code, we need robust processes to ensure AI-generated code meets quality
      standards, follows project conventions, and maintains architectural consistency.
      This is especially important for Morpheus given the complex tech stack (Fastify,
      Next.js, Supabase, ML integrations) where inconsistent patterns could create
      maintenance nightmares. The process needs to catch AI hallucinations, enforce
      TypeScript best practices, and ensure security standards for API endpoints handling
      user data and ML model integrations.


      **Technical Approach:**

      Implement automated code quality gates using ESLint + Prettier + TypeScript
      strict mode, integrate SonarQube or CodeClimate for code quality metrics, set
      up Danger.js for automated PR checks, create custom ESLint rules for Morpheus-specific
      patterns (e.g., API response shapes, error handling), implement pre-commit hooks
      with Husky + lint-staged, and establish PR templates with AI-specific review
      checklists. Use Conventional Commits for consistent commit messages and semantic-release
      for automated versioning. Integrate with GitHub Actions for CI/CD pipeline that
      blocks merges on quality gate failures.


      **AI Suitability Analysis:**

      - High AI effectiveness: ESLint rule configurations, Prettier setup, GitHub
      Actions workflows, PR template creation, Husky hook scripts, Jest/Vitest test
      configurations

      - Medium AI effectiveness: Custom ESLint rule implementations, Danger.js rule
      logic, SonarQube configuration, documentation templates

      - Low AI effectiveness: Defining code review criteria, establishing architectural
      guidelines, security review processes, deciding on acceptable complexity thresholds


      **Dependencies:**

      - External: @typescript-eslint/parser, @typescript-eslint/eslint-plugin, prettier,
      husky, lint-staged, danger, conventional-changelog, semantic-release, @commitlint/cli

      - Internal: Existing Turborepo workspace configuration, current TypeScript configs,
      CI/CD pipeline setup


      **Risks:**

      - Over-engineering review process: Start with essential rules, iterate based
      on team feedback

      - AI-generated code bypassing standards: Implement strict pre-commit hooks and
      required status checks

      - Performance impact of extensive linting: Use incremental linting and caching
      strategies

      - Team adoption resistance: Provide clear documentation and gradual rollout


      **Complexity Notes:**

      Initially seemed straightforward but complexity increases with AI-assisted development
      considerations. Need custom rules for AI-generated code patterns, automated
      detection of potential AI hallucinations (e.g., non-existent APIs), and processes
      for reviewing AI suggestions. AI can significantly speed up tooling setup but
      human oversight critical for rule definition and process design.


      **Key Files:**

      - .eslintrc.js: Main linting configuration

      - .prettierrc: Code formatting rules

      - .github/pull_request_template.md: Standardized PR reviews

      - .github/workflows/code-quality.yml: Automated quality checks

      - .husky/pre-commit: Pre-commit quality gates

      - docs/CODING_STANDARDS.md: Team guidelines

      - dangerfile.js: Automated PR feedback

      - package.json: Scripts and tool configurations

      '
    design_decisions:
    - decision: Use ESLint with TypeScript-specific rules as primary code quality
        tool
      rationale: Excellent TypeScript support, extensive plugin ecosystem, integrates
        well with AI tools like Copilot, and can catch common AI-generated code issues
      alternatives_considered:
      - TSLint (deprecated)
      - SWC linter (too new)
      - Rome/Biome (limited TypeScript support)
      ai_implementation_note: AI can generate comprehensive ESLint configurations
        and custom rules based on existing codebase patterns
    - decision: Implement Danger.js for automated PR review feedback
      rationale: Provides automated context-aware feedback, can check for AI-generated
        code patterns, and reduces manual review overhead for common issues
      alternatives_considered:
      - GitHub Actions only
      - SonarQube PR decoration
      - Custom webhook solution
      ai_implementation_note: AI excellent at creating Danger.js rules for common
        scenarios like missing tests, large PRs, or documentation updates
    - decision: Use Conventional Commits with commitlint for standardized commit messages
      rationale: Enables automated changelog generation, integrates with semantic-release,
        and provides structure for AI-assisted commit message generation
      alternatives_considered:
      - Free-form commits
      - Custom commit format
      - Gitmoji-based commits
      ai_implementation_note: AI can generate commitlint configurations and help team
        adopt conventional commit patterns
    researched_at: '2026-02-08T18:28:07.181298'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:10:42.162436'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 9a102db2
    planning_hash: 57c4cb36
  technical_notes:
    approach: 'Establish a multi-layered code quality process starting with pre-commit
      hooks for immediate feedback, followed by automated PR checks via GitHub Actions,
      and concluding with standardized human review using detailed checklists. Focus
      on TypeScript strict mode enforcement, consistent error handling patterns across
      Fastify APIs, and standardized React component patterns. Implement progressive
      rollout starting with new code, then gradually apply to existing codebase to
      minimize disruption while establishing quality culture.

      '
    external_dependencies:
    - name: '@typescript-eslint/eslint-plugin'
      version: ^6.0.0
      reason: TypeScript-specific linting rules and AI-generated code validation
    - name: husky
      version: ^8.0.3
      reason: Git hooks for pre-commit quality checks and preventing low-quality commits
    - name: lint-staged
      version: ^15.0.0
      reason: Run linting only on staged files for faster feedback loops
    - name: danger
      version: ^11.3.0
      reason: Automated PR review feedback and quality gate enforcement
    - name: '@commitlint/cli'
      version: ^18.0.0
      reason: Enforce conventional commit format for automated changelog generation
    - name: semantic-release
      version: ^22.0.0
      reason: Automated versioning and release notes based on commit history
    files_to_modify:
    - path: package.json
      changes: Add code quality dependencies, scripts for linting/formatting, Husky
        installation
    - path: turbo.json
      changes: Add lint and format tasks to pipeline configuration
    - path: apps/backend/package.json
      changes: Backend-specific linting scripts and dependencies
    - path: apps/web/package.json
      changes: Frontend-specific ESLint rules for React/Next.js
    new_files:
    - path: .eslintrc.js
      purpose: Root ESLint configuration with TypeScript and AI-generated code rules
    - path: .prettierrc
      purpose: Code formatting standards for consistent style
    - path: .github/pull_request_template.md
      purpose: Standardized PR template with AI code review checklist
    - path: .github/workflows/code-quality.yml
      purpose: Automated CI checks for code quality gates
    - path: .husky/pre-commit
      purpose: Pre-commit hooks for immediate quality feedback
    - path: .commitlintrc.js
      purpose: Conventional commit message enforcement
    - path: dangerfile.js
      purpose: Automated PR feedback and suggestions
    - path: docs/CODING_STANDARDS.md
      purpose: Team coding guidelines with AI-specific best practices
    - path: tools/eslint-rules/morpheus-patterns.js
      purpose: Custom ESLint rules for Morpheus-specific code patterns
    - path: sonar-project.properties
      purpose: SonarQube configuration for code quality metrics
  acceptance_criteria:
  - criterion: All code must pass automated quality gates before merge
    verification: GitHub Actions workflow 'code-quality' passes with ESLint, Prettier,
      and TypeScript checks on all PRs
  - criterion: Pre-commit hooks prevent low-quality code from reaching repository
    verification: Run 'git commit' with intentionally malformed code - should be blocked
      by Husky hooks
  - criterion: Pull requests follow standardized review process with AI-specific guidelines
    verification: Check .github/pull_request_template.md contains AI code review checklist
      and is auto-populated on new PRs
  - criterion: Code quality metrics are tracked and enforced
    verification: SonarQube integration reports <10% code duplication, >80% test coverage,
      and A-grade maintainability
  - criterion: Team coding standards are documented and enforced
    verification: docs/CODING_STANDARDS.md exists with AI-generated code guidelines,
      custom ESLint rules active in workspace
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/utils/code-quality.test.ts
      coverage_target: 90%
      scenarios:
      - ESLint rule validation for API response shapes
      - Custom rule detection of AI hallucination patterns
      - TypeScript strict mode compliance checks
      - Error handling pattern validation
    integration_tests:
    - file: .github/workflows/__tests__/code-quality.test.yml
      scenarios:
      - Full CI pipeline with quality gates
      - PR merge blocking on quality failures
      - Incremental linting on changed files only
    manual_testing:
    - step: Create PR with intentionally poor code quality
      expected: Danger.js comments on PR with specific improvement suggestions
    - step: Commit code with formatting issues locally
      expected: Pre-commit hook auto-fixes with Prettier and stages changes
    - step: Generate code with GitHub Copilot and commit
      expected: AI-specific ESLint rules validate generated patterns
  estimates:
    development: 2.5
    code_review: 1
    testing: 0.8
    documentation: 0.3
    total: 4.6
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Setup root workspace linting configuration (.eslintrc.js, .prettierrc)'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define Morpheus-specific coding standards and AI code review
        criteria'
      done: false
      ai_friendly: false
    - task: '[AI] Create GitHub Actions workflow for automated code quality checks'
      done: false
      ai_friendly: true
    - task: '[AI] Implement pre-commit hooks with Husky and lint-staged configuration'
      done: false
      ai_friendly: true
    - task: '[AI] Generate custom ESLint rules for API response shapes and error handling
        patterns'
      done: false
      ai_friendly: true
    - task: '[AI] Create PR template with AI-specific review checklist'
      done: false
      ai_friendly: true
    - task: '[AI] Setup Danger.js for automated PR feedback and conventional commits'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure SonarQube integration and define quality gate thresholds'
      done: false
      ai_friendly: false
    - task: '[AI] Write comprehensive documentation for coding standards and processes'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Team review of standards, pilot testing, and process refinement'
      done: false
      ai_friendly: false
- key: T37
  title: Wolne Lektury API Integration
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: ingestion
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      Wolne Lektury (wolnelektury.pl) is Poland''s largest digital library of public
      domain literature, offering 6,000+ books in multiple formats via REST API. For
      Morpheus, this integration enables automatic ingestion of classic literature
      as source material for comic transformation. This solves the content acquisition
      problem by providing legally safe, high-quality literary works that users can
      transform into comics without copyright concerns. Business value includes reduced
      legal risk, premium content library, and appeal to educational markets.


      **Technical Approach:**

      Implement as a dedicated ingestion service using Fastify plugins pattern:

      - REST client using `@fastify/http-proxy` or `undici` for API calls

      - Queue-based processing with `bullmq` for batch ingestion

      - Content parsing service to extract chapters/sections from various formats
      (HTML, TXT, PDF)

      - Metadata normalization to map Wolne Lektury schema to Morpheus book schema

      - Rate limiting and caching layer using `@fastify/rate-limit` and Redis

      - Database integration with Supabase for storing imported books and tracking
      sync status


      **AI Suitability Analysis:**

      - High AI effectiveness: API client boilerplate, data transformation schemas,
      unit tests, CRUD operations for book storage, error handling patterns

      - Medium AI effectiveness: Queue job implementations, metadata mapping logic,
      content parsing utilities, integration tests

      - Low AI effectiveness: Rate limiting strategy design, content chunking algorithms
      for comic panels, API authentication flow architecture


      **Dependencies:**

      - External: undici (HTTP client), bullmq (job queue), ioredis (Redis client),
      cheerio (HTML parsing), pdf-parse (PDF extraction)

      - Internal: Database schemas (books, authors, genres), authentication service,
      content processing pipeline, logging service


      **Risks:**

      - API rate limiting: implement exponential backoff and respect API limits (likely
      100 req/min)

      - Large file processing: stream-based parsing to avoid memory issues with full
      book content

      - Data consistency: transaction-based imports to handle partial failures

      - API changes: version the integration and add fallback mechanisms


      **Complexity Notes:**

      Medium complexity - higher than typical CRUD due to external API integration
      and content processing requirements. AI can significantly accelerate development
      of data transformation and API client code (~60% velocity boost), but architecture
      decisions around queuing and content parsing require human oversight.


      **Key Files:**

      - apps/api/src/plugins/wolne-lektury.ts: Main plugin registration

      - apps/api/src/services/ingestion/wolne-lektury-client.ts: API client

      - apps/api/src/jobs/wolne-lektury-sync.ts: Background sync jobs

      - apps/api/src/schemas/wolne-lektury.ts: API response types

      - packages/database/migrations/: Book import tracking tables

      '
    design_decisions:
    - decision: Use queue-based ingestion instead of real-time API calls
      rationale: Wolne Lektury has rate limits and large book content requires processing
        time. Queues enable reliable batch processing and retry logic.
      alternatives_considered:
      - Direct API calls
      - Scheduled cron jobs
      - Event-driven webhooks
      ai_implementation_note: AI can generate complete BullMQ job definitions and
        error handling patterns from existing queue examples
    - decision: Implement content chunking service for comic panel preparation
      rationale: Books need to be pre-processed into manageable chunks for LLM analysis
        and comic panel generation
      alternatives_considered:
      - Process full chapters
      - Dynamic chunking during comic creation
      - Fixed word-count segments
      ai_implementation_note: AI can help with text splitting algorithms but human
        input needed for optimal chunk size strategy
    - decision: Cache book metadata locally with sync tracking
      rationale: Reduces API calls, enables offline operation, and provides faster
        search/browse experience
      alternatives_considered:
      - Always fetch from API
      - Simple Redis cache
      - Full book content caching
      ai_implementation_note: AI excellent for generating cache invalidation logic
        and sync status tracking CRUD operations
    researched_at: '2026-02-08T18:28:37.137255'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:11:07.737187'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 3adb9d4f
    planning_hash: d931756d
  technical_notes:
    approach: 'Create a Fastify plugin that registers Wolne Lektury API client as
      a service, with background jobs for syncing book catalog and individual book
      content. Implement streaming content parser that chunks books into comic-ready
      segments. Use Redis for caching and BullMQ for reliable processing. Store normalized
      book data in Supabase with sync status tracking for incremental updates.

      '
    external_dependencies:
    - name: undici
      version: ^6.0.0
      reason: Fast HTTP client for Wolne Lektury API calls with built-in connection
        pooling
    - name: bullmq
      version: ^5.0.0
      reason: Reliable job queue for batch book processing and sync operations
    - name: cheerio
      version: ^1.0.0
      reason: Server-side HTML parsing for extracting book content from HTML format
    - name: pdf-parse
      version: ^1.1.1
      reason: Extract text content from PDF format books
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for caching and BullMQ queue backend
    files_to_modify:
    - path: apps/api/src/app.ts
      changes: Register wolne-lektury plugin and job queue initialization
    - path: packages/database/src/schema.ts
      changes: Add book imports tracking table and sync status fields
    - path: apps/api/src/config/index.ts
      changes: Add Wolne Lektury API configuration and Redis settings
    new_files:
    - path: apps/api/src/plugins/wolne-lektury.ts
      purpose: Main plugin registration with routes and services
    - path: apps/api/src/services/ingestion/wolne-lektury-client.ts
      purpose: HTTP client for Wolne Lektury API with rate limiting
    - path: apps/api/src/services/ingestion/content-parser.ts
      purpose: Multi-format content parsing and chunking service
    - path: apps/api/src/jobs/wolne-lektury-sync.ts
      purpose: Background job definitions for catalog and book sync
    - path: apps/api/src/schemas/wolne-lektury.ts
      purpose: TypeScript types for API responses and internal data models
    - path: packages/database/migrations/20240115_add_book_imports.sql
      purpose: Database schema for imported books and sync tracking
    - path: apps/api/src/routes/admin/ingestion.ts
      purpose: Admin endpoints for triggering and monitoring sync jobs
  acceptance_criteria:
  - criterion: Wolne Lektury API client successfully fetches book catalog and individual
      book content
    verification: Run integration test suite against live API - `npm test -- wolne-lektury-client`
  - criterion: Background sync job processes books without memory leaks for files
      up to 10MB
    verification: Monitor memory usage during test sync of 100 books - check Redis
      metrics and job completion logs
  - criterion: Book content is parsed and chunked into comic-ready segments with proper
      metadata
    verification: Verify database contains parsed books with chapter/section breakdown
      - query books table for imported content
  - criterion: Rate limiting respects API constraints (100 req/min) with exponential
      backoff
    verification: Load test with 200 requests - verify 429 responses are handled gracefully
      and retried
  - criterion: System recovers gracefully from partial failures and API downtime
    verification: Simulate API failures during sync - verify jobs are retried and
      data consistency is maintained
  testing:
    unit_tests:
    - file: apps/api/src/services/ingestion/__tests__/wolne-lektury-client.test.ts
      coverage_target: 90%
      scenarios:
      - Successful API responses for catalog and book endpoints
      - Rate limiting and retry logic with exponential backoff
      - Error handling for network failures and invalid responses
      - Response schema validation and data transformation
    - file: apps/api/src/services/ingestion/__tests__/content-parser.test.ts
      coverage_target: 85%
      scenarios:
      - HTML content parsing with chapter detection
      - Text file chunking for comic panels
      - PDF extraction and text normalization
      - Metadata extraction from various formats
    integration_tests:
    - file: apps/api/src/__tests__/integration/wolne-lektury-sync.test.ts
      scenarios:
      - End-to-end book import workflow from API to database
      - Queue job processing with Redis and BullMQ
      - Incremental sync with existing books
      - Error recovery and transaction rollback
    manual_testing:
    - step: Trigger full catalog sync via admin endpoint
      expected: Books appear in database with proper metadata and content chunks
    - step: Monitor sync job progress in Redis dashboard
      expected: Jobs complete successfully with proper error reporting
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create TypeScript schemas for Wolne Lektury API responses and book
        models'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design database schema for books, authors, and sync tracking
        with proper indexes'
      done: false
      ai_friendly: false
    - task: '[AI] Implement HTTP client with undici, rate limiting, and retry logic'
      done: false
      ai_friendly: true
    - task: '[AI] Create content parser service for HTML, TXT, and PDF formats using
        cheerio and pdf-parse'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design content chunking algorithm for optimal comic panel conversion'
      done: false
      ai_friendly: false
    - task: '[AI] Implement BullMQ job definitions for catalog sync and individual
        book processing'
      done: false
      ai_friendly: true
    - task: '[AI] Create Fastify plugin with routes for triggering sync and checking
        status'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit tests for all service components'
      done: false
      ai_friendly: true
    - task: '[AI] Implement database migration and CRUD operations for book storage'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Integration testing with live API and performance optimization'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Code review focusing on error handling, security, and architecture'
      done: false
      ai_friendly: false
- key: T38
  title: Book Upload Handler
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: ingestion
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      Book Upload Handler is the entry point for the Morpheus ingestion pipeline,
      allowing users to upload novel files (PDF, EPUB, TXT) through the dashboard.
      This is critical for M1 as it enables the core workflow: upload  text extraction
       chapter segmentation  comic generation. Without robust file handling, the
      entire transformation pipeline fails. Business value includes supporting multiple
      formats, validating content, and providing upload progress feedback.


      **Technical Approach:**

      Implement a multipart file upload endpoint using Fastify''s native multipart
      support with @fastify/multipart. Use streaming for large files to avoid memory
      issues. Store files temporarily in local storage during processing, then move
      to Supabase Storage for persistence. Integrate with a file validation service
      to check format, size, and content. Use bullmq for async processing queue to
      handle extraction jobs. Implement upload progress via WebSocket or Server-Sent
      Events.


      **AI Suitability Analysis:**

      - High AI effectiveness: File validation schemas, multipart parsing boilerplate,
      error handling patterns, unit tests for upload scenarios, OpenAPI documentation

      - Medium AI effectiveness: Fastify route handlers, file streaming logic, integration
      with Supabase Storage, progress tracking implementation

      - Low AI effectiveness: File format detection algorithms, upload strategy architecture
      decisions, security policy design, storage organization patterns


      **Dependencies:**

      - External: @fastify/multipart, @supabase/storage-js, file-type, bullmq, ioredis

      - Internal: Database schemas for upload tracking, authentication middleware,
      file processing services, WebSocket connection manager


      **Risks:**

      - Large file uploads: Use streaming and implement file size limits (500MB max)

      - Security vulnerabilities: Validate file types, scan for malicious content,
      sanitize filenames

      - Storage costs: Implement cleanup policies, compress files where possible

      - Concurrent uploads: Rate limiting, user upload quotas, queue management


      **Complexity Notes:**

      More complex than initially estimated due to multiple file formats and streaming
      requirements. AI will significantly accelerate boilerplate code and testing,
      but architectural decisions around file processing pipeline require human design.
      Estimate 40% faster development with AI assistance.


      **Key Files:**

      - apps/api/src/routes/upload.ts: Main upload endpoint implementation

      - apps/api/src/services/fileProcessor.ts: File validation and processing logic

      - apps/api/src/schemas/upload.ts: Validation schemas for upload requests

      - packages/database/migrations/: Upload tracking tables

      - apps/dashboard/src/components/BookUpload.tsx: Frontend upload component

      '
    design_decisions:
    - decision: Use streaming multipart upload with temporary local storage
      rationale: Handles large files efficiently, provides better error recovery than
        direct-to-cloud uploads
      alternatives_considered:
      - Direct Supabase Storage upload
      - Base64 encoding
      - Chunked upload
      ai_implementation_note: AI can generate streaming boilerplate and error handling
        patterns
    - decision: Implement async processing queue with BullMQ
      rationale: Decouples upload from processing, enables retry logic, provides job
        monitoring
      alternatives_considered:
      - Synchronous processing
      - Simple setTimeout
      - AWS SQS
      ai_implementation_note: AI excels at queue job definitions and worker implementations
    - decision: Support PDF, EPUB, TXT formats with extensible validation
      rationale: Covers 95% of novel formats, plugin architecture allows future expansion
      alternatives_considered:
      - PDF only
      - All formats via universal parser
      - DOCX support
      ai_implementation_note: AI can generate format-specific validation logic and
        parsers
    researched_at: '2026-02-08T18:29:02.601601'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:11:32.490663'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 8cd8487c
    planning_hash: 455eba50
  technical_notes:
    approach: 'Create a Fastify multipart endpoint that streams uploaded files to
      temporary storage while validating format and size. Implement a file processor
      service that extracts metadata and queues processing jobs. Use Supabase Storage
      for permanent file storage with organized folder structure. Provide real-time
      upload progress via WebSocket connections and comprehensive error handling for
      network issues, format problems, and storage failures.

      '
    external_dependencies:
    - name: '@fastify/multipart'
      version: ^8.0.0
      reason: Native Fastify multipart file upload support with streaming
    - name: file-type
      version: ^19.0.0
      reason: Reliable file format detection from binary signatures
    - name: bullmq
      version: ^5.0.0
      reason: Redis-based job queue for async file processing
    - name: '@supabase/storage-js'
      version: ^2.5.0
      reason: Supabase Storage integration for file persistence
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for BullMQ job queue backend
    files_to_modify:
    - path: apps/api/src/server.ts
      changes: Register multipart plugin and upload routes
    - path: apps/api/src/middleware/auth.ts
      changes: Add upload route authentication
    - path: packages/database/src/schema.ts
      changes: Add uploads table schema
    new_files:
    - path: apps/api/src/routes/upload.ts
      purpose: Main upload endpoint with multipart handling
    - path: apps/api/src/services/fileProcessor.ts
      purpose: File validation, metadata extraction, and storage operations
    - path: apps/api/src/services/uploadProgress.ts
      purpose: WebSocket-based progress tracking service
    - path: apps/api/src/schemas/upload.ts
      purpose: Zod schemas for upload request/response validation
    - path: apps/api/src/queues/fileProcessing.ts
      purpose: BullMQ queue setup for async file processing
    - path: apps/api/src/utils/fileValidation.ts
      purpose: File type detection and security validation utilities
    - path: packages/database/migrations/004_create_uploads_table.sql
      purpose: Database schema for upload tracking
  acceptance_criteria:
  - criterion: System accepts PDF, EPUB, and TXT files up to 500MB via multipart upload
    verification: curl -F 'file=@test-book.pdf' http://localhost:3000/api/upload returns
      202 status
  - criterion: Upload progress is tracked and communicated to frontend via WebSocket
    verification: WebSocket connection receives progress events 0-100% during file
      upload
  - criterion: Files are validated for format, size, and stored in Supabase with metadata
    verification: Database shows upload record with file_path, size, format after
      successful upload
  - criterion: Processing job is queued in BullMQ after successful upload
    verification: Redis queue 'file-processing' contains job with upload_id after
      upload completes
  - criterion: Upload endpoint handles errors gracefully with appropriate HTTP status
      codes
    verification: Invalid file types return 400, oversized files return 413, server
      errors return 500
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/fileProcessor.test.ts
      coverage_target: 90%
      scenarios:
      - File format validation (PDF, EPUB, TXT)
      - File size validation and limits
      - Metadata extraction from different formats
      - Error handling for corrupted files
    - file: apps/api/src/__tests__/routes/upload.test.ts
      coverage_target: 85%
      scenarios:
      - Successful multipart upload flow
      - Invalid file type rejection
      - File size limit enforcement
      - Authentication requirement
      - Rate limiting behavior
    integration_tests:
    - file: apps/api/src/__tests__/integration/upload-flow.test.ts
      scenarios:
      - Complete upload to storage pipeline
      - WebSocket progress tracking
      - BullMQ job creation
      - Database record persistence
    manual_testing:
    - step: Upload 100MB PDF through dashboard interface
      expected: Progress bar shows incremental updates, file appears in user's library
    - step: Attempt upload of 600MB file
      expected: Error message displayed, upload rejected before processing
    - step: Upload malicious .exe file renamed to .pdf
      expected: File type validation fails, upload rejected with clear error
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.4
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create upload table migration with fields for file metadata'
      done: false
      ai_friendly: true
    - task: '[AI] Implement Zod schemas for upload validation in schemas/upload.ts'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design file storage organization strategy in Supabase'
      done: false
      ai_friendly: false
    - task: '[AI] Create file validation utilities with type detection and size checks'
      done: false
      ai_friendly: true
    - task: '[AI] Implement multipart upload route handler with streaming'
      done: false
      ai_friendly: true
    - task: '[AI] Build file processor service with metadata extraction'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure security policies and rate limiting parameters'
      done: false
      ai_friendly: false
    - task: '[AI] Implement WebSocket progress tracking service'
      done: false
      ai_friendly: true
    - task: '[AI] Create BullMQ queue integration for processing jobs'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit tests for all services'
      done: false
      ai_friendly: true
    - task: '[AI] Write integration tests for complete upload flow'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Security review of file handling and validation logic'
      done: false
      ai_friendly: false
    - task: '[AI] Generate OpenAPI documentation for upload endpoints'
      done: false
      ai_friendly: true
- key: T39
  title: Chapter Extraction & Parsing
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T38
  agent_notes:
    research_findings: '**Context:**

      Chapter Extraction & Parsing is a critical ingestion service that takes raw
      novel text (uploaded files, pasted content, or API imports) and intelligently
      segments it into structured chapters with metadata. This solves the problem
      of converting unstructured narrative content into manageable, comic-adaptable
      units. Essential for the comic transformation pipeline as it creates the foundational
      data structure that downstream services (scene detection, panel generation)
      depend on. High business value as poor chapter segmentation directly impacts
      comic quality and user experience.


      **Technical Approach:**

      Implement as a dedicated microservice within the Fastify backend using a multi-stage
      parsing pipeline:

      1. Text preprocessing (encoding detection, cleaning)

      2. Chapter boundary detection (regex patterns + LLM validation)

      3. Metadata extraction (titles, summaries, character lists)

      4. Content analysis (word count, complexity scoring)

      5. Database persistence with full-text search indexing


      Use streaming processing for large files, implement caching for repeated operations,
      and design for extensibility to support multiple novel formats (PDF, EPUB, TXT,
      DOCX).


      **AI Suitability Analysis:**

      - High AI effectiveness: Text preprocessing utilities, regex pattern generation,
      CRUD operations for chapter storage, unit tests for parsing functions, TypeScript
      interfaces for chapter schemas

      - Medium AI effectiveness: LLM integration code, file format handlers, database
      migration scripts, API endpoint implementation

      - Low AI effectiveness: Chapter boundary detection algorithm design, LLM prompt
      engineering for content analysis, performance optimization strategies, error
      handling for edge cases


      **Dependencies:**

      - External: mammoth (DOCX), pdf-parse (PDF), epub2 (EPUB), iconv-lite (encoding),
      compromise (NLP), tiktoken (token counting)

      - Internal: Database schemas, file upload service, job queue system, logging
      infrastructure


      **Risks:**

      - Memory exhaustion on large files: Implement streaming and chunked processing

      - Inconsistent chapter detection across genres: Create genre-specific detection
      patterns with fallback rules

      - LLM API rate limits/costs: Implement intelligent caching and batch processing

      - Unicode/encoding issues: Use robust encoding detection and normalization

      - Performance degradation: Add processing time limits and progress tracking


      **Complexity Notes:**

      Initially appears straightforward but complexity increases significantly when
      handling diverse novel formats and genres. AI can accelerate development of
      standard parsing utilities by 60-70%, but core algorithm design and LLM integration
      require human architectural decisions. Estimate 40% AI-assisted implementation
      overall.


      **Key Files:**

      - packages/api/src/services/ChapterExtractionService.ts: Main service logic

      - packages/api/src/routes/ingestion.ts: API endpoints

      - packages/database/migrations/: Chapter schema tables

      - packages/api/src/utils/textParsers/: Format-specific parsers

      - packages/api/src/jobs/: Background processing jobs

      '
    design_decisions:
    - decision: Multi-stage pipeline with LLM validation
      rationale: Combines speed of rule-based detection with accuracy of AI validation,
        allows fallback strategies
      alternatives_considered:
      - Pure rule-based parsing
      - LLM-only detection
      - ML model training
      ai_implementation_note: AI agent can generate comprehensive test cases and implement
        pipeline stages, but requires human design of validation logic
    - decision: Streaming processing for large files
      rationale: Prevents memory issues, enables real-time progress updates, supports
        files >100MB
      alternatives_considered:
      - In-memory processing
      - Temporary file chunking
      ai_implementation_note: AI excellent for implementing stream utilities and chunk
        processing logic
    - decision: PostgreSQL full-text search with chapter content indexing
      rationale: Leverages existing Supabase infrastructure, enables fast content
        searches across chapters
      alternatives_considered:
      - Elasticsearch
      - Simple LIKE queries
      - External search service
      ai_implementation_note: AI can generate indexing migrations and search query
        builders effectively
    researched_at: '2026-02-08T18:29:31.389326'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:12:01.188708'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 04ff7656
    planning_hash: e21aa0c4
  technical_notes:
    approach: 'Build a ChapterExtractionService that processes uploaded novels through
      a configurable pipeline: detect encoding  clean text  identify chapter boundaries
      using regex patterns  validate boundaries with LLM calls  extract metadata
       store in PostgreSQL with search indexing. Implement as background jobs for
      large files with WebSocket progress updates. Use caching to avoid re-processing
      identical content and implement graceful degradation when LLM services are unavailable.

      '
    external_dependencies:
    - name: mammoth
      version: ^1.6.0
      reason: DOCX file parsing with good formatting preservation
    - name: pdf-parse
      version: ^1.1.1
      reason: PDF text extraction, handles most PDF formats
    - name: epub2
      version: ^3.0.1
      reason: EPUB parsing for e-book format support
    - name: iconv-lite
      version: ^0.6.3
      reason: Character encoding detection and conversion
    - name: compromise
      version: ^14.11.0
      reason: Natural language processing for content analysis
    - name: tiktoken
      version: ^1.0.10
      reason: Token counting for LLM API optimization
    files_to_modify:
    - path: packages/api/src/routes/ingestion.ts
      changes: Add POST /upload, GET /chapters/:id, GET /chapters/search endpoints
    - path: packages/api/src/services/JobQueueService.ts
      changes: Register chapter extraction job handlers
    - path: packages/database/src/schema.ts
      changes: Add chapters, novels, and chapter_search tables
    new_files:
    - path: packages/api/src/services/ChapterExtractionService.ts
      purpose: Main orchestration service for chapter parsing pipeline
    - path: packages/api/src/utils/textParsers/PDFParser.ts
      purpose: PDF text extraction using pdf-parse
    - path: packages/api/src/utils/textParsers/DOCXParser.ts
      purpose: DOCX parsing using mammoth
    - path: packages/api/src/utils/textParsers/EPUBParser.ts
      purpose: EPUB chapter extraction using epub2
    - path: packages/api/src/utils/textParsers/TextPreprocessor.ts
      purpose: Text cleaning, encoding detection, normalization
    - path: packages/api/src/utils/ChapterDetector.ts
      purpose: Regex patterns and boundary detection logic
    - path: packages/api/src/utils/MetadataExtractor.ts
      purpose: Extract titles, summaries, character lists from chapter text
    - path: packages/api/src/jobs/ChapterExtractionJob.ts
      purpose: Background job handler with progress tracking
    - path: packages/api/src/integrations/LLMService.ts
      purpose: OpenAI/Anthropic integration for validation and analysis
    - path: packages/database/migrations/20241201_create_chapters.sql
      purpose: Database schema for chapters and full-text search
  acceptance_criteria:
  - criterion: System successfully extracts chapters from TXT, PDF, DOCX, and EPUB
      files with >90% accuracy on common chapter patterns
    verification: Upload test novels in each format, verify chapters match expected
      segmentation via GET /api/ingestion/chapters/{novelId}
  - criterion: Processing completes within 30 seconds for files <10MB, with streaming
      progress updates via WebSocket
    verification: Monitor /api/ingestion/upload endpoint response times and WebSocket
      events during file processing
  - criterion: Extracted chapters include metadata (title, word count, character count,
      summary) and are searchable
    verification: Query database chapters table and test full-text search via GET
      /api/ingestion/chapters/search endpoint
  - criterion: Service gracefully handles malformed files and provides meaningful
      error messages
    verification: Upload corrupted/invalid files, verify 400/422 responses with descriptive
      error messages
  - criterion: LLM integration validates chapter boundaries and extracts summaries
      when available, falls back to rule-based parsing when offline
    verification: Test with LLM service enabled/disabled, verify chapter extraction
      still functions in both modes
  testing:
    unit_tests:
    - file: packages/api/src/__tests__/services/ChapterExtractionService.test.ts
      coverage_target: 90%
      scenarios:
      - Text preprocessing and cleaning
      - Chapter boundary detection patterns
      - Metadata extraction from chapter content
      - Error handling for invalid input
      - Caching behavior for duplicate content
    - file: packages/api/src/__tests__/utils/textParsers.test.ts
      coverage_target: 85%
      scenarios:
      - PDF text extraction
      - DOCX parsing with formatting
      - EPUB chapter detection
      - Encoding detection and conversion
    integration_tests:
    - file: packages/api/src/__tests__/integration/ingestion.test.ts
      scenarios:
      - End-to-end file upload and chapter extraction
      - Database persistence and retrieval
      - WebSocket progress notifications
      - LLM service integration
    manual_testing:
    - step: Upload 'Pride and Prejudice' TXT file via web interface
      expected: 61 chapters extracted with proper titles and metadata
    - step: Upload large PDF novel (>5MB)
      expected: Progress bar shows incremental updates, completes successfully
    - step: Search for 'wedding' across extracted chapters
      expected: Relevant chapters returned with highlighted snippets
  estimates:
    development: 4.5
    code_review: 0.5
    testing: 1.2
    documentation: 0.3
    total: 6.5
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create database migration for chapters/novels tables with indexes'
      done: false
      ai_friendly: true
    - task: '[AI] Implement basic file parser utilities (PDF, DOCX, EPUB, TXT)'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design chapter boundary detection algorithm and regex patterns'
      done: false
      ai_friendly: false
    - task: '[AI] Implement TextPreprocessor with encoding detection and cleaning'
      done: false
      ai_friendly: true
    - task: '[AI] Create ChapterExtractionService class structure and interfaces'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design LLM prompts for chapter validation and metadata extraction'
      done: false
      ai_friendly: false
    - task: '[AI] Implement background job processing with progress tracking'
      done: false
      ai_friendly: true
    - task: '[AI] Add ingestion API endpoints with file upload handling'
      done: false
      ai_friendly: true
    - task: '[AI] Write comprehensive unit tests for all parser utilities'
      done: false
      ai_friendly: true
    - task: '[AI] Implement caching layer for duplicate content detection'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Performance optimization and memory management review'
      done: false
      ai_friendly: false
    - task: '[AI] Add integration tests for end-to-end workflows'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Security review for file upload and processing vulnerabilities'
      done: false
      ai_friendly: false
- key: T40
  title: Text Preprocessing Pipeline
  type: Task
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T39
  agent_notes:
    research_findings: '**Context:**

      Text preprocessing is crucial for novel-to-comic transformation as it prepares
      raw novel text for scene segmentation, character extraction, and dialogue identification.
      This pipeline must handle various text formats, clean inconsistent formatting,
      extract structural elements (chapters, paragraphs, dialogue), and normalize
      text for downstream ML processing. Without proper preprocessing, the LLM-based
      scene generation and character analysis will produce poor results.


      **Technical Approach:**

      Implement a multi-stage pipeline using Node.js streams for memory efficiency
      with large novels. Use a plugin-based architecture where each preprocessing
      step is a separate module (encoding detection, text cleaning, structure extraction,
      dialogue parsing, character mention detection). Leverage libraries like `natural`
      for NLP tasks, `iconv-lite` for encoding, and `cheerio` for HTML cleanup if
      needed. Store intermediate results in PostgreSQL with JSONB for flexible schema
      evolution. Implement as Fastify service with queue-based processing using BullMQ
      for handling large files asynchronously.


      **AI Suitability Analysis:**

      - High AI effectiveness: Text cleaning utilities, regex patterns for dialogue
      detection, test data generation, CRUD operations for preprocessed data, error
      handling boilerplate

      - Medium AI effectiveness: Pipeline orchestration logic, database schema design,
      API endpoint implementations, integration with file upload service

      - Low AI effectiveness: Novel format detection algorithms, complex text structure
      analysis, performance optimization decisions, preprocessing strategy selection


      **Dependencies:**

      - External: natural (NLP), iconv-lite (encoding), cheerio (HTML parsing), bullmq
      (job queue), zod (validation), mammoth (docx parsing)

      - Internal: File storage service (for novel uploads), database models, logging
      service, metrics collection


      **Risks:**

      - Memory exhaustion with large novels: Use streaming and chunked processing

      - Encoding detection failures: Implement fallback strategies and user override
      options

      - Processing time for large files: Implement progress tracking and timeout handling

      - Text structure misidentification: Create confidence scores and manual review
      flags

      - Character encoding corruption: Validate at multiple pipeline stages


      **Complexity Notes:**

      Initially appears straightforward but complexity emerges from novel format variety
      (txt, epub, docx, PDF), encoding issues, and structure detection accuracy requirements.
      AI will significantly accelerate utility functions and test creation but human
      expertise needed for parsing strategies. Estimate 40% faster development with
      AI assistance.


      **Key Files:**

      - apps/backend/src/services/text-preprocessing/: Main service directory

      - apps/backend/src/services/text-preprocessing/pipeline.ts: Core pipeline orchestrator

      - apps/backend/src/services/text-preprocessing/processors/: Individual processing
      modules

      - apps/backend/src/routes/preprocessing.ts: API endpoints

      - packages/database/src/schema/preprocessed-text.sql: Database schema

      - apps/backend/src/queues/preprocessing-queue.ts: Background job processing

      '
    design_decisions:
    - decision: Stream-based processing with chunking
      rationale: Handles large novels without memory exhaustion, enables progress
        tracking, allows cancellation
      alternatives_considered:
      - In-memory processing
      - Temporary file-based processing
      ai_implementation_note: AI can generate stream processing boilerplate and chunk
        size optimization logic
    - decision: Plugin-based processor architecture
      rationale: Modular design allows easy addition of new preprocessing steps, testability,
        and reusability
      alternatives_considered:
      - Monolithic pipeline class
      - Function composition approach
      ai_implementation_note: AI excellent for generating individual processor modules
        and plugin registration system
    - decision: JSONB storage for preprocessed results
      rationale: Flexible schema for evolving preprocessing outputs, efficient querying,
        maintains structure
      alternatives_considered:
      - Separate tables per data type
      - File-based storage
      ai_implementation_note: AI can generate database models and query builders effectively
    researched_at: '2026-02-08T18:30:00.129089'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:12:40.391303'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 2c96e9b1
    planning_hash: b8638080
  technical_notes:
    approach: 'Create a streaming text preprocessing service that accepts novel files
      through Fastify endpoints, queues processing jobs with BullMQ, and runs files
      through configurable processor plugins (encoding detection, text cleaning, structure
      extraction, dialogue parsing). Store results in PostgreSQL with JSONB for flexibility,
      emit progress events via WebSocket, and provide APIs for retrieving processed
      text segments ready for LLM consumption.

      '
    external_dependencies:
    - name: natural
      version: ^6.0.0
      reason: NLP tokenization, sentence detection, and text analysis
    - name: iconv-lite
      version: ^0.6.3
      reason: Character encoding detection and conversion
    - name: cheerio
      version: ^1.0.0-rc.12
      reason: HTML parsing and cleanup for web-scraped content
    - name: bullmq
      version: ^4.0.0
      reason: Reliable job queue for async text processing
    - name: mammoth
      version: ^1.6.0
      reason: DOCX file parsing to extract text content
    - name: node-html-to-text
      version: ^9.0.5
      reason: Convert HTML content to clean plain text
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register preprocessing routes and WebSocket handlers
    - path: packages/database/src/schema/index.ts
      changes: Add preprocessed_texts table schema with JSONB fields
    - path: apps/backend/src/config/index.ts
      changes: Add preprocessing service configuration options
    new_files:
    - path: apps/backend/src/services/text-preprocessing/pipeline.ts
      purpose: Main pipeline orchestrator managing processor chain and data flow
    - path: apps/backend/src/services/text-preprocessing/processors/encoding-detector.ts
      purpose: Automatic text encoding detection and conversion to UTF-8
    - path: apps/backend/src/services/text-preprocessing/processors/text-cleaner.ts
      purpose: Remove formatting artifacts, normalize whitespace, fix common OCR errors
    - path: apps/backend/src/services/text-preprocessing/processors/structure-extractor.ts
      purpose: Identify chapters, paragraphs, scene breaks using NLP and pattern matching
    - path: apps/backend/src/services/text-preprocessing/processors/dialogue-parser.ts
      purpose: Extract dialogue segments and attempt speaker attribution
    - path: apps/backend/src/services/text-preprocessing/processors/character-detector.ts
      purpose: Identify character mentions using NER and frequency analysis
    - path: apps/backend/src/services/text-preprocessing/file-parsers/txt-parser.ts
      purpose: Plain text file parsing with encoding detection
    - path: apps/backend/src/services/text-preprocessing/file-parsers/docx-parser.ts
      purpose: DOCX parsing using mammoth library
    - path: apps/backend/src/services/text-preprocessing/file-parsers/epub-parser.ts
      purpose: EPUB parsing and content extraction
    - path: apps/backend/src/routes/preprocessing.ts
      purpose: Fastify routes for file upload, job status, configuration, and data
        retrieval
    - path: apps/backend/src/queues/preprocessing-queue.ts
      purpose: BullMQ job queue for async processing with progress tracking
    - path: apps/backend/src/websockets/preprocessing-events.ts
      purpose: WebSocket event handlers for real-time progress updates
    - path: packages/database/src/schema/preprocessed-text.sql
      purpose: Database schema for storing preprocessed text data and metadata
    - path: apps/backend/src/types/preprocessing.ts
      purpose: TypeScript interfaces for preprocessing data structures
    - path: apps/backend/src/services/text-preprocessing/config.ts
      purpose: Configuration management for processor parameters and pipeline settings
  acceptance_criteria:
  - criterion: Pipeline processes TXT, DOCX, and EPUB files with automatic encoding
      detection and outputs structured JSON with chapters, paragraphs, dialogue, and
      character mentions
    verification: Upload test files via POST /api/preprocessing/upload, verify JSONB
      output in database contains 'chapters', 'dialogue', 'characters' arrays with
      confidence scores
  - criterion: Streaming processing handles files up to 50MB without memory issues
      and provides real-time progress updates via WebSocket
    verification: Upload 50MB test novel, monitor memory usage <500MB peak, receive
      progress events every 10% completion via WebSocket connection
  - criterion: Queue system processes files asynchronously with failure recovery and
      provides status API for job monitoring
    verification: Submit multiple files, verify BullMQ dashboard shows jobs, test
      failure scenarios, check GET /api/preprocessing/jobs/{id} returns status
  - criterion: Plugin architecture allows configurable preprocessing steps with individual
      enable/disable and parameter tuning
    verification: POST /api/preprocessing/config with processor settings, verify pipeline
      skips disabled processors and applies custom parameters
  - criterion: API provides paginated access to processed text segments optimized
      for LLM consumption with metadata
    verification: GET /api/preprocessing/text/{jobId}/segments?page=1&limit=100 returns
      structured segments with word counts, character lists, and dialogue flags
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/text-preprocessing/pipeline.test.ts
      coverage_target: 90%
      scenarios:
      - Pipeline orchestration with all processors enabled
      - Error handling for corrupted files
      - Memory streaming for large files
      - Plugin enable/disable functionality
    - file: apps/backend/src/__tests__/services/text-preprocessing/processors/encoding-detector.test.ts
      coverage_target: 85%
      scenarios:
      - UTF-8, UTF-16, ISO-8859-1 detection
      - Malformed encoding fallback
      - Binary file rejection
    - file: apps/backend/src/__tests__/services/text-preprocessing/processors/dialogue-parser.test.ts
      coverage_target: 85%
      scenarios:
      - Quote-based dialogue extraction
      - Dialogue attribution to characters
      - Nested quote handling
      - Non-English dialogue patterns
    - file: apps/backend/src/__tests__/services/text-preprocessing/processors/structure-extractor.test.ts
      coverage_target: 85%
      scenarios:
      - Chapter detection via headings and patterns
      - Paragraph segmentation
      - Scene break identification
      - Table of contents parsing
    integration_tests:
    - file: apps/backend/src/__tests__/integration/preprocessing-pipeline.test.ts
      scenarios:
      - End-to-end file upload to processed segments API
      - WebSocket progress events during processing
      - Database storage and retrieval of processed data
      - Queue job failure and retry handling
    - file: apps/backend/src/__tests__/integration/preprocessing-api.test.ts
      scenarios:
      - File upload with various formats
      - Configuration updates and processor toggling
      - Pagination of processed segments
      - Job status tracking and cancellation
    manual_testing:
    - step: Upload Project Gutenberg novel (Pride and Prejudice) via web interface
      expected: Processing completes in <2 minutes, extracts 61 chapters, identifies
        main characters (Elizabeth, Darcy, etc.)
    - step: Upload DOCX file with complex formatting and embedded images
      expected: Text extracted cleanly, formatting metadata preserved, images ignored
        gracefully
    - step: Monitor WebSocket connection during large file processing
      expected: Progress updates every few seconds, final completion event with summary
        statistics
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create database schema and migration for preprocessed_texts table
        with JSONB fields'
      done: false
      ai_friendly: true
    - task: '[AI] Implement file parser classes for TXT, DOCX, and EPUB with error
        handling'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design processor plugin architecture and interface contracts
        for extensibility'
      done: false
      ai_friendly: false
    - task: '[AI] Implement encoding detector using iconv-lite with fallback strategies'
      done: false
      ai_friendly: true
    - task: '[AI] Create text cleaning processor with regex patterns for common formatting
        issues'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Develop structure extraction algorithms for chapter/paragraph
        detection with confidence scoring'
      done: false
      ai_friendly: false
    - task: '[AI] Implement dialogue parser with quote detection and basic speaker
        attribution'
      done: false
      ai_friendly: true
    - task: '[AI] Build character detection using natural library NER and frequency
        analysis'
      done: false
      ai_friendly: true
    - task: '[AI] Create pipeline orchestrator with streaming support and processor
        chain management'
      done: false
      ai_friendly: true
    - task: '[AI] Implement BullMQ job queue with progress tracking and failure recovery'
      done: false
      ai_friendly: true
    - task: '[AI] Build Fastify API endpoints for file upload, job management, and
        data retrieval'
      done: false
      ai_friendly: true
    - task: '[AI] Add WebSocket handlers for real-time progress events'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit tests for all processor modules'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for end-to-end pipeline functionality'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Performance testing and optimization for memory usage and processing
        speed'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Code review focusing on architecture decisions and error handling
        strategies'
      done: false
      ai_friendly: false
- key: T44
  title: Semantic Search Implementation
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T43
  agent_notes:
    research_findings: '**Context:**

      Semantic search enables users to find novels, comics, and content using natural
      language queries rather than exact keyword matches. For Morpheus, this is critical
      for content discovery - users should be able to search "fantasy novels with
      dragons and magic schools" and get relevant results even if those exact words
      don''t appear in titles/descriptions. This improves user experience and content
      discoverability across the platform''s novel-to-comic catalog.


      **Technical Approach:**

      Implement vector-based semantic search using OpenAI embeddings with PostgreSQL
      pgvector extension (already available in Supabase). Create embeddings for novel
      titles, descriptions, tags, and generated comic metadata. Use cosine similarity
      for matching user queries to content. Build search API endpoints in Fastify
      backend with caching layer (Redis) for frequently searched terms. Implement
      hybrid search combining semantic similarity with traditional full-text search
      for best results.


      **AI Suitability Analysis:**

      - High AI effectiveness: CRUD operations for embeddings storage, API route boilerplate,
      database migration scripts, unit tests for search endpoints, embedding batch
      processing utilities

      - Medium AI effectiveness: Search ranking algorithms, caching strategies, API
      integration with OpenAI, search result formatting/pagination

      - Low AI effectiveness: Vector similarity threshold tuning, search relevance
      scoring logic, embedding model selection, performance optimization strategies


      **Dependencies:**

      - External: @supabase/supabase-js (already in project), openai SDK, @pgvector/pgvector
      for PostgreSQL, ioredis for caching

      - Internal: Database service layer, content ingestion pipeline, authentication
      middleware, existing novel/comic models


      **Risks:**

      - Embedding costs: OpenAI API charges per token - mitigate with batch processing
      and caching

      - Search latency: Vector similarity can be slow - mitigate with proper indexing
      and result caching

      - Relevance tuning: Poor search results without proper weighting - mitigate
      with A/B testing framework

      - Storage growth: Embeddings consume significant database space - mitigate with
      embedding compression strategies


      **Complexity Notes:**

      This is moderately complex - higher than initial estimate due to vector database
      setup and relevance tuning requirements. However, AI can significantly accelerate
      implementation of boilerplate code, API endpoints, and testing. The main complexity
      lies in fine-tuning search quality, which requires human judgment and iterative
      testing.


      **Key Files:**

      - packages/backend/src/routes/search.ts: New search API endpoints

      - packages/backend/src/services/embedding.ts: Embedding generation service

      - packages/backend/src/services/search.ts: Core search logic

      - packages/database/migrations/: Add pgvector extension and embedding columns

      - packages/backend/src/workers/embedding-processor.ts: Background embedding
      generation

      '
    design_decisions:
    - decision: Use OpenAI text-embedding-3-small for generating embeddings
      rationale: Cost-effective, good performance for general text, 1536 dimensions
        manageable for PostgreSQL storage
      alternatives_considered:
      - text-embedding-3-large (higher cost)
      - Sentence Transformers (self-hosted complexity)
      - Cohere embeddings (vendor diversification)
      ai_implementation_note: AI can generate embedding API integration code, error
        handling, and batch processing logic
    - decision: Implement hybrid search combining semantic + full-text search
      rationale: Semantic search excels at conceptual matching, full-text handles
        exact terms better - combination provides best user experience
      alternatives_considered:
      - Pure semantic search
      - Pure full-text search
      - Separate semantic/text endpoints
      ai_implementation_note: AI can implement search result merging algorithms and
        scoring normalization
    - decision: Use PostgreSQL pgvector with cosine similarity
      rationale: Already using Supabase PostgreSQL, pgvector is mature and performant,
        cosine similarity works well for text embeddings
      alternatives_considered:
      - Pinecone vector database
      - Weaviate
      - Elasticsearch with dense_vector
      ai_implementation_note: AI can generate vector storage schemas, similarity queries,
        and indexing strategies
    researched_at: '2026-02-08T18:30:27.116104'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:13:06.490901'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: c55d6582
    planning_hash: 4bdcca9e
  technical_notes:
    approach: 'Create embedding service that generates vectors for novel/comic content
      using OpenAI API. Store embeddings in PostgreSQL with pgvector extension for
      efficient similarity search. Build search API that converts user queries to
      embeddings and finds similar content using cosine similarity. Implement caching
      layer for popular searches and background workers for embedding generation.
      Combine semantic results with PostgreSQL full-text search for hybrid relevance
      scoring.

      '
    external_dependencies:
    - name: pgvector
      version: ^0.5.0
      reason: PostgreSQL extension for vector similarity search operations
    - name: openai
      version: ^4.24.0
      reason: Generate text embeddings for semantic search functionality
    - name: ioredis
      version: ^5.3.0
      reason: Cache search results and embeddings to reduce API calls and improve
        performance
    - name: '@types/pg'
      version: ^8.10.0
      reason: TypeScript definitions for PostgreSQL vector operations
    files_to_modify:
    - path: packages/database/supabase/migrations/20240101000000_add_pgvector.sql
      changes: Enable pgvector extension and add embedding columns to novels/comics
        tables
    - path: packages/backend/src/types/content.ts
      changes: Add embedding vector type definitions and search result interfaces
    - path: packages/backend/src/config/database.ts
      changes: Add pgvector configuration and connection pool settings
    new_files:
    - path: packages/backend/src/services/embedding.ts
      purpose: OpenAI embedding generation, batch processing, and vector management
    - path: packages/backend/src/services/search.ts
      purpose: Semantic search logic, similarity calculations, and result ranking
    - path: packages/backend/src/services/cache.ts
      purpose: Redis caching layer for search results and embeddings
    - path: packages/backend/src/routes/search.ts
      purpose: Search API endpoints with pagination, filtering, and authentication
    - path: packages/backend/src/workers/embedding-processor.ts
      purpose: Background worker for generating embeddings for new/updated content
    - path: packages/backend/src/middleware/search-rate-limit.ts
      purpose: Rate limiting middleware specific to search endpoints
    - path: packages/backend/src/utils/vector-operations.ts
      purpose: Vector similarity calculations and embedding utilities
  acceptance_criteria:
  - criterion: Search API returns semantically relevant novels/comics for natural
      language queries
    verification: POST /api/search with query 'fantasy novels with magic schools'
      returns Harry Potter-style content even without exact keyword matches
  - criterion: Embedding generation processes all existing content within 24 hours
    verification: Run embedding worker and verify all novels/comics have non-null
      embedding vectors in database
  - criterion: Search response time under 500ms for cached queries, under 2s for new
      queries
    verification: Load test /api/search endpoint with Apache Bench - 95th percentile
      meets targets
  - criterion: Hybrid search combines semantic and full-text results with configurable
      weighting
    verification: Search results include both semantically similar content and exact
      keyword matches with relevance scores
  - criterion: Search API handles 1000+ concurrent users without degradation
    verification: Load test with 1000 concurrent requests maintains response times
      and accuracy
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/embedding.test.ts
      coverage_target: 90%
      scenarios:
      - Generate embeddings for novel content
      - Batch processing with rate limiting
      - Error handling for OpenAI API failures
      - Embedding vector validation
    - file: packages/backend/src/__tests__/services/search.test.ts
      coverage_target: 85%
      scenarios:
      - Semantic similarity search
      - Hybrid search scoring
      - Query preprocessing and validation
      - Empty/invalid query handling
    - file: packages/backend/src/__tests__/routes/search.test.ts
      coverage_target: 80%
      scenarios:
      - Search endpoint with valid queries
      - Pagination and filtering
      - Authentication and rate limiting
      - Malformed request handling
    integration_tests:
    - file: packages/backend/src/__tests__/integration/search-flow.test.ts
      scenarios:
      - End-to-end search from query to results
      - Embedding generation to search retrieval
      - Cache hit/miss behavior
      - Database connection and vector operations
    manual_testing:
    - step: Search for 'dark fantasy romance novels' via API
      expected: Returns relevant novels with romance and dark fantasy themes
    - step: Monitor embedding generation worker logs
      expected: Processes content without errors, updates database progressively
    - step: Test search with various query lengths and complexity
      expected: Maintains accuracy for short and long natural language queries
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create database migration for pgvector extension and embedding columns'
      done: false
      ai_friendly: true
    - task: '[AI] Implement embedding service with OpenAI API integration'
      done: false
      ai_friendly: true
    - task: '[AI] Build vector operations utilities for similarity calculations'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design hybrid search scoring algorithm and relevance weighting'
      done: false
      ai_friendly: false
    - task: '[AI] Implement search service with semantic and full-text combination'
      done: false
      ai_friendly: true
    - task: '[AI] Create Redis caching layer for search results'
      done: false
      ai_friendly: true
    - task: '[AI] Build search API routes with pagination and filtering'
      done: false
      ai_friendly: true
    - task: '[AI] Implement background embedding generation worker'
      done: false
      ai_friendly: true
    - task: '[AI] Generate comprehensive unit tests for all services'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Fine-tune similarity thresholds and search relevance parameters'
      done: false
      ai_friendly: false
    - task: '[AI] Create integration tests for end-to-end search flow'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Performance testing and optimization review'
      done: false
      ai_friendly: false
- key: T45
  title: Book Status & Progress Tracking
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: ingestion
  dependsOn:
  - T38
  agent_notes:
    research_findings: '**Context:**

      Book Status & Progress Tracking is essential for the novel-to-comic transformation
      pipeline. Users need visibility into where their book is in the transformation
      process (uploaded, processing, chapters generated, images created, etc.). This
      provides transparency, manages expectations, and enables proper error handling
      and recovery. Without this, users have no feedback on long-running AI operations,
      leading to poor UX and support burden.


      **Technical Approach:**

      Implement a state machine-based status system with real-time updates:

      - PostgreSQL enum for book statuses (UPLOADED, PARSING, CHAPTER_GENERATION,
      IMAGE_GENERATION, COMPLETED, FAILED)

      - Event-driven progress tracking with WebSocket notifications

      - Job queue integration (BullMQ) for async processing status updates

      - Progress percentage calculation based on completed steps

      - Audit trail for status changes with timestamps and metadata

      - Retry mechanisms for failed states with exponential backoff


      **AI Suitability Analysis:**

      - High AI effectiveness: CRUD operations for status/progress models, database
      migrations, basic API endpoints, unit tests, TypeScript interfaces, validation
      schemas

      - Medium AI effectiveness: WebSocket event handlers, job queue integration,
      state machine transitions, progress calculation logic

      - Low AI effectiveness: Status transition business rules, error recovery strategies,
      progress percentage algorithms, real-time architecture decisions


      **Dependencies:**

      - External: @fastify/websocket, bullmq, ioredis, zod (validation), @supabase/supabase-js

      - Internal: Database service, Authentication middleware, Job queue service,
      Notification service, Book ingestion pipeline


      **Risks:**

      - Race conditions in status updates: Use database transactions and optimistic
      locking

      - WebSocket connection management: Implement connection pooling and automatic
      reconnection

      - Status inconsistency across services: Event sourcing pattern with compensation
      logic

      - Performance with many concurrent books: Database indexing and pagination

      - Failed job recovery: Dead letter queues and manual intervention UI


      **Complexity Notes:**

      More complex than initially estimated due to real-time requirements and distributed
      state management. AI can significantly accelerate CRUD and boilerplate (60-70%
      of work), but state machine design and error handling require human architecture
      decisions. Estimated 40% velocity boost with AI assistance.


      **Key Files:**

      - packages/backend/src/models/book-status.ts: Status enums and interfaces

      - packages/backend/src/services/book-progress.ts: Progress tracking service

      - packages/backend/src/routes/books/status.ts: Status API endpoints

      - packages/backend/src/websocket/book-events.ts: Real-time notifications

      - packages/backend/src/jobs/status-updater.ts: Background status updates

      - packages/database/migrations/: Add status and progress tables

      - packages/frontend/src/hooks/useBookProgress.ts: Real-time status hook

      '
    design_decisions:
    - decision: State machine with PostgreSQL enums for book status management
      rationale: Ensures data consistency, prevents invalid state transitions, and
        provides clear audit trail. PostgreSQL enums are performant and type-safe.
      alternatives_considered:
      - Redis-based status cache
      - Event sourcing with separate status store
      - Simple string status field
      ai_implementation_note: AI can generate enum definitions, migration files, and
        basic CRUD operations. Human defines valid state transitions.
    - decision: WebSocket-based real-time progress updates with fallback polling
      rationale: Provides immediate user feedback for long-running operations. Fallback
        ensures reliability when WebSocket connections fail.
      alternatives_considered:
      - Server-sent events
      - Polling-only approach
      - Push notifications only
      ai_implementation_note: AI can scaffold WebSocket handlers and polling logic.
        Human designs connection management and error handling strategy.
    - decision: BullMQ job queue integration for async status updates
      rationale: Decouples status updates from main processing pipeline, enables retry
        logic, and provides job monitoring capabilities.
      alternatives_considered:
      - Direct database updates
      - Event-based pub/sub
      - AWS SQS integration
      ai_implementation_note: AI can generate job definitions and queue setup. Human
        designs job failure recovery and monitoring strategy.
    researched_at: '2026-02-08T18:30:53.576410'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:13:33.889068'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 8a4ac960
    planning_hash: c6ad9518
  technical_notes:
    approach: 'Implement a three-tier status tracking system: (1) Database layer with
      PostgreSQL enums and audit tables for persistence, (2) Service layer with state
      machine logic and progress calculations, (3) Real-time layer with WebSocket
      notifications and job queue integration. Use event-driven architecture where
      each processing stage emits status events that update the database and notify
      connected clients. Implement graceful degradation with polling fallback for
      unreliable WebSocket connections.

      '
    external_dependencies:
    - name: '@fastify/websocket'
      version: ^10.0.1
      reason: Real-time status updates to frontend clients
    - name: bullmq
      version: ^5.1.0
      reason: Job queue for async status update processing
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for BullMQ and WebSocket session management
    - name: zod
      version: ^3.22.4
      reason: Status and progress data validation schemas
    files_to_modify:
    - path: packages/database/src/models/book.ts
      changes: Add status and progress_percentage columns to Book model
    - path: packages/backend/src/services/book-ingestion.ts
      changes: Integrate status updates at each processing stage
    new_files:
    - path: packages/backend/src/models/book-status.ts
      purpose: Status enums, interfaces, and type definitions
    - path: packages/backend/src/services/book-progress.ts
      purpose: Core progress tracking service with state machine logic
    - path: packages/backend/src/routes/books/status.ts
      purpose: REST API endpoints for status queries and history
    - path: packages/backend/src/websocket/book-events.ts
      purpose: WebSocket event handlers and client notification management
    - path: packages/backend/src/jobs/status-updater.ts
      purpose: Background job for processing status updates and retries
    - path: packages/database/migrations/20240315_add_book_status_tracking.sql
      purpose: Database schema for status, progress, and audit tables
    - path: packages/backend/src/middleware/websocket-auth.ts
      purpose: WebSocket connection authentication and authorization
    - path: packages/frontend/src/hooks/useBookProgress.ts
      purpose: React hook for real-time book status subscriptions
  acceptance_criteria:
  - criterion: Book status progresses through complete transformation pipeline states
      (UPLOADED  PARSING  CHAPTER_GENERATION  IMAGE_GENERATION  COMPLETED/FAILED)
    verification: Run integration test suite at apps/backend/src/__tests__/integration/book-status.test.ts,
      verify status transitions in database
  - criterion: Real-time progress updates deliver to WebSocket clients within 500ms
      of status changes
    verification: Manual test with WebSocket client, measure latency between status
      update and notification delivery
  - criterion: Failed processing jobs automatically retry with exponential backoff
      (3 attempts max) before marking FAILED
    verification: Simulate job failures in test environment, verify retry attempts
      in BullMQ dashboard and database audit logs
  - criterion: Progress percentage accurately reflects completion (0-100%) based on
      pipeline stage and sub-tasks
    verification: Unit tests in packages/backend/src/__tests__/services/book-progress.test.ts
      verify calculation logic
  - criterion: Status API endpoints return consistent data with <200ms response time
      under normal load
    verification: Load test GET /api/books/{id}/status with 100 concurrent requests,
      verify response times
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/models/book-status.test.ts
      coverage_target: 90%
      scenarios:
      - Status enum validation
      - Status transition validation
      - Invalid state transitions rejected
    - file: packages/backend/src/__tests__/services/book-progress.test.ts
      coverage_target: 85%
      scenarios:
      - Progress percentage calculation for each stage
      - Status update with metadata
      - Audit trail creation
      - Race condition handling
    - file: packages/backend/src/__tests__/routes/books/status.test.ts
      coverage_target: 85%
      scenarios:
      - GET status for valid book ID
      - GET status for non-existent book
      - Unauthorized access handling
      - Status history retrieval
    integration_tests:
    - file: packages/backend/src/__tests__/integration/book-status-flow.test.ts
      scenarios:
      - Complete book transformation status flow
      - WebSocket notification delivery
      - Job queue status updates
      - Database consistency across services
    - file: packages/backend/src/__tests__/integration/websocket-events.test.ts
      scenarios:
      - Client connection and subscription
      - Status change event broadcasting
      - Connection cleanup on disconnect
    manual_testing:
    - step: Upload book and monitor status progression via WebSocket client
      expected: Real-time status updates from UPLOADED through COMPLETED
    - step: Simulate processing failure during CHAPTER_GENERATION
      expected: Status changes to FAILED after retry attempts, error details in audit
        log
    - step: Connect multiple WebSocket clients, update book status
      expected: All clients receive notifications simultaneously
  estimates:
    development: 4.5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7.5
    ai_acceleration_factor: 0.65
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create database migration for book status and audit tables'
      done: false
      ai_friendly: true
    - task: '[AI] Implement BookStatus enum and TypeScript interfaces'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design state machine transition rules and business logic'
      done: false
      ai_friendly: false
    - task: '[AI] Generate BookProgressService CRUD operations and basic methods'
      done: false
      ai_friendly: true
    - task: '[AI] Implement REST API endpoints for status queries'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Architect WebSocket event system and connection management'
      done: false
      ai_friendly: false
    - task: '[AI] Generate WebSocket event handlers and client notification code'
      done: false
      ai_friendly: true
    - task: '[AI] Implement BullMQ job integration for async status updates'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design retry logic and error recovery strategies'
      done: false
      ai_friendly: false
    - task: '[AI] Generate comprehensive unit test suites'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for status flow'
      done: false
      ai_friendly: true
    - task: '[AI] Implement frontend React hook for real-time updates'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Code review focusing on state consistency and race conditions'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Manual testing of WebSocket connections and error scenarios'
      done: false
      ai_friendly: false
- key: T46
  title: Error Recovery & Retries
  type: Task
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: backend
  dependsOn:
  - T29
  agent_notes:
    research_findings: '**Context:**

      Error recovery and retries are critical for the Morpheus platform''s reliability,
      especially given our heavy reliance on external AI services (OpenAI/Anthropic
      LLMs, RunPod Stable Diffusion) for core functionality. These services can experience
      transient failures, rate limiting, and timeouts that would otherwise break the
      novel-to-comic transformation pipeline. This task addresses implementing robust
      retry mechanisms with exponential backoff, circuit breakers, and graceful degradation
      to ensure consistent user experience and prevent cascade failures.


      **Technical Approach:**

      - Implement a centralized retry service using the proven `p-retry` library with
      exponential backoff

      - Add circuit breaker pattern using `opossum` to prevent overwhelming failing
      services

      - Create retry decorators/middleware for Fastify routes handling AI operations

      - Implement dead letter queues for failed operations that exceed retry limits

      - Add comprehensive logging and metrics for retry attempts and failure patterns

      - Design fallback strategies for each AI service (cached results, simplified
      outputs, user notifications)

      - Integrate with existing error handling in Fastify error hooks


      **AI Suitability Analysis:**

      - High AI effectiveness: Boilerplate retry logic, test cases for different failure
      scenarios, configuration objects, error response handling, logging statements

      - Medium AI effectiveness: Fastify middleware integration, circuit breaker configuration,
      metrics collection setup, database error handling patterns

      - Low AI effectiveness: Retry strategy design decisions, circuit breaker thresholds,
      fallback behavior definition, business logic for handling permanent vs transient
      failures


      **Dependencies:**

      - External: p-retry (^6.0.0), opossum (^8.0.0), pino-std-serializers for error
      logging

      - Internal: Existing Fastify error handlers, AI service clients, database connection
      pools, logging infrastructure


      **Risks:**

      - Retry storms: Multiple services retrying simultaneously could overwhelm downstream
      services. Mitigation: Implement jitter and per-service rate limiting

      - Resource exhaustion: Long retry chains could consume memory/connections. Mitigation:
      Set reasonable retry limits and timeouts

      - Inconsistent state: Partial failures in multi-step operations. Mitigation:
      Implement idempotent operations and transaction rollback

      - Observability gaps: Silent failures or retry loops. Mitigation: Comprehensive
      metrics and alerting


      **Complexity Notes:**

      Initially seems straightforward but complexity increases significantly when
      considering the interaction between different AI services, database transactions,
      and user-facing operations. AI agents will accelerate the implementation of
      standard retry patterns, but human judgment is crucial for defining business-appropriate
      retry strategies and fallback behaviors. Expect 60% AI assistance on implementation
      after architectural decisions are made.


      **Key Files:**

      - packages/backend/src/services/retry-service.ts: Core retry logic and circuit
      breaker implementation

      - packages/backend/src/middleware/retry-middleware.ts: Fastify middleware for
      route-level retries

      - packages/backend/src/clients/ai-client-base.ts: Base class with retry capabilities
      for AI services

      - packages/backend/src/config/retry-config.ts: Configuration for different retry
      strategies

      - packages/backend/src/utils/error-classifier.ts: Logic to determine if errors
      are retryable

      '
    design_decisions:
    - decision: Use p-retry with custom retry strategies per service type
      rationale: Different AI services have different failure characteristics - LLM
        APIs typically need shorter retries for rate limits, while image generation
        needs longer timeouts. p-retry provides flexible strategy customization.
      alternatives_considered:
      - Custom retry implementation
      - Bull queue with retry jobs
      - Axios retry interceptors only
      ai_implementation_note: AI can generate the retry configuration objects and
        basic retry wrapper functions once the strategy interface is defined
    - decision: Implement circuit breaker pattern with opossum at service client level
      rationale: Prevents cascade failures when AI services are down and provides
        faster failure response to users rather than waiting for timeouts
      alternatives_considered:
      - No circuit breaker
      - API gateway circuit breaker
      - Custom circuit breaker implementation
      ai_implementation_note: AI can implement the circuit breaker integration and
        configuration after human defines the failure thresholds and fallback behaviors
    - decision: Create retry middleware for Fastify routes rather than decorating
        every controller method
      rationale: Provides consistent retry behavior across all endpoints with declarative
        configuration via route options
      alternatives_considered:
      - Method decorators
      - Service-level only retries
      - Manual retry in each route handler
      ai_implementation_note: AI excels at implementing Fastify middleware patterns
        and can generate the route integration code
    researched_at: '2026-02-08T18:31:23.973087'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:14:01.056934'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: d9e90220
    planning_hash: f633967c
  technical_notes:
    approach: 'Implement a layered retry system starting with a core RetryService
      that encapsulates p-retry and opossum. Create Fastify middleware that can be
      selectively applied to routes handling AI operations, with configuration specifying
      retry strategies per operation type. Extend existing AI service clients to use
      the retry service, and implement comprehensive error classification to determine
      retry eligibility. Add observability through structured logging and metrics
      to monitor retry patterns and adjust thresholds.

      '
    external_dependencies:
    - name: p-retry
      version: ^6.2.0
      reason: Battle-tested retry library with exponential backoff and custom strategy
        support
    - name: opossum
      version: ^8.1.3
      reason: Circuit breaker implementation to prevent cascade failures
    - name: pino-std-serializers
      version: ^7.0.0
      reason: Enhanced error serialization for retry operation logging
    files_to_modify:
    - path: packages/backend/src/clients/openai-client.ts
      changes: Extend base AI client class, add retry wrapper to API calls
    - path: packages/backend/src/clients/anthropic-client.ts
      changes: Extend base AI client class, add retry wrapper to API calls
    - path: packages/backend/src/clients/runpod-client.ts
      changes: Extend base AI client class, add retry wrapper to API calls
    - path: packages/backend/src/routes/comic.ts
      changes: Add retry middleware to generation endpoints
    - path: packages/backend/src/routes/novel.ts
      changes: Add retry middleware to analysis endpoints
    - path: packages/backend/package.json
      changes: Add p-retry@^6.0.0 and opossum@^8.0.0 dependencies
    new_files:
    - path: packages/backend/src/services/retry-service.ts
      purpose: Core retry logic with circuit breaker, exponential backoff, and metrics
    - path: packages/backend/src/middleware/retry-middleware.ts
      purpose: Fastify middleware for applying retry logic to specific routes
    - path: packages/backend/src/clients/ai-client-base.ts
      purpose: Base class for AI clients with built-in retry capabilities
    - path: packages/backend/src/config/retry-config.ts
      purpose: Configuration objects for different retry strategies per service
    - path: packages/backend/src/utils/error-classifier.ts
      purpose: Utility to determine if errors are transient/retryable
    - path: packages/backend/src/services/dead-letter-queue.ts
      purpose: Service for storing and managing failed operations
    - path: packages/backend/src/types/retry-types.ts
      purpose: TypeScript interfaces for retry configurations and error types
  acceptance_criteria:
  - criterion: Retry service successfully retries transient failures with exponential
      backoff
    verification: Unit tests pass for RetryService with simulated network failures,
      verify exponential delay progression
  - criterion: Circuit breaker opens after threshold failures and prevents overwhelming
      downstream services
    verification: Integration test shows circuit breaker state changes, metrics show
      request blocking during open state
  - criterion: AI service calls automatically retry on 429/503/timeout errors but
      not on 400/401 errors
    verification: Test AI client wrapper with mock responses, verify retry attempts
      logged correctly
  - criterion: Fastify routes with retry middleware gracefully handle failures and
      return appropriate error responses
    verification: E2E tests on /api/comic/generate endpoint with simulated AI service
      failures
  - criterion: Dead letter queue captures operations that exceed retry limits with
      full context
    verification: Database query shows failed operations stored with error details
      and retry attempt count
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/retry-service.test.ts
      coverage_target: 90%
      scenarios:
      - Successful retry after transient failure
      - Exponential backoff timing verification
      - Max retry limit exceeded
      - Non-retryable error bypasses retry
      - Circuit breaker state transitions
    - file: packages/backend/src/__tests__/utils/error-classifier.test.ts
      coverage_target: 95%
      scenarios:
      - HTTP status code classification
      - Network timeout errors
      - AI service specific error formats
      - Database connection errors
    - file: packages/backend/src/__tests__/middleware/retry-middleware.test.ts
      coverage_target: 85%
      scenarios:
      - Middleware applies retry logic to routes
      - Request context preserved through retries
      - Error responses formatted correctly
    integration_tests:
    - file: packages/backend/src/__tests__/integration/ai-retry-flow.test.ts
      scenarios:
      - End-to-end comic generation with simulated AI failures
      - Circuit breaker integration with multiple concurrent requests
      - Dead letter queue population and retrieval
    manual_testing:
    - step: Trigger AI service timeout during comic generation
      expected: Request retries automatically, succeeds on subsequent attempt
    - step: Simulate AI service returning 503 errors consistently
      expected: Circuit breaker opens, requests fail fast with appropriate error message
  estimates:
    development: 3.5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 6.5
    ai_acceleration_factor: 0.6
  progress:
    status: not-started
    checklist:
    - task: '[AI] Install dependencies (p-retry, opossum) and update package.json'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Define retry strategies and circuit breaker thresholds per AI
        service'
      done: false
      ai_friendly: false
    - task: '[AI] Create retry configuration objects with different strategies'
      done: false
      ai_friendly: true
    - task: '[AI] Implement error classifier utility with HTTP status code logic'
      done: false
      ai_friendly: true
    - task: '[AI] Create core RetryService class with p-retry and opossum integration'
      done: false
      ai_friendly: true
    - task: '[AI] Implement base AI client class with retry wrapper methods'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design dead letter queue schema and failure context storage'
      done: false
      ai_friendly: false
    - task: '[AI] Create Fastify middleware for route-level retry application'
      done: false
      ai_friendly: true
    - task: '[AI] Update existing AI client classes to extend base retry client'
      done: false
      ai_friendly: true
    - task: '[AI] Add retry middleware to comic and novel generation routes'
      done: false
      ai_friendly: true
    - task: '[AI] Implement comprehensive unit tests for all retry components'
      done: false
      ai_friendly: true
    - task: '[AI] Create integration tests for end-to-end retry scenarios'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Review retry thresholds and adjust based on AI service SLAs'
      done: false
      ai_friendly: false
    - task: '[HUMAN] Validate error handling edge cases and business logic'
      done: false
      ai_friendly: false
- key: T47
  title: M2 Integration Testing
  type: Task
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T44
  - T45
  - T46
  agent_notes:
    research_findings: '**Context:**

      M2 Integration Testing refers to the second milestone of comprehensive end-to-end
      integration testing for the novel-to-comic transformation pipeline. This task
      is critical for ensuring the entire ingestion flow works seamlessly from novel
      text input through to comic panel generation, including LLM processing, image
      generation via RunPod, and data persistence in Supabase. Given the complex async
      nature of the transformation pipeline involving multiple external AI services,
      robust integration testing is essential to catch issues that unit tests miss.


      **Technical Approach:**

      - Use Playwright for full E2E testing of the complete transformation pipeline

      - Implement test fixtures with real novel excerpts and expected comic outputs

      - Create mock/stub services for external AI APIs to enable deterministic testing

      - Set up dedicated test database schemas in Supabase for isolation

      - Implement async test patterns for long-running transformation jobs

      - Use Vitest for service-level integration tests between backend components

      - Create test data factories for consistent novel/comic test scenarios

      - Implement visual regression testing for generated comic panels


      **AI Suitability Analysis:**

      - High AI effectiveness: Test data generation, boilerplate test setup, CRUD
      test patterns, mock service implementations, assertion helpers

      - Medium AI effectiveness: Playwright test scripts, async test orchestration,
      database setup/teardown utilities

      - Low AI effectiveness: Test strategy decisions, complex async flow testing
      patterns, visual regression test configuration, performance benchmarking logic


      **Dependencies:**

      - External: @playwright/test, vitest, faker.js, sharp (image comparison), dockertest
      (if using containers)

      - Internal: Backend transformation services, Supabase client, RunPod integration
      service, authentication middleware


      **Risks:**

      - Flaky tests due to external AI service timeouts: Use circuit breakers and
      retries with exponential backoff

      - Test data drift with real novel content: Version control test fixtures and
      use semantic versioning

      - Expensive API calls in CI: Implement smart mocking with occasional real API
      validation

      - Database state pollution: Use transaction rollbacks and isolated test schemas

      - Long test execution times: Parallelize tests and use selective test running
      based on changes


      **Complexity Notes:**

      Higher complexity than initially estimated due to async nature of AI pipeline.
      However, AI coding assistance can significantly accelerate test scaffolding
      and data generation. The main complexity lies in orchestrating the full pipeline
      testing with proper mocking strategies.


      **Key Files:**

      - apps/backend/tests/integration/: Test suite structure

      - apps/backend/src/services/transformation.service.ts: Service to test

      - packages/test-utils/: Shared testing utilities

      - tests/fixtures/: Novel and comic test data

      - playwright.config.ts: E2E test configuration

      '
    design_decisions:
    - decision: Use Playwright for full E2E pipeline testing with selective real API
        calls
      rationale: Playwright provides excellent async handling and can test the complete
        user journey while allowing strategic real API validation to catch integration
        issues
      alternatives_considered:
      - Pure Vitest integration tests
      - Cypress E2E testing
      - Manual testing only
      ai_implementation_note: AI can generate comprehensive Playwright test scenarios
        and page object models based on existing component patterns
    - decision: Implement tiered mocking strategy with 90% mocked, 10% real API calls
      rationale: Balances test reliability and speed with catching real integration
        issues, while managing API costs
      alternatives_considered:
      - Full mocking
      - All real API calls
      - VCR-style recording
      ai_implementation_note: AI excels at generating mock implementations that match
        real API response schemas
    - decision: Use dedicated test database schemas with automatic cleanup
      rationale: Provides true isolation without the overhead of spinning up separate
        database instances
      alternatives_considered:
      - In-memory database
      - Separate test database
      - Transaction rollbacks only
      ai_implementation_note: AI can generate database setup/teardown utilities and
        migration scripts for test schemas
    researched_at: '2026-02-08T18:31:51.593794'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T19:14:28.106536'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: ad2fcf87
    planning_hash: 54b0b197
  technical_notes:
    approach: 'Implement a comprehensive integration testing suite using Playwright
      for E2E scenarios and Vitest for service-level integration. Create a tiered
      testing strategy with fast, mocked integration tests for CI and slower, real-API
      integration tests for nightly builds. Use test fixtures with versioned novel
      excerpts and expected comic outputs to ensure consistent testing. Implement
      proper async test patterns to handle the long-running nature of AI transformations
      with appropriate timeouts and retry logic.

      '
    external_dependencies:
    - name: '@playwright/test'
      version: ^1.40.0
      reason: E2E testing framework with excellent async support for testing full
        transformation pipeline
    - name: '@faker-js/faker'
      version: ^8.3.1
      reason: Generate realistic test data for novel content and user scenarios
    - name: sharp
      version: ^0.33.0
      reason: Image processing for visual regression testing of generated comic panels
    - name: testcontainers
      version: ^10.4.0
      reason: Optional container-based testing for true environment isolation
    files_to_modify:
    - path: apps/backend/src/services/transformation.service.ts
      changes: Add test hooks for dependency injection, improve error handling
    - path: apps/backend/src/config/database.ts
      changes: Add test database configuration and schema isolation
    - path: package.json
      changes: Add test scripts and Playwright configuration
    new_files:
    - path: packages/test-utils/src/factories/novel-factory.ts
      purpose: Generate consistent test novel data with faker.js
    - path: packages/test-utils/src/factories/comic-factory.ts
      purpose: Mock comic panel generation responses
    - path: packages/test-utils/src/mocks/ai-services.ts
      purpose: Mock LLM and image generation API responses
    - path: packages/test-utils/src/database/test-helpers.ts
      purpose: Database setup, teardown, and isolation utilities
    - path: apps/backend/tests/integration/transformation-pipeline.test.ts
      purpose: Service-level integration testing for transformation flow
    - path: apps/backend/tests/e2e/novel-to-comic.spec.ts
      purpose: End-to-end Playwright tests for complete user journey
    - path: apps/backend/tests/fixtures/novels/sample-chapter.txt
      purpose: Versioned test novel content for consistent testing
    - path: apps/backend/tests/fixtures/comics/expected-panels/
      purpose: Expected comic panel outputs for visual regression
    - path: playwright.config.ts
      purpose: Playwright configuration for E2E testing
    - path: vitest.integration.config.ts
      purpose: Vitest configuration for integration tests with longer timeouts
  acceptance_criteria:
  - criterion: Complete novel-to-comic transformation pipeline executes successfully
      end-to-end
    verification: Run `npm test:e2e -- --grep 'full transformation pipeline'` with
      real novel input producing comic panels
  - criterion: Integration tests handle async operations with proper timeouts and
      retries
    verification: Run `npm test:integration` with network delays simulated, tests
      pass within 30s timeout
  - criterion: Test suite supports both mocked and real API modes for CI/CD flexibility
    verification: Environment variable TEST_MODE=mock runs in <10s, TEST_MODE=real
      completes with actual API calls
  - criterion: Database isolation prevents test contamination across parallel runs
    verification: Run `npm test:integration -- --parallel` multiple times, no foreign
      key or constraint errors
  - criterion: Visual regression testing validates generated comic panel quality
    verification: Playwright visual comparisons pass with <5% pixel difference threshold
      for known inputs
  testing:
    unit_tests:
    - file: apps/backend/src/services/__tests__/transformation.service.test.ts
      coverage_target: 90%
      scenarios:
      - Novel text parsing and chunking
      - LLM prompt generation
      - Error handling for API failures
      - Async job status tracking
    - file: packages/test-utils/__tests__/test-factories.test.ts
      coverage_target: 85%
      scenarios:
      - Novel fixture generation
      - Comic panel mock data
      - Database test state setup
    integration_tests:
    - file: apps/backend/tests/integration/transformation-pipeline.test.ts
      scenarios:
      - Full pipeline with mocked AI services
      - Partial pipeline failure recovery
      - Database transaction handling
      - File upload and processing flow
    - file: apps/backend/tests/integration/supabase-integration.test.ts
      scenarios:
      - Novel storage and retrieval
      - Comic panel metadata persistence
      - User authentication flow
    e2e_tests:
    - file: apps/backend/tests/e2e/novel-to-comic.spec.ts
      scenarios:
      - Upload novel file through API
      - Monitor transformation job progress
      - Download generated comic panels
      - Handle transformation failures gracefully
    manual_testing:
    - step: Upload 10-page novel excerpt via API
      expected: Transformation job created, returns job ID
    - step: Monitor job status endpoint for completion
      expected: 'Status progresses: queued -> processing -> completed'
    - step: Verify generated comic panels in storage
      expected: Comic panels accessible with proper metadata
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
    ai_acceleration_factor: 0.7
  progress:
    status: not-started
    checklist:
    - task: '[AI] Create test utility factories for novel and comic data generation'
      done: false
      ai_friendly: true
    - task: '[AI] Implement mock services for LLM and RunPod API responses'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Design test database schema isolation strategy'
      done: false
      ai_friendly: false
    - task: '[AI] Generate database test helpers for setup/teardown'
      done: false
      ai_friendly: true
    - task: '[AI] Create Vitest integration tests for transformation service'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure async test patterns and timeout strategies'
      done: false
      ai_friendly: false
    - task: '[AI] Implement Playwright E2E test scenarios'
      done: false
      ai_friendly: true
    - task: '[AI] Set up visual regression testing with baseline images'
      done: false
      ai_friendly: true
    - task: '[HUMAN] Configure CI/CD test execution strategy (mock vs real APIs)'
      done: false
      ai_friendly: false
    - task: '[AI] Generate comprehensive test documentation and examples'
      done: false
      ai_friendly: true
