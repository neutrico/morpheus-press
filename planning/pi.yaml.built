project:
  owner: neutrico
  repo: morpheus
  projectV2Number: 1
safe:
  pi:
    name: PI-2026-Q1
    start: '2026-02-10'
    iterationLengthDays: 14
    iterations:
    - I1
    - I2
    - I3
    - I4
    - I5
    - I6
    - I7
    capacity:
      pointsPerIteration: 30
issueTypes:
- Task
- Feature
- Bug
milestones:
- name: M0 - Infrastructure & Setup
  description: Dev environment, CI/CD, testing framework, monitoring, security, deployment
    pipelines
  state: open
- name: M1 - Backend Services
  description: API layer, database, authentication, book management, ingestion pipeline,
    semantic search
  state: open
- name: M2 - ML Training & Development
  description: Dataset preparation, model training (dialogue, NER, scenes), evaluation,
    W&B integration, active learning
  state: open
- name: M3 - Content Generation Pipeline
  description: Prompt engineering, SDXL image generation, quality assessment, ComfyUI
    integration, batch processing
  state: open
- name: M4 - Dashboard & UI
  description: Web interface for book/comic management, preview components, quality
    control dashboard
  state: open
- name: M5 - Product Assembly
  description: Comic layout engine, panel generation, PDF export, archive creation,
    print-ready output
  state: open
- name: M6 - Commerce & Distribution
  description: E-commerce (Stripe, storefront, checkout), distribution (Discord, Reddit,
    browser extension, WordPress)
  state: open
- name: M7 - Launch & Release
  description: Final integration testing, production deployment, launch procedures
  state: open
labels:
  area:
  - 'area: setup'
  - 'area: backend'
  - 'area: ml'
  - 'area: ingestion'
  - 'area: image-gen'
  - 'area: comic'
  - 'area: distribution'
  - 'area: ecommerce'
  - 'area: release'
  priority:
  - priority:p0
  - priority:p1
  - priority:p2
  - priority:p3
  status:
  - status:triage
  - status:ready
  - status:in-progress
  - status:blocked
  - status:done
projectFields:
  priority:
    name: Priority
    type: SINGLE_SELECT
    options:
    - P0
    - P1
    - P2
    - P3
  effort:
    name: Effort
    type: NUMBER
  iteration:
    name: Iteration
    type: ITERATION
  risk:
    name: Risk
    type: SINGLE_SELECT
    options:
    - Low
    - Medium
    - High
issues:
- key: T1
  title: Tech Stack Decision Documentation
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nThis task involves creating comprehensive documentation\
      \ of the technology stack decisions for the Morpheus platform. This is critical\
      \ for M0 as it establishes the architectural foundation, ensures team alignment,\
      \ and provides justification for technology choices. Without proper documentation,\
      \ future developers will struggle to understand why specific technologies were\
      \ chosen, leading to inconsistent implementations and potential rewrites.\n\n\
      **Technical Approach:**\n- Use Architecture Decision Records (ADRs) format for\
      \ structured decision documentation\n- Create a centralized tech stack registry\
      \ with version constraints and upgrade policies  \n- Implement automated dependency\
      \ tracking and vulnerability scanning\n- Document integration patterns between\
      \ services (API contracts, event schemas)\n- Establish coding standards and\
      \ linting rules per technology\n- Create decision templates for future technology\
      \ evaluations\n\n**Dependencies:**\n- External: [@typescript-eslint/eslint-plugin,\
      \ @playwright/test, supabase-js, openai, anthropic-ai]\n- Internal: [turborepo\
      \ configuration, shared ESLint configs, TypeScript project references]\n\n**Risks:**\n\
      - Technology lock-in: Document migration strategies and abstraction layers for\
      \ critical dependencies\n- Version drift: Implement workspace dependency constraints\
      \ and automated updates\n- Knowledge silos: Ensure documentation is accessible\
      \ and searchable, not buried in wikis\n- Outdated decisions: Establish review\
      \ cycles and decision deprecation processes\n\n**Complexity Notes:**\nMore complex\
      \ than initially thought due to the multi-service architecture requiring documentation\
      \ of:\n- Inter-service communication patterns (REST, WebSockets, pub/sub)\n\
      - Shared library versioning strategies across workspaces\n- Database schema\
      \ evolution and migration strategies\n- ML model deployment and versioning workflows\n\
      \n**Key Files:**\n- docs/architecture/: ADR files for each major technology\
      \ decision\n- package.json: Workspace dependency constraints and tooling versions\n\
      - .eslintrc.js: Shared linting rules across all workspaces\n- turbo.json: Build\
      \ pipeline and caching strategies\n- README.md: High-level architecture overview\
      \ and quick start\n"
    design_decisions:
    - decision: Use Architecture Decision Records (ADRs) for documentation
      rationale: Provides structured, version-controlled, and contextual documentation
        that lives with the codebase
      alternatives_considered:
      - Confluence/Wiki pages
      - Code comments only
      - Inline documentation
    - decision: Implement strict workspace dependency management
      rationale: Prevents version drift in monorepo and ensures consistent tooling
        across services
      alternatives_considered:
      - Allow flexible versions
      - Manual coordination
      - Separate repositories
    - decision: Document API contracts using OpenAPI/JSON Schema
      rationale: Enables contract-first development and automated validation between
        services
      alternatives_considered:
      - Code-first documentation
      - Manual API docs
      - GraphQL schema
    researched_at: '2026-02-08T16:51:44.149828'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:23:01.054526'
    planned_by: planning-agent-claude-sonnet-4
    content_hash: 5545ab03
  technical_notes:
    approach: 'Create a structured documentation system using ADRs stored in the repository
      alongside code. Implement automated tooling to validate and update technology
      inventories. Establish templates and processes for evaluating new technologies.
      Use OpenAPI specifications for service contracts and maintain a central registry
      of all external dependencies with their purposes and alternatives.

      '
    external_dependencies:
    - name: '@typescript-eslint/eslint-plugin'
      version: ^7.0.0
      reason: TypeScript-specific linting rules across all services
    - name: madge
      version: ^6.1.0
      reason: Dependency graph visualization and circular dependency detection
    - name: npm-check-updates
      version: ^16.14.0
      reason: Automated dependency update tracking and management
    - name: swagger-jsdoc
      version: ^6.2.8
      reason: Generate OpenAPI specs from code comments in Fastify services
    files_to_modify:
    - path: package.json
      changes: Add log4brains, mermaid-cli dependencies and ADR management scripts
    - path: .github/pull_request_template.md
      changes: Add checklist item for ADR creation/updates when making architectural
        decisions
    - path: .github/workflows/docs.yml
      changes: Add ADR validation and static site generation steps
    - path: docs/README.md
      changes: Add link to architecture decisions and usage instructions
    new_files:
    - path: docs/architecture/README.md
      purpose: ADR index, decision log, and contributor guidelines
    - path: docs/architecture/.log4brains.yml
      purpose: Log4brains configuration for ADR management and web interface
    - path: docs/architecture/templates/decision-template.md
      purpose: Standard ADR template with required sections and examples
    - path: docs/architecture/decisions/001-backend-framework-fastify.md
      purpose: Document rationale for choosing Fastify over Express/Koa
    - path: docs/architecture/decisions/002-frontend-framework-nextjs.md
      purpose: Document Next.js selection for React framework with SSR capabilities
    - path: docs/architecture/decisions/003-database-supabase.md
      purpose: Document Supabase choice for PostgreSQL hosting and auth services
    - path: docs/architecture/decisions/004-ml-integration-strategy.md
      purpose: Document approach for integrating multiple ML services and APIs
    - path: docs/architecture/decisions/005-testing-strategy.md
      purpose: Document testing framework choices and coverage requirements
    - path: scripts/validate-adrs.js
      purpose: Automated validation script for ADR format and required sections
    - path: scripts/generate-adr-index.js
      purpose: Auto-generate decision index and update README with current ADRs
  acceptance_criteria:
  - criterion: ADR management system is fully functional with log4brains
    verification: Run `npm run docs:adr:serve` and verify web interface loads at localhost:4004
      with navigation and search
  - criterion: All major architectural decisions are documented with consistent format
    verification: Check `/docs/architecture/decisions/` contains ADRs for backend
      framework, frontend framework, database, ML services, and testing strategy (minimum
      5 ADRs)
  - criterion: ADR creation is integrated into development workflow
    verification: PR template includes ADR checklist item and `npm run docs:adr:new`
      command creates properly formatted ADR
  - criterion: Documentation is automatically validated and accessible
    verification: CI pipeline validates ADR format, generates static site, and deploys
      to docs hosting with working links
  - criterion: Architecture diagrams are embedded and render correctly
    verification: ADRs contain Mermaid.js diagrams that render in both log4brains
      interface and GitHub markdown
  testing:
    unit_tests:
    - file: scripts/__tests__/adr-validation.test.js
      coverage_target: 90%
      scenarios:
      - Valid ADR format validation
      - Missing required sections detection
      - Invalid status values rejection
      - Link integrity checking
    integration_tests:
    - file: scripts/__tests__/integration/docs-pipeline.test.js
      scenarios:
      - ADR creation to web interface generation flow
      - Mermaid diagram rendering in multiple outputs
    manual_testing:
    - step: Create new ADR using `npm run docs:adr:new 'Test Decision'`
      expected: New ADR file created with proper numbering and template structure
    - step: Modify existing ADR and run `npm run docs:adr:build`
      expected: Web interface updates with changes reflected
    - step: Submit PR with architectural change but no ADR update
      expected: PR template checklist reminds to update ADRs
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Install and configure log4brains with project-specific settings
      done: false
    - task: Create ADR templates and directory structure
      done: false
    - task: Document existing major architectural decisions retroactively
      done: false
    - task: Set up npm scripts for ADR creation, building, and validation
      done: false
    - task: Integrate ADR checklist into PR template workflow
      done: false
    - task: Configure CI/CD pipeline for ADR validation and site generation
      done: false
    - task: Create validation scripts for format consistency and link checking
      done: false
    - task: Add Mermaid diagrams to key architectural ADRs
      done: false
    - task: Test complete workflow from ADR creation to web interface deployment
      done: false
    - task: Update project documentation with ADR usage guidelines
      done: false
- key: T10
  title: shadcn/ui Component Audit & Verification
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nThis task involves auditing and verifying the\
      \ shadcn/ui component library integration across the Morpheus platform's frontend\
      \ applications (Dashboard and Storefront). shadcn/ui provides copy-paste React\
      \ components built on Radix primitives and Tailwind CSS, offering consistent\
      \ design system implementation. This audit ensures proper setup, identifies\
      \ missing components needed for the novel-to-comic transformation workflows,\
      \ verifies theming consistency, and establishes component usage patterns across\
      \ the monorepo.\n\n**Technical Approach:**\n1. Audit existing shadcn/ui installation\
      \ in both Next.js apps using the CLI tool\n2. Create a centralized component\
      \ inventory and usage matrix\n3. Establish shared theming configuration across\
      \ Dashboard/Storefront\n4. Set up component testing patterns with Vitest for\
      \ isolated component testing\n5. Create component documentation and usage guidelines\n\
      6. Implement design tokens for comic/novel-specific theming (dark themes for\
      \ reading, high contrast for editing)\n7. Verify accessibility compliance across\
      \ all components\n8. Set up Storybook integration for component development\
      \ and documentation\n\n**Dependencies:**\n- External: shadcn/ui CLI, @radix-ui/*\
      \ primitives, tailwindcss, class-variance-authority\n- Internal: Turborepo shared\
      \ configs, design system tokens, TypeScript shared types\n\n**Risks:**\n- Version\
      \ mismatches: Ensure consistent shadcn/ui versions across workspaces through\
      \ pnpm workspace constraints\n- Bundle size bloat: Audit for unused Radix primitives\
      \ and implement tree-shaking\n- Theme inconsistency: Risk of different color\
      \ schemes between Dashboard and Storefront apps\n- TypeScript conflicts: Potential\
      \ type conflicts between Radix versions and custom extensions\n\n**Complexity\
      \ Notes:**\nMedium complexity - while shadcn/ui components are straightforward\
      \ to use, coordinating them across a monorepo with shared theming and ensuring\
      \ proper TypeScript integration adds complexity. The novel-to-comic domain may\
      \ require custom component extensions.\n\n**Key Files:**\n- packages/ui/: Shared\
      \ component library workspace\n- apps/dashboard/components.json: shadcn/ui config\
      \ for dashboard\n- apps/storefront/components.json: shadcn/ui config for storefront\
      \  \n- packages/tailwind-config/: Shared Tailwind configuration\n- apps/*/tailwind.config.ts:\
      \ App-specific Tailwind extensions\n"
    design_decisions:
    - decision: Create shared @morpheus/ui package for shadcn/ui components
      rationale: Ensures consistency across Dashboard and Storefront, reduces duplication,
        and provides single source of truth for component library
      alternatives_considered:
      - Duplicate components in each app
      - Use shadcn/ui CLI in each app separately
    - decision: Extend default shadcn/ui theme with comic/novel-specific design tokens
      rationale: Novel reading and comic creation have specific UX needs (reading
        modes, panel editing themes) that require custom color schemes and spacing
      alternatives_considered:
      - Use default shadcn/ui themes only
      - Create completely custom component library
    - decision: Use Storybook for component documentation and testing
      rationale: Provides visual component testing, documentation, and development
        environment for design system components
      alternatives_considered:
      - Custom documentation site
      - Component testing in Jest only
    researched_at: '2026-02-07T18:46:27.994286'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:23:24.987183'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Set up a shared @morpheus/ui workspace containing audited shadcn/ui
      components with custom theming for novel/comic workflows. Create component inventory
      spreadsheet tracking usage across Dashboard/Storefront apps. Implement centralized
      Tailwind configuration with design tokens for reading modes, editing interfaces,
      and accessibility. Set up Vitest component testing and Storybook documentation.
      Verify all components work with TypeScript strict mode and establish linting
      rules for consistent component usage patterns.

      '
    external_dependencies:
    - name: shadcn-ui
      version: ^0.8.0
      reason: Component library CLI for installation and updates
    - name: '@radix-ui/react-*'
      version: ^1.0.0
      reason: Primitive components that shadcn/ui is built upon
    - name: class-variance-authority
      version: ^0.7.0
      reason: Component variant management for shadcn/ui components
    - name: '@storybook/react-vite'
      version: ^7.6.0
      reason: Component documentation and visual testing environment
    - name: tailwindcss-animate
      version: ^1.0.7
      reason: Animation utilities for shadcn/ui components
    files_to_modify:
    - path: apps/dashboard/components.json
      changes: Update to use @morpheus/ui as component source, adjust aliases
    - path: apps/storefront/components.json
      changes: Update to use @morpheus/ui as component source, adjust aliases
    - path: packages/tailwind-config/index.ts
      changes: Add comic/novel design tokens, reading-mode variants
    - path: apps/dashboard/tailwind.config.ts
      changes: Extend shared config with dashboard-specific overrides
    - path: apps/storefront/tailwind.config.ts
      changes: Extend shared config with storefront-specific overrides
    - path: turbo.json
      changes: Add build pipeline for @morpheus/ui workspace
    new_files:
    - path: packages/ui/package.json
      purpose: Shared UI workspace configuration with shadcn/ui dependencies
    - path: packages/ui/components.json
      purpose: Master shadcn/ui configuration for component generation
    - path: packages/ui/src/components/index.ts
      purpose: Centralized component exports for the monorepo
    - path: packages/ui/src/lib/utils.ts
      purpose: Shared utility functions (cn, theme helpers)
    - path: packages/ui/src/styles/globals.css
      purpose: Global CSS variables and base styles
    - path: packages/ui/COMPONENT_INVENTORY.md
      purpose: Comprehensive component usage tracking and documentation
    - path: packages/ui/.storybook/main.ts
      purpose: Storybook configuration for component development
    - path: packages/ui/src/stories/
      purpose: Directory containing all component stories
    - path: packages/ui/src/themes/novel.ts
      purpose: Novel-specific theme configuration and tokens
    - path: packages/ui/src/themes/comic.ts
      purpose: Comic-specific theme configuration and tokens
    - path: packages/ui/vitest.config.ts
      purpose: Component testing configuration
  acceptance_criteria:
  - criterion: All shadcn/ui components are centralized in @morpheus/ui workspace
      with consistent theming
    verification: Run 'pnpm build' in packages/ui and verify all exports work, check
      components.json configs match
  - criterion: Component inventory spreadsheet tracks 100% of UI components used across
      Dashboard/Storefront
    verification: Open packages/ui/COMPONENT_INVENTORY.md and verify all components
      listed with usage locations
  - criterion: Custom comic/novel themes render correctly in both light and dark modes
    verification: Toggle theme in Dashboard and Storefront, verify reading-mode and
      editing-mode color schemes
  - criterion: All components pass accessibility audit with WCAG 2.1 AA compliance
    verification: Run 'pnpm test:a11y' and verify axe-core tests pass with 0 violations
  - criterion: Storybook documentation covers all shared components with interactive
      examples
    verification: Run 'pnpm storybook' and verify each component has stories with
      controls and documentation
  testing:
    unit_tests:
    - file: packages/ui/src/__tests__/components.test.tsx
      coverage_target: 90%
      scenarios:
      - Component rendering with default props
      - Theme variants (light/dark/reading-mode)
      - Accessibility attributes and keyboard navigation
      - Custom prop combinations
    - file: packages/ui/src/__tests__/theme.test.ts
      coverage_target: 85%
      scenarios:
      - Theme token resolution
      - CSS variable generation
      - Color scheme switching
    integration_tests:
    - file: apps/dashboard/src/__tests__/components/integration.test.tsx
      scenarios:
      - Components work within Dashboard layout
      - Theme switching affects all components
    - file: apps/storefront/src/__tests__/components/integration.test.tsx
      scenarios:
      - Components work within Storefront layout
      - Reading mode theme integration
    manual_testing:
    - step: Navigate to Dashboard and toggle between light/dark/editing themes
      expected: All components update colors smoothly without layout shifts
    - step: Navigate to Storefront and enable reading mode
      expected: Typography and spacing optimized for reading experience
    - step: Test keyboard navigation on all form components
      expected: Focus indicators visible, logical tab order maintained
    - step: Verify components in Storybook match actual app appearance
      expected: Visual consistency between Storybook and live apps
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Create @morpheus/ui workspace and configure package.json
      done: false
    - task: Audit existing shadcn/ui components in Dashboard and Storefront
      done: false
    - task: Migrate components to centralized packages/ui workspace
      done: false
    - task: Implement custom themes for novel/comic workflows
      done: false
    - task: Create component inventory and usage documentation
      done: false
    - task: Set up Vitest component testing with React Testing Library
      done: false
    - task: Configure Storybook with theme switching controls
      done: false
    - task: Run accessibility audit and fix violations
      done: false
    - task: Update app configurations to use shared components
      done: false
    - task: Create usage guidelines and contribution docs
      done: false
- key: T11
  title: Test Environments Setup (Staging + Production)
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Test environments are critical for the Morpheus platform to enable safe deployment
      pipelines, integration testing, and stakeholder preview without risking production
      data or user experience. Given the complexity of our stack (ML services, database
      migrations, multi-service architecture), we need isolated environments that
      mirror production for proper testing of novel-to-comic transformations, user
      workflows, and API integrations.


      **Technical Approach:**

      - Environment-specific configuration using dotenv-vault or similar for secrets
      management

      - Separate Supabase projects for staging/production with database branching

      - Docker containerization with environment-specific docker-compose files

      - CI/CD pipeline integration with GitHub Actions for automated deployments

      - Separate RunPod endpoints/API keys for staging ML workloads

      - Environment-specific feature flags using tools like LaunchDarkly or custom
      implementation

      - Monitoring with different log levels and alerting thresholds per environment


      **Dependencies:**

      - External: [@supabase/supabase-js, dotenv, docker, github-actions, playwright
      for e2e testing]

      - Internal: [database schemas, API routes, ML service integrations, authentication
      flows]


      **Risks:**

      - Cost escalation: Multiple ML inference endpoints and database instances

      - Data sync issues: Staging data becoming stale or inconsistent

      - Configuration drift: Environments diverging over time

      - Secret management: Accidentally using production secrets in staging

      - ML model versioning: Different model versions across environments causing
      inconsistent results


      **Complexity Notes:**

      More complex than typical web apps due to ML pipeline dependencies, Supabase
      project management, and the need to mock/stage expensive AI services. The multi-tenancy
      nature of the comic transformation workflow adds complexity to test data management.


      **Key Files:**

      - packages/config/environments.ts: Environment configuration management

      - docker-compose.staging.yml: Staging container orchestration

      - .github/workflows/deploy-staging.yml: Staging deployment pipeline

      - apps/api/src/config/database.ts: Environment-specific DB connections

      - packages/ml-client/src/config.ts: ML service endpoint configuration

      '
    design_decisions:
    - decision: Use Supabase branching for database environments rather than single
        shared instance
      rationale: Provides true isolation, enables safe schema migrations, and prevents
        test data pollution
      alternatives_considered:
      - Shared database with prefixed tables
      - Local PostgreSQL instances
      - Database-per-feature-branch
    - decision: Implement tiered ML service usage (mock → staging → production)
      rationale: Reduces costs while maintaining realistic testing, with fallbacks
        for different testing scenarios
      alternatives_considered:
      - Full ML services in all environments
      - Mock-only for staging
      - Shared ML endpoints
    - decision: Environment-specific feature flags system
      rationale: Enables testing new features in staging without production risk and
        gradual rollouts
      alternatives_considered:
      - No feature flags
      - Third-party service only
      - Database-driven flags only
    researched_at: '2026-02-07T18:46:50.915065'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:23:51.035273'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement infrastructure-as-code approach using Docker Compose for
      local development, with separate Supabase projects for staging/production data
      isolation. Create environment-specific configuration management with secure
      secret handling and automated CI/CD pipelines. Establish tiered ML service integration
      (mock/staging/prod) to balance cost and testing fidelity. Set up comprehensive
      monitoring and logging with environment-appropriate alerting thresholds.

      '
    external_dependencies:
    - name: dotenv-vault
      version: ^1.25.0
      reason: Secure environment variable and secrets management across environments
    - name: zod
      version: ^3.22.0
      reason: Runtime environment variable validation and type safety
    - name: '@supabase/cli'
      version: ^1.110.0
      reason: Database migrations and Supabase project management
    - name: dockerode
      version: ^4.0.0
      reason: Docker container management for local development environments
    files_to_modify:
    - path: apps/api/src/config/database.ts
      changes: Add environment-specific Supabase connection logic, connection pooling
        per environment
    - path: apps/web/next.config.js
      changes: Add environment-specific build configurations, API endpoint URLs
    - path: packages/shared/src/constants.ts
      changes: Add environment-specific constants (URLs, limits, feature flags)
    - path: .github/workflows/ci.yml
      changes: Add staging deployment job triggered on main branch pushes
    new_files:
    - path: packages/config/src/environments.ts
      purpose: Centralized environment configuration with type safety and validation
    - path: docker-compose.staging.yml
      purpose: Staging-specific container orchestration with resource limits
    - path: docker-compose.production.yml
      purpose: Production container configuration with performance optimizations
    - path: .github/workflows/deploy-staging.yml
      purpose: Automated staging deployment pipeline with health checks
    - path: .github/workflows/deploy-production.yml
      purpose: Production deployment with manual approval and rollback capability
    - path: packages/ml-client/src/config.ts
      purpose: Environment-aware ML service endpoint and API key management
    - path: scripts/setup-staging.sh
      purpose: Staging environment initialization and database seeding script
    - path: scripts/deploy-production.sh
      purpose: Production deployment script with pre-flight checks and monitoring
    - path: apps/api/src/middleware/environment.ts
      purpose: Request middleware to inject environment context and feature flags
    - path: packages/config/src/secrets.ts
      purpose: Secure secret management with environment isolation
    - path: docs/deployment.md
      purpose: Environment setup and deployment procedures documentation
  acceptance_criteria:
  - criterion: Staging environment successfully deploys and runs all services (API,
      web app, ML services) with isolated database
    verification: Deploy to staging via CI/CD pipeline, verify health checks at staging.morpheus.app/health
      return 200, confirm separate Supabase project is used
  - criterion: Production environment deploys with zero-downtime and proper secret
      management
    verification: Execute production deployment, verify no service interruption, confirm
      production secrets are isolated from staging via environment variable audit
  - criterion: Environment-specific ML service integration works with cost controls
    verification: Run comic transformation in staging (uses mock/cheaper models),
      production (uses full RunPod endpoints), verify different API keys and rate
      limits
  - criterion: Automated CI/CD pipeline deploys to staging on main branch, production
      on release tags
    verification: Create PR to main, verify staging auto-deploy. Create release tag,
      verify production deployment with manual approval gate
  - criterion: Environment configuration prevents cross-contamination of data and
      secrets
    verification: Audit environment variables, verify staging cannot access production
      DB/APIs, test secret rotation affects only target environment
  testing:
    unit_tests:
    - file: packages/config/src/__tests__/environments.test.ts
      coverage_target: 90%
      scenarios:
      - Environment variable loading for each env
      - Secret validation and masking
      - Configuration schema validation
      - Default value fallbacks
    - file: packages/ml-client/src/__tests__/config.test.ts
      coverage_target: 85%
      scenarios:
      - Endpoint selection based on environment
      - API key configuration per environment
      - Model version mapping
    integration_tests:
    - file: apps/api/src/__tests__/integration/environment.test.ts
      scenarios:
      - Database connection with staging config
      - ML service integration with staging endpoints
      - Authentication flow in staging environment
      - File upload/storage with staging buckets
    - file: apps/web/src/__tests__/e2e/environment.spec.ts
      scenarios:
      - Full comic transformation workflow in staging
      - User registration and auth in staging
      - Cross-service communication
    manual_testing:
    - step: Deploy to staging and create test comic transformation
      expected: Comic generated using staging ML endpoints, saved to staging database
    - step: Verify production deployment with existing user data
      expected: No data loss, existing users can login, previous comics accessible
    - step: Test environment variable changes don't affect other environments
      expected: Staging config change doesn't impact production services
  estimates:
    development: 4
    code_review: 1
    testing: 2
    documentation: 1
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Setup separate Supabase projects for staging and production
      done: false
    - task: Create environment configuration package with TypeScript types
      done: false
    - task: Implement Docker Compose files for each environment
      done: false
    - task: Configure GitHub Actions CI/CD pipelines with approval gates
      done: false
    - task: Set up environment-specific ML service endpoints and API keys
      done: false
    - task: Implement database migration strategy across environments
      done: false
    - task: Create environment-specific monitoring and alerting
      done: false
    - task: Test end-to-end deployment workflows
      done: false
    - task: Document environment setup and deployment procedures
      done: false
    - task: Security audit of secret management and environment isolation
      done: false
- key: T12
  title: Monitoring & Observability Setup
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Monitoring & observability is critical for a production ML/AI platform like
      Morpheus that processes novel-to-comic transformations. This task solves visibility
      into system health, performance bottlenecks, error tracking, and user behavior
      across the complex pipeline involving LLM calls, image generation, database
      operations, and multi-service architecture. Without proper observability, debugging
      production issues, optimizing expensive ML operations, and ensuring SLA compliance
      becomes nearly impossible.


      **Technical Approach:**

      Implement a three-pillar observability stack: metrics (Prometheus/OpenTelemetry),
      logs (structured JSON with correlation IDs), and traces (distributed tracing
      across Fastify backend, Next.js frontend, and external ML services). Use OpenTelemetry
      as the unified collection layer, with Grafana for dashboards and AlertManager
      for notifications. Integrate application performance monitoring (APM) specifically
      for expensive operations like LLM calls and Stable Diffusion requests. Include
      business metrics like transformation success rates, processing times per comic
      page, and cost tracking for OpenAI/RunPod usage.


      **Dependencies:**

      - External: @opentelemetry/api, @opentelemetry/sdk-node, @sentry/node, @sentry/nextjs,
      pino (structured logging), prometheus-api-metrics, grafana/faro-web-sdk

      - Internal: Authentication middleware (for user correlation), ML service wrappers,
      Supabase client adapters, shared types package


      **Risks:**

      - Performance overhead: OpenTelemetry instrumentation can add 5-15ms latency;
      mitigate with sampling strategies and async batching

      - Data volume explosion: ML operations generate verbose logs; implement log
      level filtering and retention policies

      - Secret exposure: API keys/tokens in traces; use sanitization middleware and
      secure trace exporters

      - Vendor lock-in: Avoid proprietary agents; stick to OpenTelemetry standard
      for portability


      **Complexity Notes:**

      More complex than initially estimated due to distributed tracing across external
      ML services (OpenAI, RunPod) and correlation of async comic generation pipelines.
      The multi-tenant nature (users, comics, pages) requires sophisticated tagging
      strategies. However, the TypeScript monorepo structure simplifies shared observability
      utilities.


      **Key Files:**

      - packages/observability/: Shared instrumentation utilities and types

      - apps/api/src/plugins/telemetry.ts: Fastify OpenTelemetry plugin

      - apps/dashboard/instrumentation.ts: Next.js OpenTelemetry configuration

      - apps/storefront/instrumentation.ts: Storefront observability setup

      - packages/ml-clients/: Add instrumentation to LLM and SD wrappers

      - docker-compose.monitoring.yml: Local Prometheus/Grafana stack

      '
    design_decisions:
    - decision: OpenTelemetry as primary observability framework
      rationale: Vendor-neutral, comprehensive tracing/metrics, excellent TypeScript
        support, works across Fastify/Next.js
      alternatives_considered:
      - Datadog APM (expensive)
      - New Relic (vendor lock-in)
      - Custom Prometheus setup (limited tracing)
    - decision: Sentry for error tracking and performance monitoring
      rationale: Best-in-class error tracking, React/Next.js integration, reasonable
        pricing for startups
      alternatives_considered:
      - Bugsnag
      - Rollbar
      - Self-hosted error tracking
    - decision: Grafana + Prometheus for metrics visualization
      rationale: Industry standard, excellent dashboard ecosystem, cost-effective
        self-hosting option
      alternatives_considered:
      - Datadog dashboards
      - CloudWatch
      - Custom visualization
    researched_at: '2026-02-07T18:47:16.722657'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:24:18.722871'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Set up OpenTelemetry SDK in both Fastify backend and Next.js applications
      with auto-instrumentation for HTTP, database, and external API calls. Create
      custom spans for ML operations (LLM requests, image generation) with cost and
      performance metrics. Implement correlation IDs to trace user requests across
      comic transformation pipelines. Deploy Prometheus for metrics collection, Grafana
      for dashboards, and integrate Sentry for real-time error tracking and performance
      monitoring.

      '
    external_dependencies:
    - name: '@opentelemetry/api'
      version: ^1.7.0
      reason: Core OpenTelemetry API for instrumentation
    - name: '@opentelemetry/sdk-node'
      version: ^0.45.0
      reason: Node.js OpenTelemetry SDK with auto-instrumentation
    - name: '@sentry/node'
      version: ^7.80.0
      reason: Error tracking and APM for Fastify backend
    - name: '@sentry/nextjs'
      version: ^7.80.0
      reason: Error tracking and performance monitoring for Next.js apps
    - name: pino
      version: ^8.16.0
      reason: High-performance structured logging for Node.js
    - name: prometheus-api-metrics
      version: ^3.2.2
      reason: Prometheus metrics middleware for Fastify
    files_to_modify:
    - path: apps/api/src/app.ts
      changes: Add OpenTelemetry plugin registration and correlation ID middleware
    - path: apps/api/src/routes/comics.ts
      changes: Add custom spans for comic transformation pipeline with business metrics
    - path: apps/dashboard/next.config.js
      changes: Add instrumentation configuration and Sentry integration
    - path: apps/storefront/next.config.js
      changes: Add basic telemetry for public-facing metrics
    - path: packages/ml-clients/src/openai.ts
      changes: Instrument LLM calls with token usage and cost tracking
    - path: packages/ml-clients/src/stable-diffusion.ts
      changes: Add tracing for image generation requests with performance metrics
    - path: packages/supabase/src/client.ts
      changes: Add database operation instrumentation and query performance tracking
    new_files:
    - path: packages/observability/src/index.ts
      purpose: Main observability package exports and initialization
    - path: packages/observability/src/instrumentation.ts
      purpose: OpenTelemetry SDK configuration and auto-instrumentation setup
    - path: packages/observability/src/metrics.ts
      purpose: Custom business metrics definitions and collection utilities
    - path: packages/observability/src/tracing.ts
      purpose: Distributed tracing utilities and span helpers
    - path: packages/observability/src/logging.ts
      purpose: Structured logging configuration with correlation ID support
    - path: packages/observability/src/sanitization.ts
      purpose: Data sanitization middleware to prevent secret exposure
    - path: apps/api/src/plugins/telemetry.ts
      purpose: Fastify plugin for OpenTelemetry integration
    - path: apps/dashboard/instrumentation.ts
      purpose: Next.js instrumentation configuration for frontend telemetry
    - path: apps/storefront/instrumentation.ts
      purpose: Storefront-specific observability setup
    - path: docker-compose.monitoring.yml
      purpose: Local development stack for Prometheus, Grafana, and Jaeger
    - path: grafana/dashboards/morpheus-system.json
      purpose: System health and performance dashboard configuration
    - path: grafana/dashboards/morpheus-business.json
      purpose: Business metrics dashboard for comic generation analytics
    - path: prometheus/prometheus.yml
      purpose: Prometheus configuration for metrics collection
  acceptance_criteria:
  - criterion: All API endpoints and ML service calls are instrumented with OpenTelemetry
      traces
    verification: Run `curl -X POST http://localhost:3000/api/comics/transform` and
      verify traces appear in Grafana with spans for LLM calls, image generation,
      and database operations
  - criterion: Correlation IDs track user requests across the entire comic transformation
      pipeline
    verification: Generate a comic and verify the same correlation ID appears in logs
      from frontend, API, ML services, and database operations
  - criterion: Custom business metrics are collected for ML operations including cost
      tracking
    verification: Check Prometheus metrics endpoint `/metrics` contains `morpheus_openai_tokens_total`,
      `morpheus_comic_generation_duration_seconds`, and `morpheus_transformation_success_rate`
  - criterion: Error tracking captures and correlates failures across services
    verification: Trigger an OpenAI API failure and verify Sentry shows the error
      with full context including user ID, comic ID, and request traces
  - criterion: Grafana dashboards display system health and business metrics
    verification: Access Grafana at http://localhost:3001 and verify dashboards show
      API response times, ML service costs, comic generation success rates, and system
      resource usage
  testing:
    unit_tests:
    - file: packages/observability/src/__tests__/instrumentation.test.ts
      coverage_target: 90%
      scenarios:
      - OpenTelemetry span creation and attributes
      - Correlation ID generation and propagation
      - Metric collection and labeling
      - Log sanitization for sensitive data
    - file: packages/observability/src/__tests__/metrics.test.ts
      coverage_target: 85%
      scenarios:
      - Custom metric registration
      - Business metric calculation
      - Cost tracking accuracy
    integration_tests:
    - file: apps/api/src/__tests__/integration/observability.test.ts
      scenarios:
      - End-to-end trace propagation through comic generation
      - Database operation instrumentation
      - External ML service call tracing
    - file: apps/dashboard/src/__tests__/integration/telemetry.test.ts
      scenarios:
      - Frontend trace collection and export
      - User interaction tracking
      - Error boundary integration with Sentry
    manual_testing:
    - step: Generate a comic with invalid OpenAI API key
      expected: Error appears in Sentry with full trace context and correlation ID
    - step: Process high-resolution image through Stable Diffusion
      expected: Performance metrics show processing time and cost in Grafana dashboard
    - step: Simulate database connection failure during comic save
      expected: Distributed trace shows failure point and retry attempts
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Create observability package with OpenTelemetry SDK configuration
      done: false
    - task: Implement correlation ID middleware and propagation utilities
      done: false
    - task: Set up structured logging with Pino and log sanitization
      done: false
    - task: Instrument Fastify API with custom spans and business metrics
      done: false
    - task: Configure Next.js applications with frontend telemetry
      done: false
    - task: Add instrumentation to ML service clients (OpenAI, Stable Diffusion)
      done: false
    - task: Integrate Sentry for error tracking and performance monitoring
      done: false
    - task: Create Docker Compose stack for local monitoring services
      done: false
    - task: Build Grafana dashboards for system and business metrics
      done: false
    - task: Configure Prometheus metrics collection and alerting rules
      done: false
    - task: Test end-to-end observability pipeline with sample comic generation
      done: false
    - task: Document observability setup and troubleshooting guide
      done: false
- key: T13
  title: Security & Compliance Infrastructure
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 5
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Security & compliance infrastructure is foundational for Morpheus as it handles
      user-generated content, payment processing, and AI-generated assets. This task
      establishes authentication, authorization, data protection, and compliance frameworks
      before any user-facing features are built. Given the creative IP nature of novels
      and comics, robust security prevents unauthorized access, data breaches, and
      ensures GDPR/CCPA compliance from day one.


      **Technical Approach:**

      - Authentication: Supabase Auth with JWT tokens, social logins (Google, GitHub),
      MFA support

      - Authorization: Row Level Security (RLS) in PostgreSQL, role-based access control
      (RBAC)

      - API Security: Rate limiting with @fastify/rate-limit, request validation with
      Zod, CORS configuration

      - Data Protection: Field-level encryption for sensitive data, secure file uploads
      to Supabase Storage

      - Monitoring: Security event logging, audit trails, vulnerability scanning with
      Snyk

      - Compliance: Data retention policies, user consent management, cookie policies

      - Infrastructure: HTTPS enforcement, security headers middleware, environment
      variable management


      **Dependencies:**

      - External: @supabase/supabase-js, @fastify/rate-limit, @fastify/cors, @fastify/helmet,
      zod, bcryptjs, jsonwebtoken, snyk

      - Internal: Database schema with user roles, middleware layer in Fastify, auth
      context in Next.js


      **Risks:**

      - Over-engineering early: Start with Supabase Auth basics, add complexity incrementally

      - Performance impact: Rate limiting and encryption can slow requests - implement
      with monitoring

      - Compliance complexity: GDPR requirements may be extensive - focus on core
      requirements first

      - Third-party dependencies: AI services need secure API key management and usage
      monitoring


      **Complexity Notes:**

      Higher complexity than initially expected due to multi-tenant nature (users,
      creators, admins) and AI integration security requirements. The creative content
      aspect adds IP protection concerns. However, Supabase provides much of the auth
      infrastructure out-of-the-box.


      **Key Files:**

      - packages/backend/src/middleware/auth.ts: JWT validation, user context

      - packages/backend/src/middleware/security.ts: Rate limiting, CORS, security
      headers

      - packages/backend/src/lib/supabase.ts: Supabase client configuration

      - packages/frontend/src/lib/auth.ts: Client-side auth utilities

      - packages/frontend/src/middleware.ts: Next.js middleware for protected routes

      - apps/dashboard/src/components/AuthProvider.tsx: Auth context provider

      - supabase/migrations/: RLS policies and user roles schema

      '
    design_decisions:
    - decision: Use Supabase Auth as primary authentication system
      rationale: Provides enterprise-grade auth with minimal setup, includes social
        logins, MFA, and integrates seamlessly with PostgreSQL RLS
      alternatives_considered:
      - Auth0
      - Firebase Auth
      - Custom JWT implementation
    - decision: Implement PostgreSQL Row Level Security for authorization
      rationale: Database-level security ensures data isolation even if application
        logic fails, scales automatically with queries
      alternatives_considered:
      - Application-level RBAC
      - Separate authorization service
    - decision: Use Zod for API request/response validation
      rationale: Type-safe validation that generates TypeScript types, prevents injection
        attacks and data corruption
      alternatives_considered:
      - Joi
      - Yup
      - Custom validation
    researched_at: '2026-02-07T18:47:39.570861'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:24:45.220737'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Establish a layered security architecture starting with Supabase Auth
      for user management and PostgreSQL RLS for data access control. Implement Fastify
      middleware for rate limiting, CORS, and security headers. Create reusable auth
      utilities for both backend and frontend, with Next.js middleware protecting
      dashboard routes. Set up automated security scanning and audit logging for compliance
      requirements.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.39.0
      reason: Primary authentication and database client
    - name: '@fastify/rate-limit'
      version: ^9.1.0
      reason: API rate limiting protection
    - name: '@fastify/helmet'
      version: ^11.1.1
      reason: Security headers middleware
    - name: '@fastify/cors'
      version: ^9.0.1
      reason: Cross-origin request security
    - name: zod
      version: ^3.22.4
      reason: Request validation and type safety
    - name: bcryptjs
      version: ^2.4.3
      reason: Password hashing for additional security layers
    - name: '@fastify/env'
      version: ^4.3.0
      reason: Environment variable validation and management
    files_to_modify:
    - path: packages/backend/package.json
      changes: 'Add security dependencies: @fastify/rate-limit, @fastify/cors, @fastify/helmet,
        bcryptjs, zod, snyk'
    - path: packages/frontend/package.json
      changes: Add @supabase/supabase-js and auth-related utilities
    - path: supabase/config.toml
      changes: Configure auth providers, JWT settings, and security policies
    new_files:
    - path: packages/backend/src/middleware/auth.ts
      purpose: JWT validation, user context injection, auth guards
    - path: packages/backend/src/middleware/security.ts
      purpose: Rate limiting, CORS, security headers, input validation
    - path: packages/backend/src/lib/supabase.ts
      purpose: Supabase client configuration and connection management
    - path: packages/backend/src/types/auth.ts
      purpose: TypeScript interfaces for user, session, and auth context
    - path: packages/backend/src/utils/encryption.ts
      purpose: Field-level encryption utilities for sensitive data
    - path: packages/frontend/src/lib/auth.ts
      purpose: Client-side auth utilities and Supabase integration
    - path: packages/frontend/src/middleware.ts
      purpose: Next.js middleware for route protection
    - path: apps/dashboard/src/components/AuthProvider.tsx
      purpose: React context provider for authentication state
    - path: apps/dashboard/src/hooks/useAuth.ts
      purpose: Custom hook for auth state management
    - path: supabase/migrations/001_auth_schema.sql
      purpose: User roles, RLS policies, and auth-related database schema
    - path: supabase/migrations/002_audit_logging.sql
      purpose: Audit trail tables and triggers for security events
    - path: .github/workflows/security-scan.yml
      purpose: Automated security scanning with Snyk
    - path: docs/security/authentication.md
      purpose: Authentication implementation and usage documentation
    - path: docs/security/authorization.md
      purpose: Authorization patterns and RLS policy documentation
  acceptance_criteria:
  - criterion: Authentication system successfully validates users with JWT tokens
      and supports Google OAuth login
    verification: Run `npm test auth.test.ts` and manually test login flow at /login
      with Google OAuth
  - criterion: Authorization system enforces RLS policies preventing unauthorized
      data access across user roles
    verification: Integration tests in `auth-integration.test.ts` verify users can
      only access their own data
  - criterion: API security middleware blocks requests exceeding 100 req/min and validates
      all input schemas
    verification: Load test with `curl` commands and verify 429 responses, check Zod
      validation errors in logs
  - criterion: Security headers and HTTPS enforcement are properly configured in production
    verification: Run `npm run security:audit` and verify headers with `curl -I https://app.morpheus.com/api/health`
  - criterion: Audit logging captures all authentication events and data access patterns
    verification: Check Supabase logs show login/logout events and verify audit trail
      queries return expected data
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/middleware/auth.test.ts
      coverage_target: 90%
      scenarios:
      - Valid JWT token validation
      - Expired token rejection
      - Missing token handling
      - Invalid token format
    - file: packages/backend/src/__tests__/middleware/security.test.ts
      coverage_target: 85%
      scenarios:
      - Rate limiting enforcement
      - CORS header validation
      - Input schema validation
      - Security header injection
    - file: packages/frontend/src/__tests__/lib/auth.test.ts
      coverage_target: 80%
      scenarios:
      - User session management
      - Auth state persistence
      - Logout cleanup
    integration_tests:
    - file: packages/backend/src/__tests__/integration/auth-integration.test.ts
      scenarios:
      - Complete OAuth login flow
      - Protected route access control
      - Multi-tenant data isolation
      - Rate limiting across requests
    - file: apps/dashboard/src/__tests__/integration/auth-flow.test.ts
      scenarios:
      - Dashboard authentication flow
      - Route protection middleware
    manual_testing:
    - step: Attempt login with Google OAuth
      expected: Redirects to Google, returns with valid session
    - step: Try accessing protected API endpoint without token
      expected: Returns 401 Unauthorized
    - step: Make 101 requests in 1 minute to any endpoint
      expected: Request 101 returns 429 Too Many Requests
    - step: Verify security headers in browser dev tools
      expected: X-Frame-Options, CSP, HSTS headers present
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Install and configure security dependencies in backend and frontend packages
      done: false
    - task: Set up Supabase Auth configuration with Google OAuth provider
      done: false
    - task: Create database migrations for user roles, RLS policies, and audit logging
      done: false
    - task: Implement backend auth and security middleware with rate limiting
      done: false
    - task: Create frontend auth utilities and Next.js middleware for route protection
      done: false
    - task: Build dashboard AuthProvider component and useAuth hook
      done: false
    - task: Implement field-level encryption utilities for sensitive data
      done: false
    - task: Set up automated security scanning workflow with Snyk
      done: false
    - task: Write comprehensive unit and integration tests for auth flows
      done: false
    - task: Create security documentation and compliance procedures
      done: false
    - task: Manual testing of complete authentication and authorization flows
      done: false
    - task: Code review and security audit of implementation
      done: false
- key: T14
  title: API Standards & Best Practices
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 2
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      API Standards & Best Practices is foundational for Morpheus as it establishes
      consistent patterns for all backend endpoints handling novel-to-comic transformations.
      This task ensures scalable, maintainable APIs for complex workflows involving
      LLM processing, image generation, user management, and real-time comic creation
      progress. Without standardized patterns, the API surface will become inconsistent
      as features like chapter processing, image generation queues, and user dashboards
      are added.


      **Technical Approach:**

      - Implement OpenAPI 3.1 spec with @fastify/swagger for auto-generated docs

      - Use @fastify/type-provider-typebox for runtime validation + TypeScript inference

      - Establish RESTful conventions with async operation patterns (POST /comics/{id}/generate
      -> GET /operations/{id})

      - Implement standardized error handling with RFC 7807 Problem Details

      - Add request/response logging with @fastify/under-pressure for backpressure

      - Use @fastify/rate-limit for API protection

      - Implement API versioning strategy (/v1/comics, /v2/comics)

      - Add structured logging with pino for observability


      **Dependencies:**

      - External: @fastify/swagger, @sinclair/typebox, @fastify/rate-limit, @fastify/under-pressure,
      pino

      - Internal: Supabase client setup, authentication middleware, database schema
      definitions


      **Risks:**

      - Over-engineering: mitigation via incremental implementation focusing on core
      comic/user endpoints first

      - Validation overhead: mitigation by using TypeBox''s compile-time optimizations

      - Breaking changes: mitigation through semantic versioning and deprecation strategies

      - Complex async workflows: mitigation via standardized operation/job patterns
      with status polling


      **Complexity Notes:**

      More complex than initially estimated due to Morpheus''s unique requirements
      for long-running ML operations, file uploads for comic panels, and real-time
      progress updates. The async nature of comic generation (text analysis -> scene
      extraction -> image generation -> panel assembly) requires sophisticated API
      design patterns beyond simple CRUD.


      **Key Files:**

      - apps/backend/src/types/api.ts: Central API type definitions and schemas

      - apps/backend/src/plugins/swagger.ts: OpenAPI configuration and documentation

      - apps/backend/src/plugins/validation.ts: Request/response validation setup

      - apps/backend/src/lib/errors.ts: Standardized error handling and Problem Details

      - apps/backend/src/routes/v1/: Version 1 API route implementations

      - apps/backend/src/middleware/: Auth, rate limiting, logging middleware

      '
    design_decisions:
    - decision: Use TypeBox instead of Zod for API validation
      rationale: Better performance for high-throughput comic generation APIs, native
        Fastify integration, and JSON Schema compliance for OpenAPI
      alternatives_considered:
      - Zod with @fastify/type-provider-zod
      - Custom validation
      - Joi
    - decision: Implement async operation pattern for ML workflows
      rationale: Comic generation involves multiple ML services (LLM + Stable Diffusion)
        taking 30-300 seconds, requiring non-blocking API design
      alternatives_considered:
      - WebSocket streaming
      - Server-sent events
      - Synchronous with timeouts
    - decision: Use RFC 7807 Problem Details for error responses
      rationale: Provides structured error format compatible with frontend error handling
        and API client libraries
      alternatives_considered:
      - Custom error format
      - Simple message strings
      - HTTP status only
    researched_at: '2026-02-07T18:48:06.145639'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:25:07.810537'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Establish a three-layer API architecture: validation layer (TypeBox
      schemas), business logic layer (comic processing services), and response formatting
      layer (OpenAPI-compliant). Implement async operation patterns for ML workflows
      where POST requests return operation IDs and clients poll GET /operations/{id}
      for status. Use Fastify plugins for cross-cutting concerns like rate limiting,
      authentication, and error handling. Create comprehensive API documentation with
      interactive examples for comic generation workflows.

      '
    external_dependencies:
    - name: '@fastify/swagger'
      version: ^8.14.0
      reason: OpenAPI 3.1 documentation generation and Swagger UI integration
    - name: '@fastify/swagger-ui'
      version: ^2.1.0
      reason: Interactive API documentation interface for development and testing
    - name: '@sinclair/typebox'
      version: ^0.32.0
      reason: High-performance JSON Schema validation with TypeScript inference
    - name: '@fastify/type-provider-typebox'
      version: ^4.0.0
      reason: Fastify integration for TypeBox with automatic type inference
    - name: '@fastify/rate-limit'
      version: ^9.1.0
      reason: API rate limiting to protect ML service resources and prevent abuse
    - name: '@fastify/under-pressure'
      version: ^8.3.0
      reason: Backpressure handling during high comic generation load
    - name: '@fastify/helmet'
      version: ^11.1.1
      reason: Security headers for API endpoints
    - name: pino-pretty
      version: ^10.3.1
      reason: Development-friendly log formatting (dev dependency)
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register swagger, validation, rate-limit, and logging plugins
    - path: apps/backend/package.json
      changes: Add @fastify/swagger, @sinclair/typebox, @fastify/rate-limit, @fastify/under-pressure
        dependencies
    new_files:
    - path: apps/backend/src/types/api.ts
      purpose: Central API type definitions and TypeBox schemas for all endpoints
    - path: apps/backend/src/plugins/swagger.ts
      purpose: OpenAPI 3.1 configuration with Fastify swagger plugin setup
    - path: apps/backend/src/plugins/validation.ts
      purpose: TypeBox-based request/response validation with error formatting
    - path: apps/backend/src/lib/errors.ts
      purpose: RFC 7807 Problem Details error handling and standardized error types
    - path: apps/backend/src/middleware/correlation.ts
      purpose: Request correlation ID generation and propagation
    - path: apps/backend/src/middleware/logging.ts
      purpose: Structured request/response logging with pino
    - path: apps/backend/src/routes/v1/operations.ts
      purpose: Generic async operation status tracking endpoints
    - path: apps/backend/src/routes/v1/comics.ts
      purpose: Comic-specific endpoints demonstrating async patterns
    - path: apps/backend/src/services/operation-tracker.ts
      purpose: Service for managing long-running operation state
  acceptance_criteria:
  - criterion: All API endpoints conform to OpenAPI 3.1 spec with auto-generated documentation
      accessible at /docs
    verification: Navigate to http://localhost:3001/docs and verify interactive Swagger
      UI loads with all endpoints documented
  - criterion: Request/response validation works with TypeBox schemas providing both
      runtime validation and TypeScript inference
    verification: Send invalid request to POST /v1/comics and verify RFC 7807 Problem
      Details error response with 400 status
  - criterion: Async operations follow standardized pattern with operation tracking
    verification: POST /v1/comics/{id}/generate returns operation ID, GET /v1/operations/{id}
      returns status/progress
  - criterion: Rate limiting and backpressure protection active on all endpoints
    verification: Send 100 requests/minute to any endpoint and verify 429 responses
      after limit exceeded
  - criterion: Structured logging captures all API requests with correlation IDs
    verification: Check logs for request/response pairs with matching correlation
      IDs and proper log levels
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/types/api.test.ts
      coverage_target: 90%
      scenarios:
      - TypeBox schema validation success/failure
      - Error formatting to Problem Details
      - Operation status transitions
    - file: apps/backend/src/__tests__/lib/errors.test.ts
      coverage_target: 95%
      scenarios:
      - Standard HTTP errors to Problem Details
      - Custom comic generation errors
      - Error correlation ID propagation
    integration_tests:
    - file: apps/backend/src/__tests__/integration/api-standards.test.ts
      scenarios:
      - Full comic generation async workflow
      - Rate limiting behavior across endpoints
      - OpenAPI spec validation against actual responses
      - Authentication middleware integration
    manual_testing:
    - step: Access /docs endpoint and test comic generation workflow
      expected: Interactive API docs load, can execute POST /v1/comics with valid
        auth
    - step: Submit malformed JSON to any endpoint
      expected: Consistent Problem Details error format returned
    - step: Check application logs during API usage
      expected: Structured JSON logs with correlation IDs and proper levels
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Install and configure required dependencies (@fastify/swagger, @sinclair/typebox,
        etc.)
      done: false
    - task: Create core API types and TypeBox schemas in apps/backend/src/types/api.ts
      done: false
    - task: Implement standardized error handling with RFC 7807 Problem Details
      done: false
    - task: Setup OpenAPI/Swagger documentation plugin with interactive UI
      done: false
    - task: Create validation middleware using TypeBox for runtime checking
      done: false
    - task: Implement correlation ID and structured logging middleware
      done: false
    - task: Add rate limiting and backpressure protection
      done: false
    - task: Create async operation tracking service and endpoints
      done: false
    - task: Build example comic endpoints demonstrating all patterns
      done: false
    - task: Write comprehensive tests covering all validation and error scenarios
      done: false
- key: T15
  title: Documentation Standards & Templates
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nDocumentation standards are critical for a complex\
      \ monorepo like Morpheus with multiple teams, technologies, and moving parts.\
      \ Without clear documentation templates and standards, we risk knowledge silos,\
      \ inconsistent API docs, poor onboarding experience, and technical debt accumulation.\
      \ This is especially important for M0 as it sets the foundation for all future\
      \ development phases.\n\n**Technical Approach:**\n- Use TypeDoc for TypeScript\
      \ API documentation with automated generation\n- Implement VitePress for comprehensive\
      \ project documentation (supports Vue components, TypeScript, and excellent\
      \ DX)\n- Create standardized templates for: ADRs (Architecture Decision Records),\
      \ API endpoints, component documentation, deployment guides\n- Integrate documentation\
      \ generation into Turborepo build pipeline\n- Use OpenAPI/Swagger for REST API\
      \ documentation with automatic generation from Fastify schemas\n- Implement\
      \ JSDoc standards for React components with Storybook integration\n- Create\
      \ documentation linting with markdownlint and vale for prose quality\n\n**Dependencies:**\n\
      - External: VitePress, TypeDoc, @apidevtools/swagger-jsdoc, markdownlint-cli2,\
      \ vale\n- Internal: Fastify route schemas, React component structure, Turborepo\
      \ build system\n\n**Risks:**\n- Documentation drift: docs become outdated as\
      \ code evolves\n  Mitigation: Automated doc generation, CI/CD checks, documentation\
      \ reviews in PR process\n- Over-documentation: too much process slows development\n\
      \  Mitigation: Focus on high-value docs (APIs, architecture decisions, onboarding),\
      \ lightweight templates\n- Tool fragmentation: different tools for different\
      \ doc types creates confusion\n  Mitigation: Centralized documentation site\
      \ with unified search and navigation\n\n**Complexity Notes:**\nInitially seems\
      \ simple but actually quite complex due to the need to integrate with multiple\
      \ technologies (TypeScript, React, Fastify, Supabase) and establish sustainable\
      \ processes. The automation and tooling setup is more involved than just writing\
      \ markdown files.\n\n**Key Files:**\n- docs/: VitePress documentation site structure\n\
      - .vitepress/config.ts: documentation site configuration\n- templates/: ADR,\
      \ API, component documentation templates\n- turbo.json: add documentation build\
      \ tasks\n- package.json: documentation dependencies and scripts\n- .github/workflows/:\
      \ CI for documentation validation and deployment\n"
    design_decisions:
    - decision: Use VitePress over Docusaurus or GitBook
      rationale: Better TypeScript integration, Vue ecosystem alignment with our frontend
        stack, excellent performance, and simpler setup for technical documentation
      alternatives_considered:
      - Docusaurus (React-based but heavier)
      - GitBook (not dev-friendly)
      - Nextra (tied to Next.js)
    - decision: Implement ADRs (Architecture Decision Records) for major technical
        decisions
      rationale: Captures decision context and rationale for future reference, especially
        important for ML/AI architecture choices in Morpheus
      alternatives_considered:
      - Wiki pages (less structured)
      - Code comments only (not discoverable)
    - decision: Automate API documentation generation from Fastify schemas
      rationale: Ensures documentation stays in sync with actual API implementation,
        reduces manual maintenance burden
      alternatives_considered:
      - Manual Swagger writing (prone to drift)
      - Postman collections (not dev-friendly)
    researched_at: '2026-02-07T18:48:29.765794'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:25:29.086569'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Set up VitePress as the central documentation hub with automated TypeDoc
      integration for API references. Create structured templates for common documentation
      needs (ADRs, API endpoints, components) and integrate generation into the Turborepo
      build pipeline. Implement documentation linting and validation in CI/CD to ensure
      quality and consistency across all team contributions.

      '
    external_dependencies:
    - name: vitepress
      version: ^1.0.0
      reason: Modern documentation framework with excellent TypeScript support
    - name: typedoc
      version: ^0.25.0
      reason: Generate API documentation from TypeScript code
    - name: '@fastify/swagger'
      version: ^8.12.0
      reason: Generate OpenAPI specs from Fastify route schemas
    - name: markdownlint-cli2
      version: ^0.10.0
      reason: Lint markdown files for consistency and quality
    - name: vale
      version: ^2.29.0
      reason: Prose linting for documentation quality and style consistency
    files_to_modify:
    - path: package.json
      changes: Add VitePress, TypeDoc, markdownlint dependencies and documentation
        scripts
    - path: turbo.json
      changes: Add docs:build, docs:lint, and docs:generate-api tasks with proper
        dependencies
    - path: .github/workflows/ci.yml
      changes: Add documentation validation and deployment steps
    - path: apps/backend/src/routes/index.ts
      changes: Add OpenAPI schema documentation annotations
    new_files:
    - path: docs/.vitepress/config.ts
      purpose: VitePress configuration with navigation, theme, and plugin setup
    - path: docs/index.md
      purpose: Main documentation homepage with project overview
    - path: docs/contributing/README.md
      purpose: Contribution guidelines and development workflow
    - path: docs/templates/adr-template.md
      purpose: Architecture Decision Record template with instructions
    - path: docs/templates/api-endpoint-template.md
      purpose: REST API endpoint documentation template
    - path: docs/templates/component-template.md
      purpose: React component documentation template
    - path: tools/docs-generator/package.json
      purpose: Documentation tooling package for custom generators
    - path: tools/docs-generator/src/typedoc-plugin.ts
      purpose: Custom TypeDoc plugin for Morpheus-specific formatting
    - path: tools/docs-generator/src/openapi-generator.ts
      purpose: Generate OpenAPI docs from Fastify route schemas
    - path: .markdownlint.json
      purpose: Markdown linting configuration for consistent formatting
    - path: .vale/config.ini
      purpose: Prose linting configuration for documentation quality
    - path: docs/architecture/decisions/README.md
      purpose: ADR index with links to all architecture decisions
    - path: docs/api/README.md
      purpose: API documentation index with links to generated docs
    - path: docs/deployment/README.md
      purpose: Deployment guides and infrastructure documentation
  acceptance_criteria:
  - criterion: VitePress documentation site runs locally and builds successfully
    verification: Run `npm run docs:dev` and `npm run docs:build` commands execute
      without errors
  - criterion: TypeDoc API documentation auto-generates from TypeScript sources
    verification: Check that `/docs/api/` contains generated API docs after running
      `npm run docs:generate-api`
  - criterion: All documentation templates are accessible and functional
    verification: Verify ADR, API endpoint, and component templates exist in `/docs/templates/`
      with example usage
  - criterion: Documentation linting passes in CI/CD pipeline
    verification: Run `npm run docs:lint` passes with zero errors and warnings
  - criterion: OpenAPI documentation auto-generates from Fastify schemas
    verification: Access `/docs/api/rest/` and verify Swagger UI displays all API
      endpoints with schemas
  testing:
    unit_tests:
    - file: tools/docs-generator/src/__tests__/template-parser.test.ts
      coverage_target: 90%
      scenarios:
      - Template parsing with valid markdown
      - Error handling for malformed templates
      - Variable substitution in templates
    integration_tests:
    - file: tools/docs-generator/src/__tests__/integration/build-pipeline.test.ts
      scenarios:
      - End-to-end documentation generation
      - TypeDoc integration with VitePress
      - OpenAPI schema extraction and formatting
    manual_testing:
    - step: Navigate to local docs site at http://localhost:5173
      expected: Documentation site loads with proper navigation and search
    - step: Create new ADR using template and build docs
      expected: ADR appears in documentation with proper formatting
    - step: Modify TypeScript interface and regenerate API docs
      expected: API documentation reflects the interface changes
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Install and configure VitePress with TypeScript support
      done: false
    - task: Create documentation site structure and navigation
      done: false
    - task: Set up TypeDoc integration with custom configuration
      done: false
    - task: Create standardized documentation templates
      done: false
    - task: Implement OpenAPI documentation generation from Fastify schemas
      done: false
    - task: Configure documentation linting with markdownlint and vale
      done: false
    - task: Integrate documentation tasks into Turborepo pipeline
      done: false
    - task: Set up CI/CD for documentation validation and deployment
      done: false
    - task: Create initial documentation content and examples
      done: false
    - task: Test full documentation generation and deployment workflow
      done: false
- key: T16
  title: Deployment Strategy & Procedures
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nDeployment strategy is critical for Morpheus\
      \ as it's a multi-service platform with backend APIs, frontend applications,\
      \ database migrations, and ML model integrations. This task establishes reliable,\
      \ automated deployment procedures for development, staging, and production environments.\
      \ Without proper deployment strategy, the team risks manual errors, inconsistent\
      \ environments, downtime during releases, and difficulty scaling the platform.\n\
      \n**Technical Approach:**\n- **Container-first deployment** using Docker for\
      \ consistent environments across services\n- **Platform-as-a-Service** deployment\
      \ via Railway/Render for backend, Vercel for Next.js frontends\n- **Database\
      \ migrations** integrated into deployment pipeline with Supabase CLI\n- **Multi-environment\
      \ strategy** (dev/staging/prod) with environment-specific configurations\n-\
      \ **CI/CD pipeline** using GitHub Actions triggered by Turborepo change detection\n\
      - **Health checks and rollback procedures** for zero-downtime deployments\n\
      - **Environment variable management** using platform-native secret management\n\
      - **ML model deployment** strategy for RunPod integration and model versioning\n\
      \n**Dependencies:**\n- External: Docker, GitHub Actions, Vercel CLI, Railway\
      \ CLI, Supabase CLI, @vercel/turborepo\n- Internal: All workspace packages,\
      \ environment configuration, database schemas, API routes\n\n**Risks:**\n- **Environment\
      \ drift**: Different behavior between local/staging/prod - mitigate with containerization\
      \ and strict environment parity\n- **Database migration failures**: Schema changes\
      \ breaking production - mitigate with migration testing and rollback procedures\n\
      - **ML model availability**: RunPod downtime affecting image generation - mitigate\
      \ with fallback strategies and health checks\n- **Secrets exposure**: API keys\
      \ leaked in deployment logs - mitigate with proper secret management and log\
      \ filtering\n- **Monorepo complexity**: Deploying unchanged services unnecessarily\
      \ - mitigate with Turborepo's change detection\n\n**Complexity Notes:**\nMore\
      \ complex than initially estimated due to ML integration requirements and monorepo\
      \ coordination. The need to deploy multiple services with different requirements\
      \ (Fastify backend, Next.js apps, ML workers) while maintaining data consistency\
      \ adds significant complexity. However, modern PaaS platforms reduce infrastructure\
      \ management overhead.\n\n**Key Files:**\n- .github/workflows/deploy.yml: Main\
      \ CI/CD pipeline\n- docker-compose.yml: Local development environment\n- apps/*/Dockerfile:\
      \ Service-specific container configurations  \n- packages/database/migrations/:\
      \ Database schema changes\n- deploy/: Deployment scripts and configurations\n\
      - turbo.json: Build and deployment task definitions\n"
    design_decisions:
    - decision: Use platform-specific deployment (Vercel for Next.js, Railway for
        Fastify) rather than single platform
      rationale: Optimizes each service for its ideal platform - Vercel excels at
        Next.js with edge functions, Railway provides better backend service management
      alternatives_considered:
      - Single platform (Vercel/Railway for everything)
      - Self-managed Kubernetes cluster
      - AWS ECS/Fargate
    - decision: Implement blue-green deployment strategy for backend services
      rationale: Ensures zero-downtime deployments for API services that handle payment
        processing and user data
      alternatives_considered:
      - Rolling deployments
      - Canary deployments
      - Direct replacement
    - decision: Use Supabase CLI for database migration management
      rationale: Native integration with Supabase provides atomic migrations, rollback
        capabilities, and environment synchronization
      alternatives_considered:
      - Custom migration scripts
      - Prisma migrations
      - Manual SQL execution
    researched_at: '2026-02-07T18:48:54.375562'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:25:50.742637'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a multi-platform deployment strategy using GitHub Actions
      for orchestration. Backend Fastify services deploy to Railway with Docker containers,
      Next.js applications deploy to Vercel with optimized builds, and database migrations
      run via Supabase CLI. Use Turborepo''s change detection to deploy only modified
      packages, with comprehensive health checks and automated rollback procedures
      for production deployments.

      '
    external_dependencies:
    - name: '@vercel/cli'
      version: ^32.0.0
      reason: Deploy Next.js dashboard and storefront to Vercel platform
    - name: '@railway/cli'
      version: ^3.0.0
      reason: Deploy Fastify backend services to Railway platform
    - name: supabase
      version: ^1.120.0
      reason: Manage database migrations and environment synchronization
    - name: docker
      version: ^24.0.0
      reason: Container runtime for consistent deployment environments
    - name: '@docker/actions'
      version: ^3.0.0
      reason: GitHub Actions integration for container builds and pushes
    - name: dotenv-cli
      version: ^7.3.0
      reason: Environment variable management in deployment scripts
    files_to_modify:
    - path: turbo.json
      changes: Add deploy pipeline tasks and change detection filters for each service
    - path: package.json
      changes: Add deployment scripts for all platforms and environments
    - path: apps/backend/package.json
      changes: Add Railway-specific deployment configuration and health check endpoints
    - path: apps/frontend/package.json
      changes: Add Vercel deployment configuration and build optimization
    new_files:
    - path: .github/workflows/deploy.yml
      purpose: Main CI/CD pipeline orchestrating multi-platform deployments
    - path: .github/workflows/deploy-staging.yml
      purpose: Staging environment deployment pipeline
    - path: .github/workflows/deploy-production.yml
      purpose: Production deployment with additional safety checks
    - path: docker-compose.yml
      purpose: Local development environment matching production containers
    - path: apps/backend/Dockerfile
      purpose: Backend service containerization for Railway deployment
    - path: apps/worker/Dockerfile
      purpose: ML worker service containerization
    - path: deploy/scripts/health-check.sh
      purpose: Universal health check script for all services
    - path: deploy/scripts/rollback.sh
      purpose: Automated rollback procedures for failed deployments
    - path: deploy/scripts/migrate.sh
      purpose: Database migration execution with validation
    - path: deploy/config/railway.json
      purpose: Railway platform configuration for backend services
    - path: deploy/config/vercel.json
      purpose: Vercel platform configuration for frontend applications
    - path: deploy/utils/deployment-utils.ts
      purpose: Shared utilities for deployment validation and monitoring
    - path: deploy/environments/.env.staging
      purpose: Staging environment configuration template
    - path: deploy/environments/.env.production
      purpose: Production environment configuration template
  acceptance_criteria:
  - criterion: All services deploy successfully to their respective platforms (Railway
      for backend, Vercel for frontend, Supabase for database)
    verification: Run `npm run deploy:all` and verify all deployment URLs return 200
      status with health checks passing
  - criterion: Only changed packages trigger deployments based on Turborepo change
      detection
    verification: Make isolated changes to single package and verify only that service
      deploys, check GitHub Actions logs for deployment skips
  - criterion: Database migrations run automatically before backend deployments without
      data loss
    verification: Deploy schema changes and verify migration logs show successful
      execution with rollback capability intact
  - criterion: Production deployments include automated rollback on health check failures
    verification: Simulate deployment failure and verify automatic rollback to previous
      version within 5 minutes
  - criterion: Environment variables and secrets are properly managed across all deployment
      targets
    verification: Check deployment logs contain no exposed secrets and services can
      access required environment variables
  testing:
    unit_tests:
    - file: deploy/__tests__/deployment-utils.test.ts
      coverage_target: 90%
      scenarios:
      - Health check validation
      - Environment variable parsing
      - Migration status checking
      - Rollback trigger conditions
    - file: packages/database/__tests__/migrations.test.ts
      coverage_target: 85%
      scenarios:
      - Migration execution order
      - Rollback procedures
      - Schema validation
    integration_tests:
    - file: deploy/__tests__/integration/full-deployment.test.ts
      scenarios:
      - End-to-end deployment pipeline
      - Multi-service coordination
      - Cross-platform deployment verification
    - file: deploy/__tests__/integration/rollback.test.ts
      scenarios:
      - Automatic rollback on failure
      - Manual rollback procedures
    manual_testing:
    - step: Deploy to staging environment and verify all services are accessible
      expected: All health check endpoints return 200, frontend loads correctly, backend
        APIs respond
    - step: Simulate production deployment failure by breaking health check
      expected: Automatic rollback completes within 5 minutes, previous version restored
    - step: Test database migration with sample schema change
      expected: Migration completes successfully, data integrity maintained, rollback
        available
  estimates:
    development: 4
    code_review: 1
    testing: 2
    documentation: 1
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Set up GitHub Actions workflows for CI/CD pipeline
      done: false
    - task: Create Dockerfile configurations for all containerized services
      done: false
    - task: Configure Railway deployment for backend Fastify services
      done: false
    - task: Configure Vercel deployment for Next.js frontend applications
      done: false
    - task: Implement database migration automation with Supabase CLI
      done: false
    - task: Set up Turborepo change detection and selective deployment
      done: false
    - task: Create health check endpoints and monitoring scripts
      done: false
    - task: Implement automated rollback procedures for production
      done: false
    - task: Configure environment variable management and secrets
      done: false
    - task: Test full deployment pipeline across all environments
      done: false
- key: T17
  title: SLA/SLO/SLI Definitions
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      SLA/SLO/SLI definitions are critical for establishing measurable reliability
      targets for the Morpheus platform. Given the AI-heavy workload (comic generation,
      image processing), users expect predictable response times and availability.
      SLAs (Service Level Agreements) define external commitments to users, SLOs (Service
      Level Objectives) set internal reliability targets, and SLIs (Service Level
      Indicators) provide measurable metrics. This is essential for M0 as it establishes
      the foundation for monitoring, alerting, and capacity planning before launch.


      **Technical Approach:**

      Implement OpenTelemetry for distributed tracing and metrics collection across
      the Fastify backend and Next.js frontend. Use Prometheus for metrics storage
      and Grafana for visualization. Define SLIs using the four golden signals: latency,
      traffic, errors, and saturation. For Morpheus specifically: API response times,
      comic generation completion rates, image generation success rates, dashboard
      load times, and database query performance. Store SLO configurations in code
      using a standardized format that can be consumed by alerting systems.


      **Dependencies:**

      - External: [@opentelemetry/auto-instrumentations-node, @opentelemetry/api,
      prom-client, @supabase/supabase-js (metrics), grafana/grafana, prometheus/prometheus]

      - Internal: Backend API routes, ML pipeline services, database connection pools,
      frontend performance monitoring


      **Risks:**

      - Over-instrumentation overhead: Start with critical paths only, expand gradually

      - Alert fatigue from too-strict SLOs: Begin with loose targets based on baseline
      measurements

      - Inconsistent measurement across services: Standardize telemetry collection
      patterns

      - Third-party dependency SLAs (OpenAI, RunPod): Factor external service reliability
      into internal SLOs


      **Complexity Notes:**

      More complex than initially estimated due to AI/ML workload unpredictability.
      Comic generation times vary significantly based on complexity, and GPU availability
      on RunPod affects consistency. Need separate SLOs for different user tiers and
      content types. The distributed nature (frontend, backend, ML services, database)
      requires careful correlation of metrics.


      **Key Files:**

      - packages/backend/src/telemetry/slos.ts: SLO definitions and validation

      - packages/backend/src/middleware/metrics.ts: Request instrumentation

      - apps/dashboard/lib/performance.ts: Frontend performance tracking

      - packages/shared/types/monitoring.ts: Shared SLO/SLI type definitions

      - .github/workflows/slo-reporting.yml: Automated SLO compliance reporting

      '
    design_decisions:
    - decision: Use OpenTelemetry standard with Prometheus metrics
      rationale: Industry standard, vendor-neutral, integrates well with modern observability
        stack and Supabase monitoring
      alternatives_considered:
      - DataDog APM
      - New Relic
      - Custom metrics solution
    - decision: Define SLOs as code in TypeScript configuration
      rationale: Version controlled, type-safe, can be consumed by both monitoring
        and testing systems
      alternatives_considered:
      - YAML configuration files
      - Database-stored SLOs
      - Grafana-only dashboards
    researched_at: '2026-02-07T18:49:17.341643'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:26:12.955583'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a layered monitoring approach: instrument critical paths
      with OpenTelemetry, collect metrics in Prometheus format, and define SLOs as
      TypeScript configurations. Create service-specific SLI measurements (API latency,
      ML job completion rates, database query times) that roll up to user-facing SLAs.
      Use Supabase''s built-in monitoring for database SLIs and integrate with custom
      application metrics. Establish baseline measurements during development to set
      realistic initial SLO targets.

      '
    external_dependencies:
    - name: '@opentelemetry/auto-instrumentations-node'
      version: ^0.41.0
      reason: Automatic instrumentation for Node.js backend services
    - name: prom-client
      version: ^15.1.0
      reason: Prometheus metrics collection and exposition
    - name: '@opentelemetry/semantic-conventions'
      version: ^1.21.0
      reason: Standardized telemetry attribute names
    - name: web-vitals
      version: ^3.5.0
      reason: Frontend performance metrics (CLS, FCP, LCP)
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Add OpenTelemetry initialization and metrics middleware registration
    - path: packages/backend/src/routes/comics.ts
      changes: Add comic generation success/failure metric instrumentation
    - path: apps/dashboard/pages/_app.tsx
      changes: Initialize frontend performance monitoring with Web Vitals tracking
    - path: packages/backend/package.json
      changes: Add OpenTelemetry and Prometheus client dependencies
    new_files:
    - path: packages/backend/src/telemetry/slos.ts
      purpose: Define SLO configurations, validation schemas, and compliance calculations
    - path: packages/backend/src/middleware/metrics.ts
      purpose: HTTP request instrumentation middleware for latency and error tracking
    - path: packages/backend/src/telemetry/collectors.ts
      purpose: Custom metric collectors for ML pipeline and database performance
    - path: packages/shared/types/monitoring.ts
      purpose: TypeScript interfaces for SLI/SLO definitions and metric payloads
    - path: apps/dashboard/lib/performance.ts
      purpose: Frontend performance tracking with Core Web Vitals integration
    - path: monitoring/grafana/slo-dashboard.json
      purpose: Grafana dashboard configuration for SLO visualization
    - path: monitoring/prometheus/rules.yml
      purpose: Prometheus recording rules and alert definitions for SLO violations
    - path: .github/workflows/slo-reporting.yml
      purpose: Automated weekly SLO compliance reporting workflow
    - path: scripts/validate-slos.js
      purpose: CLI tool for SLO configuration validation and deployment checks
  acceptance_criteria:
  - criterion: All critical API endpoints report latency metrics with p50, p95, p99
      percentiles
    verification: Query Prometheus /metrics endpoint and verify presence of http_request_duration_seconds
      histogram for all routes
  - criterion: SLO compliance dashboard shows green/yellow/red status for all defined
      objectives
    verification: Access Grafana dashboard at /slo-overview and verify all panels
      display current SLO burn rates and error budgets
  - criterion: Automated SLO alerting fires when error budget consumption exceeds
      thresholds
    verification: Trigger artificial latency/errors and confirm alert notifications
      within 5 minutes
  - criterion: Comic generation pipeline reports success rate and completion time
      SLIs
    verification: Generate test comic and verify ml_job_completion_rate and ml_job_duration_seconds
      metrics appear in Prometheus
  - criterion: SLO definitions are version-controlled and validated on deployment
    verification: Run 'npm run validate:slos' command successfully and check SLO configs
      in Git history
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/telemetry/slos.test.ts
      coverage_target: 90%
      scenarios:
      - SLO configuration validation
      - Metric calculation accuracy
      - Error budget computation
      - Invalid SLO config rejection
    - file: packages/backend/src/__tests__/middleware/metrics.test.ts
      coverage_target: 85%
      scenarios:
      - Request instrumentation
      - Error rate tracking
      - Custom label injection
    integration_tests:
    - file: packages/backend/src/__tests__/integration/monitoring.test.ts
      scenarios:
      - End-to-end metric collection pipeline
      - SLO compliance calculation with real requests
      - Alert trigger conditions
    manual_testing:
    - step: Load test API with 100 concurrent requests
      expected: Latency histograms populate correctly in Prometheus
    - step: Generate multiple comics with varying complexity
      expected: ML pipeline SLIs track success rates and durations
    - step: Access SLO dashboard during normal and degraded conditions
      expected: Visual indicators reflect system health accurately
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Install and configure OpenTelemetry dependencies
      done: false
    - task: Implement core SLO definition schema and validation
      done: false
    - task: Create metrics middleware for HTTP request instrumentation
      done: false
    - task: Add custom collectors for ML pipeline and database metrics
      done: false
    - task: Build frontend performance monitoring integration
      done: false
    - task: Configure Prometheus recording rules and Grafana dashboards
      done: false
    - task: Setup automated alerting for SLO violations
      done: false
    - task: Create SLO validation CLI and CI integration
      done: false
    - task: Write comprehensive documentation and runbooks
      done: false
    - task: Conduct load testing to establish baseline SLO targets
      done: false
- key: T18
  title: Performance Budgets & Optimization
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nPerformance budgets define measurable thresholds\
      \ for web performance metrics (load times, bundle sizes, Core Web Vitals) that\
      \ the Morpheus platform must not exceed. This is critical for M0 because:\n\
      - Comic generation involves heavy image processing that can impact UX\n- Dashboard\
      \ needs to remain responsive during long-running ML operations  \n- Storefront\
      \ must load quickly for user engagement and SEO\n- Early establishment prevents\
      \ performance debt accumulation\n- Sets foundation for monitoring and alerting\
      \ systems\n\n**Technical Approach:**\nImplement multi-layered performance budgets\
      \ covering:\n1. Bundle size limits per application/route\n2. Core Web Vitals\
      \ thresholds (LCP <2.5s, FID <100ms, CLS <0.1)\n3. API response time budgets\
      \ (<200ms for critical paths)\n4. Image optimization and delivery budgets\n\
      5. Real User Monitoring (RUM) integration\n6. CI/CD performance gates using\
      \ Lighthouse CI\n7. Runtime performance monitoring with error budgets\n\n**Dependencies:**\n\
      - External: @lhci/cli, web-vitals, @vercel/analytics, bundle-analyzer, perfume.js\n\
      - Internal: Requires integration with existing Turborepo build system, Supabase\
      \ logging, Next.js analytics\n\n**Risks:**\n- Budget thresholds too strict:\
      \ Start conservative, iterate based on real usage\n- CI/CD pipeline slowdown:\
      \ Use parallel execution and caching strategies\n- False positives in monitoring:\
      \ Implement proper error boundaries and sampling\n- Performance vs feature velocity\
      \ tension: Establish clear escalation processes\n\n**Complexity Notes:**\nHigher\
      \ complexity than initial estimate due to:\n- Multi-application monorepo requiring\
      \ different budgets per app\n- ML workload performance patterns being unpredictable\n\
      - Need for both synthetic and RUM monitoring\n- Integration with existing Fastify/Next.js\
      \ performance tooling\n\n**Key Files:**\n- turbo.json: Add performance check\
      \ tasks\n- apps/dashboard/next.config.js: Bundle analyzer and performance config\n\
      - apps/storefront/next.config.js: Similar performance optimizations\n- packages/monitoring:\
      \ New package for shared performance utilities\n- .github/workflows/performance.yml:\
      \ CI performance gates\n- lighthouserc.js: Lighthouse CI configuration\n"
    design_decisions:
    - decision: Use Lighthouse CI with custom performance budgets per application
      rationale: Provides consistent synthetic monitoring across all environments
        with configurable thresholds
      alternatives_considered:
      - WebPageTest API
      - Custom performance testing
      - Vercel Analytics only
    - decision: Implement tiered budget system (strict/warning/error levels)
      rationale: Allows gradual performance improvements without blocking development
      alternatives_considered:
      - Binary pass/fail budgets
      - Manual performance reviews
    - decision: Separate budgets for dashboard vs storefront applications
      rationale: Different user expectations and performance characteristics require
        tailored thresholds
      alternatives_considered:
      - Single unified budget
      - Route-level budgets only
    researched_at: '2026-02-07T18:49:40.035426'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:26:38.216567'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Establish performance budgets using Lighthouse CI for synthetic monitoring
      and web-vitals for RUM data collection. Configure tiered budgets (warning/error
      thresholds) per application in the monorepo. Integrate performance checks into
      Turborepo build pipeline and GitHub Actions. Create shared monitoring utilities
      package for consistent performance tracking across dashboard, storefront, and
      API layers. Set up automated alerts and reporting dashboard using Supabase for
      performance data storage.

      '
    external_dependencies:
    - name: '@lhci/cli'
      version: ^0.12.0
      reason: Lighthouse CI for automated performance testing and budgets
    - name: web-vitals
      version: ^3.5.0
      reason: Real user monitoring of Core Web Vitals metrics
    - name: '@next/bundle-analyzer'
      version: ^14.0.0
      reason: Bundle size analysis and optimization for Next.js apps
    - name: perfume.js
      version: ^8.4.0
      reason: Comprehensive performance monitoring library
    - name: '@vercel/analytics'
      version: ^1.1.0
      reason: Web analytics and performance insights integration
    files_to_modify:
    - path: turbo.json
      changes: Add 'perf:check', 'perf:analyze', 'perf:monitor' tasks with dependencies
    - path: apps/dashboard/next.config.js
      changes: Add bundle analyzer, performance configs, web-vitals integration
    - path: apps/storefront/next.config.js
      changes: Add performance optimizations, image optimization, Core Web Vitals
        tracking
    - path: package.json
      changes: Add performance-related scripts and dependencies (@lhci/cli, web-vitals,
        perfume.js)
    - path: apps/backend/src/middleware/index.ts
      changes: Add response time tracking middleware with performance logging
    new_files:
    - path: lighthouserc.js
      purpose: Lighthouse CI configuration with per-app performance budgets
    - path: .github/workflows/performance.yml
      purpose: CI/CD performance gates and budget enforcement
    - path: packages/monitoring/package.json
      purpose: Shared monitoring package with performance utilities
    - path: packages/monitoring/src/performance-collector.ts
      purpose: Core Web Vitals and custom metrics collection service
    - path: packages/monitoring/src/budget-validator.ts
      purpose: Performance budget validation and alerting logic
    - path: packages/monitoring/src/types.ts
      purpose: TypeScript interfaces for performance metrics and budgets
    - path: apps/dashboard/src/lib/performance.ts
      purpose: Dashboard-specific performance monitoring integration
    - path: apps/storefront/src/lib/performance.ts
      purpose: Storefront-specific performance tracking and optimization
    - path: apps/dashboard/src/pages/admin/performance.tsx
      purpose: Performance monitoring dashboard UI component
    - path: scripts/performance-report.js
      purpose: Generate performance reports for stakeholders
    - path: docs/performance-budgets.md
      purpose: Documentation for performance budget configuration and monitoring
  acceptance_criteria:
  - criterion: Lighthouse CI fails builds when performance budgets are exceeded (LCP
      >2.5s, bundle size >500KB for critical routes)
    verification: Run `npm run perf:check` - should fail with budget violations and
      pass when under thresholds
  - criterion: Real User Monitoring captures Core Web Vitals for all applications
      with <1% performance overhead
    verification: Check Supabase `performance_metrics` table contains LCP, FID, CLS
      data with timestamps within 5min of page loads
  - criterion: 'Performance budgets are enforced per application with different thresholds
      (Dashboard: 800KB, Storefront: 500KB, API: <200ms)'
    verification: Verify `.lighthouserc.js` contains app-specific budgets and CI workflow
      runs separate checks per app
  - criterion: Automated performance alerts trigger when budgets exceeded in production
    verification: Simulate performance regression - alert should fire within 5 minutes
      via configured webhook/email
  - criterion: Performance monitoring dashboard displays real-time metrics with 7-day
      historical trends
    verification: Navigate to `/admin/performance` - should show current Core Web
      Vitals, API response times, and trend charts
  testing:
    unit_tests:
    - file: packages/monitoring/src/__tests__/performance-collector.test.ts
      coverage_target: 90%
      scenarios:
      - Web vitals data collection and validation
      - API response time tracking
      - Error handling for invalid metrics
      - Performance budget threshold checking
    - file: packages/monitoring/src/__tests__/budget-validator.test.ts
      coverage_target: 85%
      scenarios:
      - Bundle size budget validation
      - Core Web Vitals threshold checking
      - Custom metric budget evaluation
    integration_tests:
    - file: apps/dashboard/src/__tests__/integration/performance.test.ts
      scenarios:
      - Performance metrics are collected during page navigation
      - Bundle size stays within configured budgets
    - file: apps/storefront/src/__tests__/integration/performance.test.ts
      scenarios:
      - Core Web Vitals tracking on product pages
      - Image optimization performance validation
    manual_testing:
    - step: Navigate to dashboard and trigger comic generation
      expected: Performance metrics logged, UI remains responsive, no budget violations
    - step: Run Lighthouse CI on feature branch with intentional performance regression
      expected: CI build fails with clear budget violation message
    - step: Check performance monitoring dashboard after 24h of usage
      expected: Historical data visible, trends accurate, alerts configured
  estimates:
    development: 4.5
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Create packages/monitoring structure and install dependencies
      done: false
    - task: Implement performance collector and budget validator utilities
      done: false
    - task: Configure Lighthouse CI with per-app budgets and thresholds
      done: false
    - task: Integrate web-vitals tracking in dashboard and storefront apps
      done: false
    - task: Add performance middleware to backend API with response time tracking
      done: false
    - task: Set up Supabase performance_metrics table and logging
      done: false
    - task: Create GitHub Actions workflow for CI/CD performance gates
      done: false
    - task: Build performance monitoring dashboard UI
      done: false
    - task: Configure automated alerts and notification system
      done: false
    - task: Write comprehensive documentation and create performance reports
      done: false
- key: T2
  title: GitHub Milestones & Issues Creation
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task involves setting up GitHub project management infrastructure to organize
      development work for the Morpheus platform. It''s needed to establish clear
      project tracking, sprint planning, and release management from the start. This
      is a foundational task that enables the team to work efficiently with proper
      issue tracking, milestone-based releases, and progress visibility. Since this
      is in M0 (Infrastructure & Setup), it''s critical for establishing development
      workflow before any feature work begins.


      **Technical Approach:**

      - Use GitHub''s built-in Issues and Milestones features for project management

      - Create milestone structure aligned with project phases (M0-Infrastructure,
      M1-Core Features, M2-ML Integration, etc.)

      - Establish issue templates for bugs, features, and tasks with consistent labeling

      - Set up issue labels for areas (backend, frontend, ml, testing), priorities
      (p0-p3), and types (bug, feature, task)

      - Create GitHub Actions workflows to automate issue/milestone management

      - Use GitHub Projects (v2) for kanban-style tracking across the monorepo

      - Integrate with commit conventions and PR templates for automatic issue linking


      **Dependencies:**

      - External: GitHub CLI (gh), GitHub REST/GraphQL APIs

      - Internal: Repository structure, team member access permissions, branching
      strategy


      **Risks:**

      - Over-engineering project management: Keep it simple initially, avoid complex
      automation

      - Inconsistent issue creation: Mitigate with templates and team guidelines

      - Milestone scope creep: Define clear milestone criteria and stick to them

      - Label proliferation: Start with core labels, add more only when needed


      **Complexity Notes:**

      This is less technically complex than initially thought - it''s primarily organizational
      setup rather than code. However, it requires careful planning of the development
      workflow and consideration of how issues will be created, tracked, and closed
      throughout the project lifecycle. The complexity lies in establishing the right
      balance of structure without over-process.


      **Key Files:**

      - .github/ISSUE_TEMPLATE/: Issue templates for consistent reporting

      - .github/workflows/: Automation for issue/milestone management

      - .github/PULL_REQUEST_TEMPLATE.md: PR template with issue linking

      - docs/CONTRIBUTING.md: Guidelines for issue creation and milestone planning

      - package.json: Scripts for GitHub CLI automation

      '
    design_decisions:
    - decision: Use GitHub native tools instead of external project management
      rationale: Keeps everything in one place, integrates naturally with code, and
        is free for the team size
      alternatives_considered:
      - Linear
      - Jira
      - Notion
      - Azure DevOps
    - decision: Milestone-driven development with time-boxed releases
      rationale: Aligns with M0-M3 structure already defined, enables clear progress
        tracking
      alternatives_considered:
      - Continuous delivery without milestones
      - Epic-based planning
    - decision: Automated issue linking via commit messages and PR templates
      rationale: Reduces manual overhead while maintaining traceability
      alternatives_considered:
      - Manual linking only
      - Complex GitHub Apps integration
    researched_at: '2026-02-07T18:50:01.198214'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:27:01.073625'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Set up GitHub repository with structured milestones (M0-Infrastructure,
      M1-Core Platform, M2-ML Integration, M3-Production Ready), create issue templates
      for different work types, and establish labeling system for areas/priorities.
      Implement GitHub Actions for automated issue management and create initial issues
      for all major features/tasks. Use GitHub Projects for visual kanban tracking
      across the monorepo workspaces.

      '
    external_dependencies:
    - name: '@actions/github'
      version: ^6.0.0
      reason: GitHub Actions automation for issue/milestone management
    - name: github-cli
      version: ^2.40.0
      reason: Command-line tools for bulk issue creation and management
    files_to_modify:
    - path: package.json
      changes: Add GitHub CLI scripts for milestone/issue management and devDependencies
        for testing
    - path: README.md
      changes: Add project management section with links to issues and milestones
    new_files:
    - path: .github/ISSUE_TEMPLATE/bug_report.yml
      purpose: Structured bug report template with required fields for reproduction
        steps
    - path: .github/ISSUE_TEMPLATE/feature_request.yml
      purpose: Feature request template with user story format and acceptance criteria
    - path: .github/ISSUE_TEMPLATE/task.yml
      purpose: Development task template for technical work items
    - path: .github/ISSUE_TEMPLATE/config.yml
      purpose: Configure issue template chooser and external links
    - path: .github/workflows/issue-management.yml
      purpose: Automated issue labeling, milestone assignment, and project board updates
    - path: .github/workflows/milestone-progress.yml
      purpose: Automated milestone progress tracking and notifications
    - path: .github/PULL_REQUEST_TEMPLATE.md
      purpose: PR template with issue linking and review checklist
    - path: scripts/setup-github.js
      purpose: Node.js script to create milestones, labels, and initial issues programmatically
    - path: scripts/create-issues.js
      purpose: Bulk issue creation script based on project breakdown
    - path: scripts/__tests__/github-setup.test.js
      purpose: Unit tests for GitHub setup scripts
    - path: scripts/__tests__/integration/github-api.test.js
      purpose: Integration tests for GitHub API interactions
    - path: docs/CONTRIBUTING.md
      purpose: Contribution guidelines including issue creation and milestone planning
    - path: docs/PROJECT_MANAGEMENT.md
      purpose: Detailed guide for using GitHub project management features
  acceptance_criteria:
  - criterion: All project milestones (M0-M3) are created with clear descriptions
      and due dates
    verification: Run 'gh milestone list' and verify 4 milestones exist with proper
      naming and descriptions
  - criterion: Issue templates are available for bug reports, feature requests, and
      tasks
    verification: Create new issue via GitHub UI and verify 3 template options appear
      with all required fields
  - criterion: GitHub Project board displays issues organized by status columns with
      proper filtering
    verification: Navigate to Projects tab and verify kanban board shows To Do, In
      Progress, Done columns with area/priority filters
  - criterion: Automated issue labeling workflow triggers on issue creation
    verification: Create test issue and verify GitHub Action runs within 30 seconds
      to apply default labels
  - criterion: Initial development issues are created and properly categorized across
      all project areas
    verification: Run 'gh issue list --limit 50' and verify at least 25 issues exist
      covering backend, frontend, ML, and DevOps areas
  testing:
    unit_tests:
    - file: scripts/__tests__/github-setup.test.js
      coverage_target: 90%
      scenarios:
      - Milestone creation with valid data
      - Issue template validation
      - Label creation and assignment logic
      - Error handling for API failures
    integration_tests:
    - file: scripts/__tests__/integration/github-api.test.js
      scenarios:
      - End-to-end milestone and issue creation flow
      - GitHub Actions workflow validation
      - Project board automation testing
    manual_testing:
    - step: Create issue using each template type (bug, feature, task)
      expected: All required fields present, proper labels auto-applied
    - step: Move issue through project board columns
      expected: Status updates automatically, automation rules trigger
    - step: Link PR to issue using closing keywords
      expected: Issue auto-closes when PR merged, milestone progress updates
    - step: Test GitHub CLI scripts for bulk operations
      expected: Scripts run without errors, create expected issues/milestones
  estimates:
    development: 2
    code_review: 0.5
    testing: 1
    documentation: 0.5
    total: 4
  progress:
    status: not-started
    checklist:
    - task: Install and configure GitHub CLI, create personal access token
      done: false
    - task: Create milestone structure (M0-M3) with descriptions and target dates
      done: false
    - task: Design and implement issue templates for bug, feature, and task types
      done: false
    - task: Set up label taxonomy for areas, priorities, and types
      done: false
    - task: Create GitHub Actions workflows for issue automation
      done: false
    - task: Build Node.js scripts for bulk operations and testing
      done: false
    - task: Set up GitHub Project board with proper columns and automation
      done: false
    - task: Create initial issues for all major development tasks
      done: false
    - task: Write documentation and contribution guidelines
      done: false
    - task: Test all templates, workflows, and automation end-to-end
      done: false
- key: T3
  title: Database Schema v2 Migration
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 5
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Database Schema v2 Migration represents the evolution from initial MVP schemas
      to production-ready database architecture for the novel-to-comic platform. This
      task is critical for establishing proper data relationships between users, novels,
      comic panels, transformation jobs, and AI-generated assets. It enables core
      business features like user authentication, content management, billing integration,
      and ML pipeline data persistence. Without this foundation, subsequent development
      tasks cannot proceed.


      **Technical Approach:**

      Leverage Supabase''s migration system with PostgreSQL-native features including
      row-level security (RLS), triggers, and functions. Implement schema versioning
      with proper rollback capabilities using Supabase CLI. Design normalized tables
      for users, projects, novels, chapters, panels, transformation_jobs, and ai_assets
      with proper foreign key constraints. Use PostgreSQL enums for status fields
      and JSONB for flexible metadata storage. Implement audit trails with created_at/updated_at
      timestamps and soft deletes where appropriate.


      **Dependencies:**

      - External: @supabase/supabase-js ^2.38.0, drizzle-orm for type-safe queries,
      zod for schema validation

      - Internal: Backend authentication service, file storage service, ML pipeline
      integration points


      **Risks:**

      - Data loss during migration: Implement comprehensive backup strategy and test
      migrations in staging

      - Performance degradation: Index key columns and implement proper query optimization

      - Breaking changes to existing code: Version API endpoints and maintain backward
      compatibility

      - RLS policy complexity: Start with simple policies and iterate, extensive testing
      required


      **Complexity Notes:**

      More complex than initially estimated due to Supabase RLS requirements and the
      need to support both dashboard and storefront access patterns. The interconnected
      nature of novel content, comic panels, and AI transformation jobs requires careful
      foreign key design and cascading delete strategies.


      **Key Files:**

      - supabase/migrations/: New migration files with CREATE TABLE statements

      - packages/shared/types/database.ts: TypeScript interfaces for all tables

      - apps/backend/src/services/: Database service layer implementations

      - supabase/config.toml: Environment and RLS configuration

      '
    design_decisions:
    - decision: Use Supabase native migrations over ORM migrations
      rationale: Better integration with Supabase dashboard, RLS policies, and edge
        functions
      alternatives_considered:
      - Drizzle migrations
      - Prisma migrations
      - Custom migration scripts
    - decision: Implement soft deletes for user content
      rationale: Enables content recovery and maintains referential integrity for
        AI training data
      alternatives_considered:
      - Hard deletes with cascade
      - Archive tables
      - Versioned records
    - decision: JSONB for AI model parameters and metadata
      rationale: Flexible schema for different AI model configurations without schema
        changes
      alternatives_considered:
      - Separate parameter tables
      - Text fields with JSON
      - EAV pattern
    researched_at: '2026-02-07T18:50:22.358403'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:27:23.339213'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create Supabase migration files defining core tables (users, projects,
      novels, chapters, panels, transformation_jobs, ai_assets) with proper relationships
      and constraints. Implement Row Level Security policies for multi-tenant access
      control. Generate TypeScript types from schema using Supabase CLI. Create database
      service layer in backend with type-safe query builders using Drizzle ORM. Test
      migration rollback scenarios and seed development data.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.38.0
      reason: Official Supabase client for database operations
    - name: drizzle-orm
      version: ^0.29.0
      reason: Type-safe SQL query builder with Supabase integration
    - name: zod
      version: ^3.22.0
      reason: Runtime schema validation for API inputs/outputs
    - name: supabase
      version: ^1.113.0
      reason: Supabase CLI for migration management and type generation
    files_to_modify:
    - path: supabase/config.toml
      changes: Add RLS configuration and environment-specific settings
    - path: packages/shared/types/index.ts
      changes: Export new database types and update existing interfaces
    - path: apps/backend/src/lib/supabase.ts
      changes: Update client configuration for new schema
    new_files:
    - path: supabase/migrations/20240101000000_schema_v2.sql
      purpose: Main migration file with all table definitions and constraints
    - path: supabase/migrations/20240101000001_rls_policies.sql
      purpose: Row Level Security policies for all tables
    - path: supabase/migrations/20240101000002_functions_triggers.sql
      purpose: Database functions and triggers for audit trails and business logic
    - path: supabase/seed.sql
      purpose: Development seed data for testing
    - path: packages/shared/types/database.ts
      purpose: Generated TypeScript interfaces for all database tables
    - path: apps/backend/src/services/database/base.ts
      purpose: Base database service with common CRUD operations
    - path: apps/backend/src/services/database/users.ts
      purpose: User-specific database operations
    - path: apps/backend/src/services/database/projects.ts
      purpose: Project and novel management database operations
    - path: apps/backend/src/services/database/transformations.ts
      purpose: AI transformation job database operations
    - path: apps/backend/src/services/database/index.ts
      purpose: Database service exports and initialization
    - path: supabase/scripts/backup.sh
      purpose: Automated backup script for migration safety
    - path: supabase/scripts/test-migration.sh
      purpose: Migration testing script for CI/CD
  acceptance_criteria:
  - criterion: All database tables (users, projects, novels, chapters, panels, transformation_jobs,
      ai_assets) are created with proper relationships and constraints
    verification: Run `supabase db describe` and verify all tables exist with correct
      foreign keys and constraints
  - criterion: Row Level Security policies enable proper multi-tenant access control
      for all tables
    verification: Execute test queries as different user roles and verify data isolation
      using `supabase test db`
  - criterion: Migration can be applied and rolled back without data loss
    verification: Run migration, seed test data, rollback, re-apply migration and
      verify data integrity
  - criterion: TypeScript types are generated and match database schema
    verification: Run `supabase gen types typescript` and verify generated types in
      packages/shared/types/database.ts compile without errors
  - criterion: Database service layer provides type-safe CRUD operations for all entities
    verification: Unit tests pass for all service methods and TypeScript compilation
      succeeds without type errors
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/database.test.ts
      coverage_target: 90%
      scenarios:
      - CRUD operations for each table
      - Foreign key constraint validation
      - Soft delete functionality
      - Query builder type safety
    - file: apps/backend/src/__tests__/services/user-service.test.ts
      coverage_target: 85%
      scenarios:
      - User creation and authentication
      - RLS policy enforcement
      - Profile updates
    integration_tests:
    - file: apps/backend/src/__tests__/integration/migration.test.ts
      scenarios:
      - Full migration apply/rollback cycle
      - Data seeding and retrieval
      - Cross-table relationship queries
      - RLS policy integration with auth
    manual_testing:
    - step: Create user account through auth service
      expected: User record created in users table with proper RLS isolation
    - step: Create project with nested novel/chapter/panel structure
      expected: All related records created with proper foreign key relationships
    - step: Test transformation job lifecycle
      expected: Status transitions work correctly with AI assets linked properly
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Set up Supabase CLI and verify connection to development environment
      done: false
    - task: Design and document database schema with entity relationship diagram
      done: false
    - task: Create migration files for tables, constraints, and indexes
      done: false
    - task: Implement Row Level Security policies for multi-tenant access
      done: false
    - task: Create database functions and triggers for audit trails
      done: false
    - task: Generate TypeScript types and update shared package
      done: false
    - task: Implement database service layer with Drizzle ORM integration
      done: false
    - task: Create comprehensive test suite for all database operations
      done: false
    - task: Test migration rollback scenarios and backup procedures
      done: false
    - task: Document schema changes and update API documentation
      done: false
    - task: Code review and security audit of RLS policies
      done: false
- key: T4
  title: Testing Infrastructure Setup
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nTesting infrastructure is foundational for a\
      \ complex multi-service platform like Morpheus. With ML workflows, async comic\
      \ generation, payment processing, and user management across multiple frontends,\
      \ we need comprehensive testing to catch regressions early, ensure API reliability,\
      \ and validate complex user workflows. This prevents costly bugs in production\
      \ and enables confident deployment of new features.\n\n**Technical Approach:**\n\
      - Unit Testing: Vitest for all packages with shared config, fast execution,\
      \ and TypeScript support\n- Integration Testing: Supertest for Fastify API testing\
      \ with test database isolation  \n- E2E Testing: Playwright for cross-browser\
      \ testing of Next.js apps with visual regression detection\n- Database Testing:\
      \ Supabase local instance with migrations + seed data for consistent test state\n\
      - ML Testing: Mock OpenAI/Anthropic responses, test RunPod integration with\
      \ fake endpoints\n- Monorepo Testing: Turborepo task orchestration for parallel\
      \ test execution across workspaces\n\n**Dependencies:**\n- External: vitest,\
      \ @vitest/ui, playwright, supertest, msw, testcontainers\n- Internal: Shared\
      \ test utilities, database factories, auth mocks, API clients\n\n**Risks:**\n\
      - Test database pollution: Use transactions + rollback or fresh DB per test\
      \ suite\n- Flaky E2E tests: Implement proper waits, stable selectors, retry\
      \ logic\n- ML API costs in tests: Mock external services, use rate limiting\
      \ for real API tests\n- Slow test suite: Parallel execution, selective testing,\
      \ proper test isolation\n\n**Complexity Notes:**\nMore complex than typical\
      \ due to ML workflows, multi-frontend coordination, and database state management.\
      \ Need sophisticated mocking for external AI services and payment flows.\n\n\
      **Key Files:**\n- packages/shared/vitest.config.ts: Base Vitest configuration\n\
      - packages/api/tests/: API integration tests with Supertest\n- apps/dashboard/tests/:\
      \ Playwright E2E tests for admin workflows  \n- apps/storefront/tests/: Customer\
      \ journey E2E tests\n- packages/database/test-utils/: DB factories and test\
      \ helpers\n"
    design_decisions:
    - decision: Use Vitest over Jest for all JavaScript/TypeScript testing
      rationale: Native ESM support, faster execution, better TypeScript integration,
        Vite ecosystem alignment
      alternatives_considered:
      - Jest with ts-jest
      - Node.js native test runner
    - decision: Implement test database isolation with Supabase local instance
      rationale: Matches production environment exactly, supports RLS policies, easier
        than mocking complex queries
      alternatives_considered:
      - SQLite for tests
      - Docker PostgreSQL
      - Full database mocking
    - decision: Mock ML services by default, with opt-in real API testing
      rationale: Prevents API costs, ensures test determinism, faster execution while
        still allowing integration validation
      alternatives_considered:
      - Always use real APIs
      - Record/replay HTTP interactions
    researched_at: '2026-02-07T18:50:46.501523'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:27:45.243531'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Establish a three-tier testing strategy: Vitest for fast unit tests
      across all packages, Supertest for Fastify API integration testing against local
      Supabase, and Playwright for full E2E workflows. Create shared test utilities
      for database seeding, auth mocking, and ML service stubbing. Use Turborepo to
      orchestrate parallel test execution and implement CI/CD integration with GitHub
      Actions for automated testing on PRs.

      '
    external_dependencies:
    - name: vitest
      version: ^2.0.0
      reason: Primary test runner with excellent TypeScript and monorepo support
    - name: '@vitest/ui'
      version: ^2.0.0
      reason: Web-based test UI for debugging and development
    - name: playwright
      version: ^1.40.0
      reason: Cross-browser E2E testing with visual regression capabilities
    - name: supertest
      version: ^6.3.0
      reason: HTTP assertion testing for Fastify API endpoints
    - name: msw
      version: ^2.0.0
      reason: Mock Service Worker for intercepting ML API calls in tests
    - name: '@supabase/supabase-js'
      version: ^2.38.0
      reason: Test database client for integration testing
    - name: testcontainers
      version: ^10.0.0
      reason: Docker container management for isolated test environments
    files_to_modify:
    - path: package.json
      changes: Add test scripts and testing dependencies
    - path: turbo.json
      changes: Configure test task orchestration and caching
    - path: .github/workflows/ci.yml
      changes: Add testing steps with matrix strategy
    new_files:
    - path: packages/shared/vitest.config.ts
      purpose: Base Vitest configuration for all packages
    - path: packages/shared/test-utils/index.ts
      purpose: Shared testing utilities and helpers
    - path: packages/database/test-utils/factories.ts
      purpose: Database test data factories
    - path: packages/database/test-utils/setup.ts
      purpose: Test database setup and teardown
    - path: packages/shared/test-utils/mocks/ai-services.ts
      purpose: Mock implementations for OpenAI/Anthropic
    - path: packages/shared/test-utils/mocks/payment.ts
      purpose: Mock Stripe and payment processing
    - path: playwright.config.ts
      purpose: Global Playwright configuration
    - path: apps/dashboard/tests/setup.ts
      purpose: Dashboard-specific test setup
    - path: apps/storefront/tests/setup.ts
      purpose: Storefront-specific test setup
    - path: packages/api/src/__tests__/setup.ts
      purpose: API test setup with database isolation
  acceptance_criteria:
  - criterion: All packages have unit testing configured with Vitest and achieve >80%
      code coverage
    verification: Run `pnpm test:unit` in root - all tests pass and coverage reports
      generated
  - criterion: API integration tests validate all endpoints with database isolation
    verification: Run `pnpm test:integration` - Fastify routes tested against local
      Supabase with transaction rollback
  - criterion: E2E tests cover critical user flows across dashboard and storefront
    verification: Run `pnpm test:e2e` - Playwright tests complete comic creation,
      payment, and admin workflows
  - criterion: ML service integrations are properly mocked with fallback to real API
      testing
    verification: Tests run with MSW mocks by default, optional REAL_API=true flag
      for integration testing
  - criterion: Testing commands integrated into CI/CD with parallel execution
    verification: GitHub Actions runs all test suites in parallel on PR creation with
      proper caching
  testing:
    unit_tests:
    - file: packages/api/src/__tests__/auth.test.ts
      coverage_target: 85%
      scenarios:
      - JWT token validation
      - User session management
      - Permission checks
    - file: packages/comic-generator/src/__tests__/generator.test.ts
      coverage_target: 80%
      scenarios:
      - Prompt processing
      - Image generation workflow
      - Error handling for API failures
    - file: packages/payment/src/__tests__/stripe.test.ts
      coverage_target: 85%
      scenarios:
      - Payment intent creation
      - Webhook processing
      - Refund handling
    integration_tests:
    - file: packages/api/src/__tests__/integration/comics.test.ts
      scenarios:
      - Create comic end-to-end
      - User authentication flow
      - Payment processing integration
    - file: packages/api/src/__tests__/integration/database.test.ts
      scenarios:
      - Database migrations
      - Seed data consistency
      - Transaction isolation
    e2e_tests:
    - file: apps/dashboard/tests/admin-workflows.spec.ts
      scenarios:
      - User management interface
      - Analytics dashboard
      - System configuration
    - file: apps/storefront/tests/customer-journey.spec.ts
      scenarios:
      - Comic creation flow
      - Payment and checkout
      - Profile management
    manual_testing:
    - step: Verify test database isolation by running multiple test suites simultaneously
      expected: No cross-contamination between test runs
    - step: Test CI/CD pipeline with intentional test failures
      expected: Pipeline fails and blocks PR merge
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Install and configure testing dependencies across monorepo
      done: false
    - task: Create shared Vitest configuration and test utilities
      done: false
    - task: Setup database test utilities with factories and isolation
      done: false
    - task: Implement MSW mocks for external AI and payment services
      done: false
    - task: Configure Playwright for E2E testing with proper selectors
      done: false
    - task: Create unit tests for core packages (auth, comic-generator, payment)
      done: false
    - task: Implement API integration tests with Supertest
      done: false
    - task: Build E2E test suites for critical user workflows
      done: false
    - task: Configure Turborepo task orchestration for parallel testing
      done: false
    - task: Setup GitHub Actions CI/CD with testing pipeline
      done: false
    - task: Documentation and team training on testing practices
      done: false
- key: T5
  title: Weights & Biases Integration
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 3
  area: ml
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Weights & Biases (W&B) integration is critical for monitoring and optimizing
      ML operations in Morpheus. Since we''re using OpenAI/Anthropic LLMs and RunPod
      Stable Diffusion for novel-to-comic transformation, we need visibility into
      model performance, token usage, generation quality, and cost optimization. W&B
      provides experiment tracking, model versioning, and performance monitoring that
      will help us iterate on prompts, track comic generation quality metrics, and
      optimize our ML pipeline costs.


      **Technical Approach:**

      - Use official `wandb` SDK for Python/Node.js integration

      - Create a dedicated MLOps service within the backend to centralize W&B operations

      - Implement async logging to avoid blocking comic generation requests

      - Track metrics: token usage, generation time, quality scores, user feedback

      - Use W&B Tables for storing comic panel metadata and generated images

      - Implement experiment tracking for A/B testing different prompts/models

      - Set up automated model performance dashboards


      **Dependencies:**

      - External: wandb SDK, wandb Python API, environment variables for API keys

      - Internal: ML service layer, comic generation pipeline, user feedback system

      - Infrastructure: Environment variable management, logging infrastructure


      **Risks:**

      - API rate limits: implement exponential backoff and batching

      - Data privacy: ensure no sensitive user content is logged inappropriately

      - Performance overhead: async logging and sampling strategies needed

      - Cost management: W&B storage costs can grow with image/artifact logging


      **Complexity Notes:**

      Medium complexity - the W&B SDK is well-documented, but integrating across our
      ML pipeline (LLM + Stable Diffusion) while maintaining performance requires
      careful architecture. The async nature of our comic generation process adds
      complexity to experiment tracking.


      **Key Files:**

      - apps/backend/src/services/mlops.ts: New W&B service wrapper

      - apps/backend/src/services/comic-generation.ts: Add W&B logging calls

      - apps/backend/src/config/wandb.ts: W&B configuration

      - packages/shared/src/types/mlops.ts: Shared ML metrics types

      - docker-compose.yml: Add W&B environment variables

      '
    design_decisions:
    - decision: Use W&B SDK with custom service wrapper pattern
      rationale: Provides abstraction over W&B API, enables testing, and centralizes
        ML monitoring logic
      alternatives_considered:
      - Direct W&B SDK usage
      - Custom analytics solution
      - MLflow
    - decision: Async logging with queue-based approach
      rationale: Prevents ML monitoring from blocking user-facing comic generation
        requests
      alternatives_considered:
      - Synchronous logging
      - Fire-and-forget approach
    - decision: Environment-based experiment organization
      rationale: Separates dev/staging/production experiments while maintaining consistency
      alternatives_considered:
      - Manual project management
      - Git-branch based organization
    researched_at: '2026-02-07T18:51:07.013281'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:28:12.868338'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a dedicated MLOps service that wraps the W&B SDK and integrates
      with our existing comic generation pipeline. Implement async logging queues
      to track LLM token usage, Stable Diffusion generation metrics, and user feedback
      without impacting performance. Use W&B''s experiment tracking to A/B test different
      prompts and models, while leveraging W&B Tables to store comic metadata and
      generated artifacts for analysis.

      '
    external_dependencies:
    - name: wandb
      version: ^0.16.0
      reason: Official Weights & Biases SDK for experiment tracking and ML monitoring
    - name: '@types/node'
      version: ^20.0.0
      reason: TypeScript support for Node.js W&B integration
    - name: bull
      version: ^4.12.0
      reason: Queue system for async ML metrics logging
    files_to_modify:
    - path: apps/backend/src/services/comic-generation.ts
      changes: Add MLOps service integration, metric collection points, experiment
        tracking calls
    - path: apps/backend/src/routes/comics.ts
      changes: Inject experiment tracking for A/B testing, add feedback correlation
        endpoints
    - path: apps/backend/package.json
      changes: Add wandb SDK dependency
    - path: docker-compose.yml
      changes: Add WANDB_API_KEY and WANDB_PROJECT environment variables
    - path: apps/backend/.env.example
      changes: Add W&B configuration variables with documentation
    new_files:
    - path: apps/backend/src/services/mlops.ts
      purpose: Core W&B integration service with async logging queue and experiment
        management
    - path: apps/backend/src/config/wandb.ts
      purpose: W&B configuration, environment validation, and SDK initialization
    - path: packages/shared/src/types/mlops.ts
      purpose: TypeScript interfaces for ML metrics, experiment tracking, and W&B
        data structures
    - path: apps/backend/src/utils/metric-calculator.ts
      purpose: Helper functions for calculating token costs, generation quality scores,
        and performance metrics
    - path: apps/backend/src/middleware/experiment-tracking.ts
      purpose: Express middleware for automatic experiment context injection and A/B
        test assignment
    - path: apps/backend/src/__tests__/fixtures/wandb-mock.ts
      purpose: Mock W&B responses for testing without external API dependency
  acceptance_criteria:
  - criterion: MLOps service successfully tracks LLM token usage, generation time,
      and costs for each comic generation request
    verification: Check W&B dashboard shows metrics for token_count, generation_time_ms,
      estimated_cost per comic generation with proper experiment grouping
  - criterion: Stable Diffusion generation metrics are logged asynchronously without
      impacting comic generation performance
    verification: Run load test generating 10 comics simultaneously - generation time
      should not increase >5% with W&B logging enabled vs disabled
  - criterion: User feedback scores are properly correlated with generation experiments
      in W&B Tables
    verification: Generate test comic, submit feedback score, verify W&B Table entry
      links feedback to correct experiment run with comic metadata
  - criterion: A/B testing framework allows comparing different prompt templates with
      statistical significance tracking
    verification: Create two prompt variants, generate 20 comics with each, verify
      W&B shows separate experiments with comparative metrics
  - criterion: Comic panel images and metadata are stored in W&B Tables without exposing
      sensitive user data
    verification: Check W&B Tables contain comic_id, panel_count, generation_params
      but exclude user_id, email, or personal content
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/mlops.test.ts
      coverage_target: 90%
      scenarios:
      - MLOps service initialization with valid/invalid API keys
      - Async metric logging queue handles high volume
      - Metric sanitization removes sensitive data
      - Error handling when W&B API is unavailable
      - Experiment tracking creates proper run groups
    - file: apps/backend/src/__tests__/services/comic-generation.test.ts
      coverage_target: 85%
      scenarios:
      - Comic generation with W&B logging enabled
      - Fallback behavior when MLOps service fails
      - Proper metric calculation for token usage
    integration_tests:
    - file: apps/backend/src/__tests__/integration/mlops-pipeline.test.ts
      scenarios:
      - End-to-end comic generation with W&B tracking
      - A/B test experiment creation and metric logging
      - User feedback correlation with experiment data
      - Batch metric upload and queue processing
    manual_testing:
    - step: Generate comic with different prompt templates
      expected: W&B dashboard shows separate experiments with distinct metrics
    - step: Submit user feedback for generated comic
      expected: Feedback appears in W&B Table linked to correct experiment run
    - step: Monitor W&B dashboard during high-load comic generation
      expected: Metrics appear consistently without gaps or delays >30 seconds
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Install wandb SDK and configure environment variables
      done: false
    - task: Create MLOps service with async logging queue and W&B SDK integration
      done: false
    - task: Implement metric calculation utilities for LLM and Stable Diffusion tracking
      done: false
    - task: Add experiment tracking middleware for A/B testing framework
      done: false
    - task: Integrate MLOps service into comic generation pipeline with performance
        monitoring
      done: false
    - task: Implement W&B Tables integration for comic metadata and user feedback
        correlation
      done: false
    - task: Create comprehensive unit and integration test suite with W&B mocking
      done: false
    - task: Set up W&B dashboard templates and automated alerts for key metrics
      done: false
    - task: Performance test async logging under high load conditions
      done: false
    - task: Documentation for MLOps service usage and W&B dashboard interpretation
      done: false
- key: T6
  title: CI/CD Workflows Setup
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p0
  effort: 3
  area: setup
  dependsOn:
  - T4
  agent_notes:
    research_findings: "**Context:**\nCI/CD workflows are critical for the Morpheus\
      \ project to ensure code quality, automated testing, and reliable deployments\
      \ across a complex monorepo with multiple services (backend API, dashboard,\
      \ storefront) and ML integrations. This solves the problem of manual testing/deployment\
      \ overhead and provides fast feedback loops for developers working on novel-to-comic\
      \ transformations.\n\n**Technical Approach:**\nGitHub Actions with matrix builds\
      \ for the Turborepo monorepo structure. Implement parallel testing strategies\
      \ for Vitest unit tests and Playwright E2E tests. Use Docker for consistent\
      \ environments and implement staging/production deployment pipelines with proper\
      \ secret management for OpenAI/Anthropic API keys and Supabase credentials.\
      \ Include ML model validation steps and image generation testing.\n\n**Dependencies:**\n\
      - External: GitHub Actions runners, Docker Hub/GHCR, Vercel/Railway for deployments\n\
      - Internal: Package.json scripts in each workspace, Turborepo build system,\
      \ Supabase migrations, RunPod API integration tests\n\n**Risks:**\n- ML API\
      \ rate limits during CI: implement test mocking and dedicated test API keys\
      \ with lower quotas\n- Monorepo build complexity: use Turborepo's remote caching\
      \ and selective builds based on changed packages\n- Secret management sprawl:\
      \ centralize in GitHub secrets with environment-specific organization\n- Long-running\
      \ image generation tests: implement test timeouts and fallback mock responses\n\
      \n**Complexity Notes:**\nMore complex than typical due to ML service dependencies,\
      \ multiple deployment targets (dashboard vs storefront), and need for visual\
      \ regression testing for comic generation outputs. The monorepo structure adds\
      \ coordination complexity but Turborepo helps significantly.\n\n**Key Files:**\n\
      - .github/workflows/ci.yml: main CI pipeline with matrix strategy\n- .github/workflows/deploy-staging.yml:\
      \ staging deployment automation  \n- .github/workflows/deploy-prod.yml: production\
      \ deployment with manual approval\n- turbo.json: build and test pipeline configuration\n\
      - docker-compose.test.yml: testing environment setup\n"
    design_decisions:
    - decision: GitHub Actions with Turborepo remote caching
      rationale: Native GitHub integration, excellent monorepo support, cost-effective
        for open source, remote caching reduces build times
      alternatives_considered:
      - Jenkins (too much maintenance overhead)
      - GitLab CI (vendor lock-in concerns)
      - CircleCI (cost for private repos)
    - decision: Matrix builds for Node.js versions and test environments
      rationale: Ensures compatibility across Node 18/20, parallel execution for faster
        feedback, separate staging/prod validation
      alternatives_considered:
      - Single Node version (risky for compatibility)
      - Sequential builds (too slow for monorepo)
    - decision: Separate workflows for CI vs CD with environment protection
      rationale: Production deployments need manual approval, staging can be automatic,
        allows for different secret scopes
      alternatives_considered:
      - Single workflow with conditional steps (harder to manage)
      - Fully manual deployments (defeats automation purpose)
    researched_at: '2026-02-07T18:51:28.351479'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:28:38.748416'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement GitHub Actions workflows leveraging Turborepo''s selective
      builds and remote caching. Create separate CI pipeline for testing/linting and
      CD pipelines for staging/production deployments. Use Docker containers for consistent
      test environments and implement proper secret management for ML API keys. Include
      visual regression testing for comic generation outputs and database migration
      validation.

      '
    external_dependencies:
    - name: '@vercel/ncc'
      version: ^0.38.0
      reason: Bundle GitHub Actions for faster execution
    - name: docker/build-push-action
      version: v5
      reason: Build and push Docker images in CI pipeline
    - name: actions/cache
      version: v3
      reason: Cache node_modules and Turborepo builds for faster CI
    - name: peaceiris/actions-gh-pages
      version: v3
      reason: Deploy Storybook and documentation to GitHub Pages
    files_to_modify:
    - path: turbo.json
      changes: Add CI-specific pipeline configurations with selective builds and caching
        strategies
    - path: package.json
      changes: Add CI scripts for testing, linting, and build validation across workspaces
    - path: apps/backend/package.json
      changes: Add test:ci script with proper environment setup and ML API mocking
    - path: apps/dashboard/package.json
      changes: Add build:ci and test:e2e scripts with Playwright configuration
    new_files:
    - path: .github/workflows/ci.yml
      purpose: Main CI pipeline with matrix builds, testing, and linting for all packages
    - path: .github/workflows/deploy-staging.yml
      purpose: Automated staging deployment triggered on main branch changes
    - path: .github/workflows/deploy-prod.yml
      purpose: Production deployment with manual approval gates and rollback capability
    - path: .github/workflows/visual-regression.yml
      purpose: Visual testing pipeline for comic generation output validation
    - path: docker-compose.test.yml
      purpose: Testing environment with Supabase, Redis, and ML API mocks
    - path: .github/scripts/setup-test-env.sh
      purpose: Environment setup script for CI runners with dependency installation
    - path: packages/shared/src/config/ci.ts
      purpose: CI-specific configuration and environment variable management
    - path: apps/backend/src/__tests__/setup/ml-mocks.ts
      purpose: Mock implementations for OpenAI/Anthropic APIs during testing
    - path: .env.ci.example
      purpose: Template for CI environment variables with safe defaults
  acceptance_criteria:
  - criterion: CI pipeline successfully runs tests and linting for all affected packages
      when code is pushed to any branch
    verification: Push code changes and verify GitHub Actions runs Turborepo selective
      builds with test results in PR checks
  - criterion: Staging deployment automatically triggers on main branch merge and
      successfully deploys all services
    verification: Merge PR to main and verify staging URLs are accessible with latest
      changes within 10 minutes
  - criterion: Production deployment requires manual approval and completes successfully
      with zero downtime
    verification: Trigger production workflow, approve deployment, verify services
      remain accessible during deployment
  - criterion: ML API integration tests run with proper mocking and fallback handling
      during CI
    verification: CI logs show ML tests passing with mock responses and timeout handling
      under 30 seconds per test
  - criterion: Visual regression tests validate comic generation output consistency
    verification: Percy or similar tool shows no unexpected visual changes in comic
      generation pipeline
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/workflows/ci.test.ts
      coverage_target: 90%
      scenarios:
      - Turborepo selective build configuration
      - Environment variable validation
      - ML API mock responses
    - file: packages/shared/src/__tests__/config.test.ts
      coverage_target: 85%
      scenarios:
      - CI environment detection
      - Secret management validation
      - Build configuration parsing
    integration_tests:
    - file: apps/backend/src/__tests__/integration/ml-pipeline.test.ts
      scenarios:
      - End-to-end comic generation with mocked APIs
      - Database migration validation
      - File upload and processing workflow
    - file: apps/dashboard/src/__tests__/e2e/deployment.spec.ts
      scenarios:
      - Staging deployment health checks
      - Authentication flow with test credentials
      - Cross-service communication validation
    manual_testing:
    - step: Trigger CI workflow on feature branch with backend and frontend changes
      expected: Only affected packages build and test, completing in under 5 minutes
    - step: Deploy to staging and verify all services are accessible
      expected: Dashboard, storefront, and API respond correctly with updated features
    - step: Test production deployment approval workflow
      expected: Deployment waits for approval and completes successfully when approved
  estimates:
    development: 4
    code_review: 1
    testing: 2
    documentation: 1
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Research and plan GitHub Actions workflow structure for monorepo
      done: false
    - task: Configure Turborepo selective builds and remote caching for CI
      done: false
    - task: Set up Docker test environment with service dependencies
      done: false
    - task: Implement ML API mocking strategy with rate limit handling
      done: false
    - task: Create CI workflow with matrix builds for parallel testing
      done: false
    - task: Configure staging deployment pipeline with health checks
      done: false
    - task: Set up production deployment with approval gates and monitoring
      done: false
    - task: Implement visual regression testing for comic generation
      done: false
    - task: Configure GitHub secrets and environment-specific variables
      done: false
    - task: Test end-to-end CI/CD flow with feature branch and deployment
      done: false
    - task: Document CI/CD processes and troubleshooting guide
      done: false
- key: T7
  title: Mock Mode for External Services
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Mock mode for external services is critical for local development and testing
      in Morpheus, which integrates with multiple costly/rate-limited external APIs
      (OpenAI/Anthropic LLMs, RunPod Stable Diffusion, Supabase, payment processors).
      Without mocking, developers face: slow feedback loops, API costs during development,
      rate limiting, network dependencies, and inability to test error scenarios.
      This enables offline development, faster CI/CD, predictable testing, and cost
      control.


      **Technical Approach:**

      Implement a hierarchical mocking system with environment-based configuration.
      Use MSW (Mock Service Worker) for HTTP API mocking, create mock implementations
      for SDK-based services, and build a unified service factory pattern that switches
      between real and mock implementations based on NODE_ENV and feature flags. Include
      realistic response delays, error simulation, and state persistence for complex
      workflows like comic generation pipelines.


      **Dependencies:**

      - External: msw ^2.0.0, faker-js ^8.0, uuid ^9.0, node-fetch-commonjs

      - Internal: Environment config service, logging service, shared types/schemas


      **Risks:**

      - Mock drift: Real APIs change but mocks don''t get updated. Mitigation: Regular
      integration tests against real APIs in staging

      - Incomplete error coverage: Missing edge cases in mocks. Mitigation: Comprehensive
      error scenario mapping from API docs

      - Performance assumptions: Mock responses too fast/slow vs reality. Mitigation:
      Configurable realistic delays based on API benchmarks

      - State management complexity: Stateful mocks becoming too complex. Mitigation:
      Keep mocks stateless where possible, use simple in-memory stores


      **Complexity Notes:**

      Medium complexity. While individual service mocks are straightforward, coordinating
      stateful interactions (e.g., novel text → scene extraction → image generation
      → comic assembly) requires careful sequencing. The biggest challenge is maintaining
      realistic data relationships across the multi-step comic generation pipeline.


      **Key Files:**

      - packages/backend/src/services/mock/: Mock service implementations directory

      - packages/backend/src/config/environment.ts: Add mock mode configuration

      - packages/backend/src/services/factory.ts: Service factory for mock/real switching

      - packages/backend/src/test/mocks/: Test-specific mock data and handlers

      - packages/shared/types/mocks.ts: Mock configuration types

      '
    design_decisions:
    - decision: Use MSW for HTTP API mocking combined with factory pattern for SDK
        services
      rationale: MSW provides realistic network-level mocking for REST APIs while
        factory pattern allows clean switching between mock/real implementations for
        SDK-based services like OpenAI
      alternatives_considered:
      - Nock for HTTP mocking
      - Manual fetch override
      - Docker containers with mock services
    - decision: Environment-based mock activation with granular service-level overrides
      rationale: Allows flexible testing scenarios (mock AI but use real database)
        and gradual development workflow
      alternatives_considered:
      - All-or-nothing mock mode
      - Runtime mock toggling
      - Test-only mocking
    researched_at: '2026-02-07T18:51:52.651483'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:29:06.119112'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a service factory pattern that instantiates either real or mock
      implementations based on environment configuration. Use MSW for REST API mocking
      (Supabase, payment APIs) and create mock classes implementing the same interfaces
      as real services for SDK-based integrations (OpenAI, RunPod). Build a mock data
      generator using Faker.js for realistic novel text, character descriptions, and
      comic metadata. Implement configurable delays and error injection to simulate
      real-world API behavior and failure scenarios.

      '
    external_dependencies:
    - name: msw
      version: ^2.0.0
      reason: Network-level HTTP API mocking for realistic testing
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate realistic mock data for novels, characters, and comic content
    - name: uuid
      version: ^9.0.0
      reason: Generate consistent mock IDs for entities across requests
    - name: canvas
      version: ^2.11.0
      reason: Generate mock comic panel images in Node.js environment
    files_to_modify:
    - path: packages/backend/src/config/environment.ts
      changes: Add MOCK_MODE, API_ERROR_RATE, MOCK_DELAY_MS environment variables
    - path: packages/backend/src/services/llm/index.ts
      changes: Update service initialization to use factory pattern
    - path: packages/backend/src/services/image-generation/index.ts
      changes: Update service initialization to use factory pattern
    - path: packages/backend/src/app.ts
      changes: Initialize mock service worker if in mock mode
    new_files:
    - path: packages/backend/src/services/mock/factory.ts
      purpose: Service factory that returns mock or real implementations based on
        config
    - path: packages/backend/src/services/mock/llm-mock.ts
      purpose: Mock implementation of LLM service with realistic text generation
    - path: packages/backend/src/services/mock/image-mock.ts
      purpose: Mock implementation of image generation service with placeholder images
    - path: packages/backend/src/services/mock/database-mock.ts
      purpose: Mock Supabase implementation with in-memory storage
    - path: packages/backend/src/services/mock/payment-mock.ts
      purpose: Mock payment processor with simulated success/failure scenarios
    - path: packages/backend/src/services/mock/data-generators.ts
      purpose: Faker.js-based generators for realistic mock data (novels, characters,
        etc.)
    - path: packages/backend/src/services/mock/msw-handlers.ts
      purpose: MSW request handlers for external HTTP APIs
    - path: packages/shared/types/mocks.ts
      purpose: TypeScript interfaces for mock configuration and responses
    - path: packages/backend/src/middleware/mock-setup.ts
      purpose: Express middleware to initialize MSW in development mode
  acceptance_criteria:
  - criterion: Mock mode is activated when NODE_ENV=development or MOCK_MODE=true,
      with all external service calls intercepted
    verification: Run `npm run dev` with MOCK_MODE=true and verify logs show 'Mock
      service factory initialized' and no real API calls in network tab
  - criterion: Comic generation pipeline works end-to-end in mock mode with realistic
      delays (2-5s per step)
    verification: POST /api/comics/generate in mock mode completes successfully within
      15 seconds with valid comic structure
  - criterion: Error scenarios can be triggered via mock configuration flags (API_ERROR_RATE=0.3
      causes 30% failure rate)
    verification: Set API_ERROR_RATE=1.0 and verify comic generation returns appropriate
      error responses with 500 status codes
  - criterion: Mock data maintains consistency across related entities (characters,
      scenes, generated images)
    verification: Generate multiple comics and verify character names/descriptions
      remain consistent within each comic's scenes
  - criterion: Real API integration tests pass in staging environment to prevent mock
      drift
    verification: Run `npm run test:integration:staging` with real APIs and verify
      all external service tests pass
  testing:
    unit_tests:
    - file: packages/backend/src/services/mock/__tests__/mock-factory.test.ts
      coverage_target: 90%
      scenarios:
      - Service factory returns mock implementations when MOCK_MODE=true
      - Service factory returns real implementations when MOCK_MODE=false
      - Mock configuration validation and error handling
    - file: packages/backend/src/services/mock/__tests__/llm-mock.test.ts
      coverage_target: 85%
      scenarios:
      - Mock LLM generates valid novel text with proper structure
      - Mock scene extraction returns consistent character data
      - Error injection works with configurable failure rates
    - file: packages/backend/src/services/mock/__tests__/image-mock.test.ts
      coverage_target: 85%
      scenarios:
      - Mock image generation returns valid image URLs
      - Realistic delay simulation (2-4 seconds)
      - Batch image generation maintains order
    integration_tests:
    - file: packages/backend/src/__tests__/integration/comic-generation-mock.test.ts
      scenarios:
      - End-to-end comic generation pipeline in mock mode
      - State persistence across multi-step generation process
      - Error recovery and partial completion scenarios
    - file: packages/backend/src/__tests__/integration/api-parity.test.ts
      scenarios:
      - Mock responses match real API response schemas
      - Mock and real service interfaces are identical
    manual_testing:
    - step: Start development server with MOCK_MODE=true
      expected: Server logs show mock services initialized, no external network calls
    - step: Generate a complete comic through the UI
      expected: Comic generation completes with realistic timing, images display properly
    - step: Test error scenarios with API_ERROR_RATE=0.5
      expected: Some requests fail gracefully with proper error messages in UI
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup MSW and Faker.js dependencies, create mock service directory structure
      done: false
    - task: Implement service factory pattern and environment configuration
      done: false
    - task: Create mock LLM service with novel generation and scene extraction
      done: false
    - task: Implement mock image generation service with placeholder images and delays
      done: false
    - task: Build mock database service with in-memory state management
      done: false
    - task: Create MSW handlers for external HTTP APIs (Supabase, payments)
      done: false
    - task: Add error injection and configurable failure scenarios
      done: false
    - task: Implement end-to-end comic generation pipeline testing
      done: false
    - task: Add mock mode documentation and developer setup guide
      done: false
    - task: Code review and integration testing with real APIs in staging
      done: false
- key: T8
  title: Project Timeline & Cost Estimates
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 2
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Project timeline and cost estimation is critical for M0 Infrastructure & Setup
      milestone to establish realistic expectations, secure appropriate resources,
      and create accountability benchmarks. This foundational task enables stakeholder
      alignment, resource planning, and risk management before development begins.
      Without proper estimation, the Morpheus project risks scope creep, budget overruns,
      and missed deadlines across its complex multi-service architecture.


      **Technical Approach:**

      - Use story point estimation with planning poker for development tasks

      - Implement Gantt charts and dependency mapping for timeline visualization

      - Create cost models for infrastructure (Supabase, RunPod GPU instances, OpenAI/Anthropic
      API usage)

      - Establish burndown tracking and velocity measurements

      - Use Monte Carlo simulation for risk-adjusted timeline estimates

      - Integrate with project management tools (Linear, Notion, or GitHub Projects)

      - Create automated cost tracking for cloud services and ML API consumption


      **Dependencies:**

      - External: [@linear/sdk, @octokit/rest, chart.js, date-fns, zod for validation]

      - Internal: All other M0 tasks need estimation, team velocity baselines, infrastructure
      cost models


      **Risks:**

      - Underestimating ML pipeline complexity: Add 40% buffer for AI/ML tasks

      - GPU cost volatility on RunPod: Implement cost monitoring and alerts

      - Third-party API rate limits affecting timeline: Plan for quota management

      - Team velocity assumptions being wrong: Use conservative estimates initially

      - Scope creep during development: Establish change request process


      **Complexity Notes:**

      More complex than initially apparent due to the multi-domain nature (web dev
      + ML + infrastructure). Requires domain expertise in cost modeling for GPU compute,
      API usage patterns, and database scaling. The interconnected nature of services
      makes dependency mapping critical.


      **Key Files:**

      - docs/project-plan.md: Master timeline and milestone breakdown

      - docs/cost-estimates.md: Infrastructure and operational cost models

      - packages/shared/src/types/project.ts: Timeline and estimation type definitions

      - tools/cost-tracker/: Automated cost monitoring utilities

      - .github/workflows/cost-report.yml: Weekly cost reporting automation

      '
    design_decisions:
    - decision: Use hybrid estimation approach combining story points for development
        and time-based estimates for infrastructure setup
      rationale: Development velocity varies but infrastructure tasks are more predictable;
        hybrid approach provides better accuracy
      alternatives_considered:
      - Pure story point estimation
      - Time-based estimation only
      - T-shirt sizing
    - decision: Implement automated cost tracking with alerts rather than manual reporting
      rationale: ML API costs and GPU usage can spike unexpectedly; automated monitoring
        prevents budget overruns
      alternatives_considered:
      - Manual weekly reports
      - Monthly budget reviews
      - Quarterly cost analysis
    - decision: Create Monte Carlo simulation for timeline risk assessment
      rationale: Complex multi-service architecture has high uncertainty; probabilistic
        estimates better than point estimates
      alternatives_considered:
      - PERT estimation
      - Buffer-based planning
      - Agile velocity forecasting
    researched_at: '2026-02-07T18:52:15.320554'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:29:28.757112'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a comprehensive project planning system that combines development
      estimation (story points) with infrastructure cost modeling and risk assessment.
      Build automated cost tracking for Supabase, RunPod, and ML APIs with alert thresholds.
      Use Monte Carlo simulation for timeline risk analysis and maintain real-time
      project dashboards. Integrate with existing project management tools and establish
      weekly reporting cadence.

      '
    external_dependencies:
    - name: chart.js
      version: ^4.4.0
      reason: Timeline visualization and burndown charts
    - name: date-fns
      version: ^3.0.0
      reason: Date manipulation for timeline calculations
    - name: '@linear/sdk'
      version: ^1.22.0
      reason: Integration with Linear for task tracking and velocity data
    - name: zod
      version: ^3.22.0
      reason: Validation for cost estimates and timeline data structures
    - name: simple-statistics
      version: ^7.8.3
      reason: Monte Carlo simulation and statistical analysis for risk assessment
    files_to_modify:
    - path: package.json
      changes: Add cost-tracker workspace, planning dependencies (@linear/sdk, chart.js,
        date-fns)
    - path: .github/workflows/cost-report.yml
      changes: Create weekly cost reporting workflow with Slack notifications
    - path: packages/shared/src/types/index.ts
      changes: Export project planning types
    new_files:
    - path: docs/project-plan.md
      purpose: Master project timeline with Gantt charts, milestones, and dependency
        mapping
    - path: docs/cost-estimates.md
      purpose: Infrastructure cost models, scaling projections, and budget allocations
    - path: packages/shared/src/types/project.ts
      purpose: TypeScript definitions for timeline, cost, and estimation data structures
    - path: tools/cost-tracker/package.json
      purpose: Cost tracking utility package configuration
    - path: tools/cost-tracker/src/cost-calculator.ts
      purpose: Core cost calculation logic for Supabase, RunPod, ML APIs
    - path: tools/cost-tracker/src/timeline-estimator.ts
      purpose: Story point estimation, Monte Carlo simulation, velocity tracking
    - path: tools/cost-tracker/src/integrations/supabase-costs.ts
      purpose: Supabase billing API integration for usage tracking
    - path: tools/cost-tracker/src/integrations/runpod-costs.ts
      purpose: RunPod API integration for GPU instance cost tracking
    - path: tools/cost-tracker/src/integrations/github-timeline.ts
      purpose: GitHub Issues/Projects API integration for timeline tracking
    - path: tools/cost-tracker/src/reports/dashboard.html
      purpose: Interactive dashboard for project metrics and cost visualization
    - path: tools/cost-tracker/src/cli.ts
      purpose: Command-line interface for manual cost reports and estimates
  acceptance_criteria:
  - criterion: Complete project timeline with story point estimates for all M0 tasks
    verification: docs/project-plan.md contains Gantt chart, dependency mapping, and
      story points totaling 100-150 points for M0
  - criterion: Infrastructure cost model with monthly projections
    verification: docs/cost-estimates.md shows breakdown of Supabase ($20-50/mo),
      RunPod GPU ($200-800/mo), ML APIs ($100-300/mo) with scaling scenarios
  - criterion: Automated cost tracking system operational
    verification: tools/cost-tracker/ generates weekly reports and sends alerts when
      costs exceed 20% of budget
  - criterion: Monte Carlo timeline simulation with risk buffers
    verification: Timeline shows P50/P90 completion dates with 40% ML task buffers
      and dependency critical path analysis
  - criterion: Project dashboard with real-time metrics
    verification: Dashboard displays current sprint velocity, burndown, cost trends,
      and milestone progress with weekly updates
  testing:
    unit_tests:
    - file: tools/cost-tracker/src/__tests__/cost-calculator.test.ts
      coverage_target: 90%
      scenarios:
      - GPU usage cost calculation
      - API usage projection
      - Cost alert threshold triggers
      - Invalid cost data handling
    - file: tools/cost-tracker/src/__tests__/timeline-estimator.test.ts
      coverage_target: 85%
      scenarios:
      - Story point to hours conversion
      - Monte Carlo simulation accuracy
      - Dependency chain calculation
    integration_tests:
    - file: tools/cost-tracker/src/__tests__/integration/reporting.test.ts
      scenarios:
      - End-to-end cost data collection from Supabase API
      - GitHub Issues API integration for timeline tracking
      - Automated report generation pipeline
    manual_testing:
    - step: Run cost tracker for 1 week with mock data
      expected: Daily cost updates, weekly report generated, alerts triggered at thresholds
    - step: Validate timeline estimates against actual task completion
      expected: Estimates within 25% of actual for completed tasks
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Research and document M0 task breakdown with story point estimates
      done: false
    - task: Create infrastructure cost models for Supabase, RunPod, ML APIs
      done: false
    - task: Implement cost-tracker utility with API integrations
      done: false
    - task: Build Monte Carlo timeline simulation with dependency mapping
      done: false
    - task: Create automated reporting dashboard with Chart.js visualizations
      done: false
    - task: Set up GitHub workflow for weekly cost reports and alerts
      done: false
    - task: Document project planning methodology and update procedures
      done: false
    - task: Validate estimates against initial task completions
      done: false
    - task: Create project dashboard and establish review cadence
      done: false
- key: T9
  title: Storybook 8 Setup for Component Development
  type: Task
  milestone: M0 - Infrastructure & Setup
  iteration: I1
  priority: p1
  effort: 3
  area: setup
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Storybook 8 setup is crucial for the Morpheus project to enable isolated component
      development, visual testing, and design system documentation across multiple
      frontend apps (Dashboard + Storefront). Given the comic transformation UI complexity,
      components need thorough visual testing and documentation. Storybook serves
      as a living style guide for the design system while enabling faster development
      cycles through component isolation.


      **Technical Approach:**

      - Setup Storybook 8 as a separate workspace in the Turborepo monorepo

      - Configure to consume components from both Dashboard and Storefront packages

      - Integrate with Tailwind CSS for consistent styling

      - Setup MSW (Mock Service Worker) for API mocking in stories

      - Configure Chromatic for visual regression testing

      - Use TypeScript throughout with proper type safety

      - Implement automatic story generation for common component patterns

      - Setup addon ecosystem: docs, controls, viewport, a11y


      **Dependencies:**

      - External: @storybook/nextjs, @storybook/addon-essentials, chromatic, msw-storybook-addon,
      storybook-addon-designs

      - Internal: shared UI components from dashboard/storefront packages, design
      tokens, Tailwind config


      **Risks:**

      - Build performance: Storybook builds can be slow with large component libraries
      - use webpack optimization and selective story loading

      - Version conflicts: Next.js 16 + Storybook compatibility issues - pin compatible
      versions and use webpack aliases

      - Monorepo complexity: Component resolution across workspaces - configure proper
      tsconfig paths and webpack resolve

      - Maintenance overhead: Stories becoming stale - implement story coverage reports
      and CI checks


      **Complexity Notes:**

      Moderate complexity due to monorepo setup and Next.js 16 integration. The multi-app
      component sharing adds complexity but provides significant value for design
      consistency across Dashboard/Storefront.


      **Key Files:**

      - packages/storybook/.storybook/main.ts: Core Storybook configuration

      - packages/storybook/.storybook/preview.ts: Global decorators and parameters

      - packages/storybook/package.json: Storybook dependencies and scripts

      - turbo.json: Add storybook build/dev tasks

      - packages/storybook/stories/: Component stories organization

      '
    design_decisions:
    - decision: Use @storybook/nextjs framework adapter
      rationale: Provides best integration with Next.js 16, handles webpack config,
        image optimization, and routing automatically
      alternatives_considered:
      - '@storybook/react-webpack5'
      - Custom webpack config
      - '@storybook/vite'
    - decision: Separate Storybook workspace in monorepo
      rationale: Allows consuming components from multiple apps, centralizes design
        system documentation, independent deployment
      alternatives_considered:
      - Storybook in each app
      - Single shared stories folder
      - External repository
    - decision: MSW integration for API mocking
      rationale: Enables realistic component testing with mocked ML/transformation
        APIs, consistent with likely testing strategy
      alternatives_considered:
      - Static mock data
      - Custom fetch mocking
      - No API integration
    researched_at: '2026-02-07T18:52:37.685342'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:29:52.753679'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a dedicated Storybook workspace that imports components from
      both Dashboard and Storefront packages. Configure Next.js framework adapter
      with proper TypeScript support and Tailwind CSS integration. Implement MSW for
      mocking Morpheus API endpoints (novel upload, comic generation) within stories.
      Setup automated visual testing pipeline with Chromatic and integrate with existing
      CI/CD workflows.

      '
    external_dependencies:
    - name: '@storybook/nextjs'
      version: ^8.0.0
      reason: Next.js 16 framework integration with webpack and routing support
    - name: '@storybook/addon-essentials'
      version: ^8.0.0
      reason: Core addons bundle (docs, controls, actions, viewport)
    - name: chromatic
      version: ^10.0.0
      reason: Visual regression testing and review workflows
    - name: msw
      version: ^2.0.0
      reason: API mocking for realistic component stories
    - name: '@storybook/addon-a11y'
      version: ^8.0.0
      reason: Accessibility testing for comic reader components
    files_to_modify:
    - path: turbo.json
      changes: Add storybook workspace with build/dev pipeline tasks
    - path: package.json
      changes: Add storybook workspace to workspaces array
    - path: apps/dashboard/package.json
      changes: Update exports field to expose components for Storybook consumption
    - path: apps/storefront/package.json
      changes: Update exports field to expose components for Storybook consumption
    new_files:
    - path: packages/storybook/package.json
      purpose: Storybook workspace package configuration with Next.js 16 compatible
        versions
    - path: packages/storybook/.storybook/main.ts
      purpose: Core Storybook configuration with Next.js adapter and monorepo component
        resolution
    - path: packages/storybook/.storybook/preview.ts
      purpose: Global decorators for Tailwind CSS, MSW, theme providers
    - path: packages/storybook/.storybook/middleware.js
      purpose: MSW service worker setup for API mocking
    - path: packages/storybook/src/mocks/handlers.ts
      purpose: MSW request handlers for Morpheus API endpoints
    - path: packages/storybook/src/mocks/data.ts
      purpose: Mock data factories for novels, comics, user profiles
    - path: packages/storybook/stories/dashboard/components.stories.ts
      purpose: Dashboard component stories with controls and docs
    - path: packages/storybook/stories/storefront/components.stories.ts
      purpose: Storefront component stories with different variants
    - path: packages/storybook/stories/shared/design-system.stories.ts
      purpose: Design tokens, colors, typography documentation stories
    - path: packages/storybook/.chromatic.json
      purpose: Chromatic configuration for visual regression testing
    - path: packages/storybook/tsconfig.json
      purpose: TypeScript configuration with path mapping to monorepo packages
  acceptance_criteria:
  - criterion: Storybook 8 workspace runs successfully in monorepo with components
      from Dashboard and Storefront packages
    verification: Run `cd packages/storybook && npm run storybook` - opens on localhost:6006
      with component stories visible
  - criterion: Tailwind CSS styling renders correctly in Storybook stories matching
      production apps
    verification: Visual comparison between story components and production components
      shows identical styling
  - criterion: MSW integration provides working API mocks for Morpheus endpoints in
      stories
    verification: Stories with API calls (novel upload, comic generation) work without
      real backend - check Network tab shows mocked responses
  - criterion: TypeScript compilation passes with proper type safety across monorepo
      packages
    verification: Run `turbo build` includes storybook build without TypeScript errors
  - criterion: Chromatic visual regression testing pipeline configured and functional
    verification: Git push triggers Chromatic build - check Chromatic dashboard shows
      visual diff detection
  testing:
    unit_tests:
    - file: packages/storybook/src/__tests__/story-utils.test.ts
      coverage_target: 90%
      scenarios:
      - Story generation utilities
      - Mock data factories
      - Component wrapper helpers
    integration_tests:
    - file: packages/storybook/src/__tests__/integration/component-loading.test.ts
      scenarios:
      - Components load from Dashboard package
      - Components load from Storefront package
      - Shared design tokens resolve correctly
      - MSW handlers intercept API calls
    manual_testing:
    - step: Open Storybook and navigate between Dashboard and Storefront component
        stories
      expected: All stories render without errors, controls work, docs pages display
    - step: Test responsive viewport addon with comic panel components
      expected: Components adapt correctly to different screen sizes
    - step: Verify accessibility addon reports for form components
      expected: A11y violations detected and displayed in addon panel
    - step: Test story with comic generation mock API call
      expected: Loading states and success/error states render with mocked data
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Create Storybook workspace package.json with Next.js 16 compatible dependencies
      done: false
    - task: Configure Storybook main.ts with Next.js adapter and monorepo webpack
        resolve
      done: false
    - task: Setup Tailwind CSS integration in preview.ts with shared design tokens
      done: false
    - task: Implement MSW integration with Morpheus API endpoint mocks
      done: false
    - task: Configure TypeScript with proper path mapping to Dashboard/Storefront
        packages
      done: false
    - task: Create initial component stories for key Dashboard and Storefront components
      done: false
    - task: Setup Chromatic account and configure visual regression testing pipeline
      done: false
    - task: Integrate Storybook build/dev tasks into Turbo pipeline
      done: false
    - task: Add accessibility addon and configure a11y testing standards
      done: false
    - task: Create documentation stories for design system tokens and guidelines
      done: false
- key: T23
  title: Backend TypeScript Port
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 8
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task involves porting the existing backend from JavaScript to TypeScript
      to improve type safety, developer experience, and maintainability. Given Morpheus
      handles complex ML workflows, file uploads, and integrates with multiple external
      services (OpenAI/Anthropic, RunPod, Supabase), strong typing will prevent runtime
      errors and improve API contract enforcement. This is critical for M1 as all
      subsequent backend development will build on this foundation.


      **Technical Approach:**

      - Implement strict TypeScript configuration with Fastify 5''s native TypeScript
      support

      - Use Zod for runtime validation and type inference for API schemas

      - Implement proper typing for Supabase client with generated types

      - Create typed interfaces for ML service integrations (OpenAI/Anthropic responses,
      RunPod API)

      - Use Fastify''s type providers for end-to-end type safety

      - Implement proper error handling with typed error responses

      - Set up path mapping for clean imports and module resolution


      **Dependencies:**

      - External: [@fastify/type-provider-typebox, zod, @types/node, tsx, supabase
      (with generated types)]

      - Internal: Database schema definitions, API route structure, authentication
      middleware


      **Risks:**

      - Type complexity explosion: Start with interfaces, gradually add generics.
      Use utility types sparingly.

      - Migration breaking existing functionality: Implement incrementally with comprehensive
      test coverage using Vitest

      - Performance overhead from TypeScript compilation: Use SWC or esbuild for faster
      builds in development

      - Supabase type generation issues: Pin Supabase CLI version and use generated
      types as base with manual overrides


      **Complexity Notes:**

      More complex than initial estimate due to ML service integrations requiring
      complex nested types for prompts, responses, and image generation parameters.
      The async nature of RunPod jobs will need careful typing for state management
      and webhook handling.


      **Key Files:**

      - src/server.ts: Main Fastify server setup with TypeScript configuration

      - src/types/: Global type definitions for ML services, database models

      - src/routes/: Convert all route handlers to TypeScript with proper typing

      - src/lib/supabase.ts: Typed Supabase client configuration

      - src/lib/ml-services.ts: Typed interfaces for OpenAI/Anthropic/RunPod

      - tsconfig.json: Strict TypeScript configuration

      - package.json: Add TypeScript build scripts

      '
    design_decisions:
    - decision: Use Zod for schema validation with TypeScript inference
      rationale: Provides runtime validation and compile-time types from single source
        of truth, essential for ML API integrations where payload structure is critical
      alternatives_considered:
      - Joi with separate TypeScript interfaces
      - Pure TypeScript without runtime validation
      - Fastify's built-in validation
    - decision: Generate Supabase types from database schema
      rationale: Ensures backend types stay in sync with database schema changes,
        critical for novel processing workflow data integrity
      alternatives_considered:
      - Manual type definitions
      - Generic database interfaces
      - ORM with built-in typing
    - decision: Implement Fastify type providers for end-to-end safety
      rationale: Ensures request/response types are enforced from route definition
        to handler implementation, preventing API contract violations
      alternatives_considered:
      - Manual typing per route
      - Shared interfaces without enforcement
      - OpenAPI schema generation
    researched_at: '2026-02-07T18:53:02.508318'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:30:15.017721'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Start by setting up strict TypeScript configuration and converting
      core server setup. Implement Zod schemas for all API endpoints with type inference
      for request/response validation. Generate Supabase types from database schema
      and create typed client wrapper. Convert ML service integrations with proper
      typing for complex nested responses. Finally, implement Fastify type providers
      for end-to-end type safety across all routes. Use incremental migration approach
      to maintain functionality throughout the port.

      '
    external_dependencies:
    - name: zod
      version: ^3.22.4
      reason: Runtime validation with TypeScript inference for API schemas and ML
        service payloads
    - name: '@fastify/type-provider-typebox'
      version: ^4.0.0
      reason: Fastify's recommended type provider for end-to-end type safety
    - name: tsx
      version: ^4.7.0
      reason: Fast TypeScript execution for development and testing
    - name: supabase
      version: ^2.38.0
      reason: Latest Supabase client with improved TypeScript support and type generation
    - name: '@types/node'
      version: ^20.10.0
      reason: Node.js type definitions compatible with current LTS
    files_to_modify:
    - path: apps/backend/package.json
      changes: Add TypeScript dependencies, update build scripts to use tsx/tsc
    - path: apps/backend/src/server.js
      changes: Rename to server.ts, add strict typing for Fastify instance and plugins
    - path: apps/backend/src/routes/*.js
      changes: Convert all route files to TypeScript with Zod schemas and type providers
    - path: apps/backend/src/lib/supabase.js
      changes: Convert to TypeScript with generated database types and typed client
        wrapper
    - path: apps/backend/src/middleware/*.js
      changes: Add TypeScript typing for auth middleware and request context
    new_files:
    - path: apps/backend/tsconfig.json
      purpose: Strict TypeScript configuration with path mapping and modern target
    - path: apps/backend/src/types/index.ts
      purpose: Global type definitions and re-exports
    - path: apps/backend/src/types/ml-services.ts
      purpose: Typed interfaces for OpenAI, Anthropic, and RunPod API contracts
    - path: apps/backend/src/types/database.ts
      purpose: Generated Supabase types and custom database interfaces
    - path: apps/backend/src/schemas/api.ts
      purpose: Zod schemas for all API endpoints with type inference
    - path: apps/backend/src/lib/validation.ts
      purpose: Utility functions for runtime validation and error handling
    - path: apps/backend/src/lib/type-guards.ts
      purpose: Type guard functions for runtime type checking
  acceptance_criteria:
  - criterion: All TypeScript compilation passes with strict mode enabled and zero
      errors
    verification: Run `npm run type-check` in backend directory with exit code 0
  - criterion: All existing API endpoints maintain functionality with proper TypeScript
      types
    verification: Run integration test suite with 100% endpoint coverage and all tests
      passing
  - criterion: Supabase client operations are fully typed with generated schema types
    verification: Database queries show proper IntelliSense and compile-time type
      checking
  - criterion: ML service integrations (OpenAI/Anthropic/RunPod) have complete type
      coverage
    verification: All ML API calls have typed request/response interfaces with runtime
      validation
  - criterion: Development experience improvements are measurable
    verification: IDE shows proper autocomplete, error detection, and refactoring
      support across all TypeScript files
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/types/ml-services.test.ts
      coverage_target: 90%
      scenarios:
      - OpenAI response type validation
      - Anthropic message format typing
      - RunPod job state transitions
      - Type inference from Zod schemas
    - file: apps/backend/src/__tests__/lib/supabase.test.ts
      coverage_target: 85%
      scenarios:
      - Generated type compatibility
      - Query result type inference
      - Error handling with typed responses
    integration_tests:
    - file: apps/backend/src/__tests__/integration/api-routes.test.ts
      scenarios:
      - End-to-end type safety from request to response
      - Zod validation integration with Fastify
      - Error response type consistency
    - file: apps/backend/src/__tests__/integration/ml-workflows.test.ts
      scenarios:
      - Complete ML pipeline with typed data flow
      - File upload processing with proper types
    manual_testing:
    - step: Start development server and check TypeScript compilation
      expected: Server starts without type errors, hot reload works
    - step: Test API endpoints in Postman with various payloads
      expected: Proper validation errors returned for invalid data
    - step: Verify IDE experience in VS Code
      expected: Full IntelliSense, go-to-definition, and error highlighting
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Setup TypeScript configuration and development tooling
      done: false
    - task: Install dependencies and configure build scripts
      done: false
    - task: Convert core server setup and plugin registration to TypeScript
      done: false
    - task: Generate and integrate Supabase database types
      done: false
    - task: Create Zod schemas for all API endpoints with validation
      done: false
    - task: Convert ML service integrations with proper typing
      done: false
    - task: Implement Fastify type providers for end-to-end type safety
      done: false
    - task: Convert all route handlers and middleware to TypeScript
      done: false
    - task: Add comprehensive type testing and validation tests
      done: false
    - task: Update documentation and development setup guides
      done: false
- key: T24
  title: Supabase Database Setup with RLS
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 5
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task establishes the foundational database infrastructure for Morpheus,
      setting up Supabase with PostgreSQL and implementing Row Level Security (RLS)
      policies. This is critical for the novel-to-comic platform as it needs to securely
      manage user accounts, novel content, comic transformations, and billing data.
      RLS ensures users can only access their own content while allowing for proper
      admin access patterns and multi-tenant architecture.


      **Technical Approach:**

      - Use Supabase CLI for schema management and migrations

      - Implement RLS policies using PostgreSQL''s native security features

      - Create TypeScript types from database schema using Supabase''s type generation

      - Design tables for users, novels, comic_pages, transformations, subscriptions,
      and audit logs

      - Use Supabase''s built-in auth integration for user management

      - Implement soft deletes and audit trails for data integrity

      - Create views for complex queries and reporting


      **Dependencies:**

      - External: @supabase/supabase-js, @supabase/cli, dotenv, pg (for local dev)

      - Internal: Will be consumed by authentication service, novel processing service,
      and API routes


      **Risks:**

      - RLS policy complexity: Start simple, iterate; use policy templates and thorough
      testing

      - Migration rollback challenges: Always create reversible migrations; test on
      staging first

      - Performance with RLS: Monitor query plans; create appropriate indexes; consider
      materialized views for heavy queries

      - Auth integration leaks: Validate RLS policies in isolation; use security-focused
      code reviews


      **Complexity Notes:**

      Higher complexity than typical database setup due to multi-tenant RLS requirements
      and the need to handle large binary assets (comic images). The ML integration
      for storing transformation metadata adds additional schema complexity.


      **Key Files:**

      - supabase/migrations/: SQL migration files for schema and RLS policies

      - packages/database/: TypeScript types and client configuration

      - apps/backend/src/lib/supabase.ts: Supabase client setup

      - .env.example: Environment variable templates

      '
    design_decisions:
    - decision: Use Supabase RLS instead of application-level authorization
      rationale: Database-level security provides defense in depth, reduces code complexity,
        and leverages PostgreSQL's battle-tested security features
      alternatives_considered:
      - Application-level auth middleware
      - API Gateway authorization
      - Custom RBAC system
    - decision: Implement soft deletes for user content
      rationale: Novel and comic data represents significant user investment; soft
        deletes enable recovery and comply with data retention policies
      alternatives_considered:
      - Hard deletes with backups
      - Archive tables
      - Event sourcing
    - decision: Use Supabase's built-in auth with custom user profiles
      rationale: Leverages proven auth system while allowing custom user metadata
        for subscription tiers and preferences
      alternatives_considered:
      - Custom JWT auth
      - Auth0 integration
      - NextAuth.js
    researched_at: '2026-02-07T18:53:23.433939'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:30:38.845342'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: "Set up Supabase project with structured migrations for core tables\
      \ (users, novels, comic_pages, transformations). \nImplement RLS policies that\
      \ enforce user ownership while allowing admin access. Create TypeScript types\
      \ from schema \nand configure Supabase client for both server and client-side\
      \ usage. Include audit logging and soft delete patterns \nfor data integrity.\
      \ Test RLS policies thoroughly with different user roles and edge cases.\n"
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.38.0
      reason: Official Supabase client for database operations and real-time subscriptions
    - name: '@supabase/cli'
      version: ^1.123.0
      reason: Database migrations, type generation, and local development environment
    - name: pg
      version: ^8.11.0
      reason: PostgreSQL client for direct database operations and connection pooling
    - name: zod
      version: ^3.22.0
      reason: Runtime validation for database schemas and API input validation
    files_to_modify:
    - path: package.json
      changes: Add Supabase CLI scripts and database-related dependencies
    - path: .env.example
      changes: Add Supabase URL, anon key, and service role key templates
    new_files:
    - path: supabase/config.toml
      purpose: Supabase project configuration
    - path: supabase/migrations/20241201000001_initial_schema.sql
      purpose: Initial database schema with core tables
    - path: supabase/migrations/20241201000002_rls_policies.sql
      purpose: Row Level Security policies for all tables
    - path: supabase/migrations/20241201000003_audit_system.sql
      purpose: Audit logging triggers and soft delete functions
    - path: packages/database/src/client.ts
      purpose: Supabase client configuration and initialization
    - path: packages/database/src/types.ts
      purpose: Generated TypeScript types from database schema
    - path: packages/database/src/policies.ts
      purpose: RLS policy helper functions and constants
    - path: packages/database/package.json
      purpose: Database package dependencies and scripts
    - path: apps/backend/src/lib/supabase.ts
      purpose: Backend-specific Supabase client with service role
    - path: apps/web/src/lib/supabase.ts
      purpose: Client-side Supabase client configuration
    - path: scripts/db-reset.sh
      purpose: Development database reset and seeding script
    - path: scripts/generate-types.sh
      purpose: Script to regenerate TypeScript types from schema
  acceptance_criteria:
  - criterion: Supabase database schema with all core tables (users, novels, comic_pages,
      transformations, subscriptions) deployed with proper relationships and constraints
    verification: Run 'npx supabase db describe' and verify all tables exist with
      correct schema
  - criterion: RLS policies enforce user ownership - users can only access their own
      novels and comic pages, admins can access all data
    verification: Execute test queries with different user JWT tokens and verify access
      restrictions work correctly
  - criterion: TypeScript types generated from database schema are available and properly
      typed
    verification: Import types from packages/database/types.ts and verify TypeScript
      compilation with proper type checking
  - criterion: Supabase client configured for both server-side and client-side usage
      with proper environment variable setup
    verification: Test database connections from apps/web and apps/backend with different
      auth contexts
  - criterion: Audit logging and soft delete functionality working for all major tables
    verification: Perform create/update/delete operations and verify audit_logs table
      populated and soft deletes preserve data
  testing:
    unit_tests:
    - file: packages/database/src/__tests__/client.test.ts
      coverage_target: 90%
      scenarios:
      - Supabase client initialization
      - Environment variable validation
      - Connection error handling
    - file: packages/database/src/__tests__/types.test.ts
      coverage_target: 85%
      scenarios:
      - Type generation validation
      - Schema relationship types
      - Enum type definitions
    integration_tests:
    - file: packages/database/src/__tests__/integration/rls.test.ts
      scenarios:
      - User can only read their own novels
      - User cannot access other users' comic pages
      - Admin can access all resources
      - Unauthenticated requests properly blocked
    - file: packages/database/src/__tests__/integration/audit.test.ts
      scenarios:
      - Audit logs created on data changes
      - Soft delete preserves data but hides from queries
      - Admin can view deleted records
    manual_testing:
    - step: Create test user via Supabase dashboard and attempt to access another
        user's novel via SQL
      expected: Query returns no results due to RLS policy
    - step: Test migration rollback on staging environment
      expected: Database schema reverts cleanly without data loss
    - step: Verify large file upload handling for comic images
      expected: Files upload successfully with proper metadata stored
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Initialize Supabase project and configure local development environment
      done: false
    - task: Create initial migration with core table schema (users, novels, comic_pages,
        transformations, subscriptions)
      done: false
    - task: Implement RLS policies migration with comprehensive user ownership rules
      done: false
    - task: Set up audit logging system with triggers and soft delete functions
      done: false
    - task: Create database package with TypeScript client and type generation
      done: false
    - task: Configure Supabase clients for backend and frontend applications
      done: false
    - task: Write comprehensive integration tests for RLS policies and audit system
      done: false
    - task: Create development scripts for database management and type generation
      done: false
    - task: Document database schema, RLS policies, and development workflows
      done: false
    - task: Deploy to staging and validate all functionality end-to-end
      done: false
- key: T25
  title: API Routes Implementation
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 5
  area: backend
  dependsOn:
  - T23
  - T24
  agent_notes:
    research_findings: '**Context:**

      This task involves implementing the core API routes for Morpheus''s backend
      services using Fastify 5. As a novel-to-comic transformation platform, we need
      RESTful endpoints for user management, novel upload/processing, comic generation
      workflows, project management, and asset serving. This is foundational infrastructure
      that enables the dashboard and storefront frontends to interact with backend
      services, manage the ML pipeline, and handle user data securely.


      **Technical Approach:**

      - Use Fastify 5''s plugin architecture for modular route organization

      - Implement OpenAPI/Swagger documentation with @fastify/swagger

      - Apply JWT-based authentication with Supabase integration

      - Use Zod for request/response validation and TypeScript type generation

      - Implement rate limiting and CORS policies

      - Follow RESTful conventions with proper HTTP status codes

      - Use Fastify''s built-in serialization for performance

      - Implement middleware for logging, error handling, and request validation


      **Dependencies:**

      - External: @fastify/cors, @fastify/jwt, @fastify/swagger, @fastify/rate-limit,
      zod, @supabase/supabase-js

      - Internal: Database schemas, authentication middleware, ML service integrations,
      file upload handlers


      **Risks:**

      - API versioning strategy: Implement /v1 prefix from start to avoid breaking
      changes

      - Rate limiting bypass: Use distributed rate limiting with Redis for multi-instance
      deployments

      - Validation performance: Cache Zod schemas and use Fastify''s built-in serialization

      - Authentication token leakage: Implement proper CORS, secure headers, and token
      rotation

      - File upload vulnerabilities: Validate file types, implement size limits, scan
      for malware


      **Complexity Notes:**

      More complex than initially estimated due to need for comprehensive validation
      schemas, proper error handling patterns, and integration with multiple external
      services (Supabase, OpenAI, RunPod). The ML pipeline integration adds significant
      complexity for handling async operations and webhook endpoints.


      **Key Files:**

      - apps/backend/src/routes/: Route definitions organized by domain

      - apps/backend/src/plugins/: Custom Fastify plugins for auth, validation

      - apps/backend/src/schemas/: Zod validation schemas

      - apps/backend/src/types/: TypeScript type definitions

      - apps/backend/src/middleware/: Authentication and authorization middleware

      '
    design_decisions:
    - decision: Use Fastify plugin architecture with domain-based route organization
      rationale: Enables modular development, easy testing, and clear separation of
        concerns while leveraging Fastify's performance benefits
      alternatives_considered:
      - Express with custom middleware
      - Koa with router
      - NestJS decorators
    - decision: Implement Zod schemas for validation with automatic TypeScript type
        generation
      rationale: Provides runtime validation, compile-time type safety, and reduces
        boilerplate while being framework agnostic
      alternatives_considered:
      - Joi validation
      - class-validator decorators
      - JSON Schema
    - decision: Use Supabase Auth integration with JWT verification middleware
      rationale: Leverages existing Supabase infrastructure, provides secure token
        validation, and integrates with database RLS policies
      alternatives_considered:
      - Custom JWT implementation
      - Passport.js strategies
      - Auth0 integration
    researched_at: '2026-02-07T18:53:44.723839'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:31:05.613800'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: "Create domain-specific Fastify plugins (auth, projects, novels, comics,\
      \ assets) with dedicated route handlers. \nImplement comprehensive Zod schemas\
      \ for request validation and response serialization. \nUse middleware chains\
      \ for authentication, rate limiting, and error handling. \nStructure routes\
      \ following RESTful conventions with proper HTTP methods and status codes. \n\
      Integrate with Supabase for data persistence and authentication, with ML services\
      \ for async processing workflows.\n"
    external_dependencies:
    - name: '@fastify/cors'
      version: ^9.0.0
      reason: Cross-origin resource sharing configuration for frontend integration
    - name: '@fastify/jwt'
      version: ^8.0.0
      reason: JWT token verification and authentication middleware
    - name: '@fastify/swagger'
      version: ^8.0.0
      reason: OpenAPI documentation generation and API explorer
    - name: '@fastify/rate-limit'
      version: ^9.0.0
      reason: API rate limiting to prevent abuse and ensure fair usage
    - name: zod
      version: ^3.22.0
      reason: Runtime validation and TypeScript type inference for API schemas
    - name: '@fastify/multipart'
      version: ^8.0.0
      reason: File upload handling for novel documents and images
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register route plugins, configure CORS, rate limiting, and Swagger
    - path: apps/backend/package.json
      changes: Add Fastify plugins and Zod dependencies
    new_files:
    - path: apps/backend/src/routes/auth.ts
      purpose: Authentication endpoints (login, register, refresh, logout)
    - path: apps/backend/src/routes/projects.ts
      purpose: Project CRUD operations and management
    - path: apps/backend/src/routes/novels.ts
      purpose: Novel upload, processing, and text extraction endpoints
    - path: apps/backend/src/routes/comics.ts
      purpose: Comic generation, status checking, and download endpoints
    - path: apps/backend/src/routes/assets.ts
      purpose: File serving, upload, and asset management
    - path: apps/backend/src/plugins/auth.ts
      purpose: JWT authentication plugin with Supabase integration
    - path: apps/backend/src/plugins/validation.ts
      purpose: Zod validation plugin with error formatting
    - path: apps/backend/src/plugins/rate-limit.ts
      purpose: Rate limiting configuration per route
    - path: apps/backend/src/schemas/auth.ts
      purpose: Authentication request/response Zod schemas
    - path: apps/backend/src/schemas/projects.ts
      purpose: Project-related Zod validation schemas
    - path: apps/backend/src/schemas/novels.ts
      purpose: Novel upload and processing schemas
    - path: apps/backend/src/schemas/comics.ts
      purpose: Comic generation and metadata schemas
    - path: apps/backend/src/schemas/common.ts
      purpose: Shared validation schemas (pagination, errors, etc.)
    - path: apps/backend/src/middleware/error-handler.ts
      purpose: Global error handling with proper HTTP status mapping
    - path: apps/backend/src/middleware/request-logger.ts
      purpose: Request/response logging middleware
    - path: apps/backend/src/types/api.ts
      purpose: TypeScript types generated from Zod schemas
    - path: apps/backend/src/utils/response-helpers.ts
      purpose: Standardized API response formatting utilities
  acceptance_criteria:
  - criterion: All core API routes (/v1/auth, /v1/projects, /v1/novels, /v1/comics,
      /v1/assets) respond with proper HTTP status codes and follow RESTful conventions
    verification: Run integration tests with `npm run test:integration` and verify
      all routes return expected status codes (200, 201, 400, 401, 404, 500)
  - criterion: Request/response validation works correctly using Zod schemas with
      comprehensive error messages
    verification: Send malformed requests to each endpoint and verify proper 400 responses
      with detailed validation errors
  - criterion: JWT authentication protects all secured endpoints and integrates with
      Supabase
    verification: Test authenticated and unauthenticated requests; verify JWT tokens
      are validated against Supabase users
  - criterion: OpenAPI documentation is auto-generated and accessible at /documentation
      endpoint
    verification: Navigate to http://localhost:3001/documentation and verify all routes
      are documented with request/response schemas
  - criterion: Rate limiting prevents abuse with configurable limits per endpoint
    verification: Send burst requests exceeding rate limits and verify 429 responses
      are returned
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/routes/auth.test.ts
      coverage_target: 90%
      scenarios:
      - JWT token validation success/failure
      - User registration and login flows
      - Token refresh mechanism
    - file: apps/backend/src/__tests__/routes/projects.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations for projects
      - Project ownership validation
      - Pagination and filtering
    - file: apps/backend/src/__tests__/schemas/validation.test.ts
      coverage_target: 95%
      scenarios:
      - Valid request schemas pass validation
      - Invalid requests return proper error messages
      - Type coercion works correctly
    integration_tests:
    - file: apps/backend/src/__tests__/integration/api-routes.test.ts
      scenarios:
      - Complete user workflow from registration to comic creation
      - File upload and asset serving pipeline
      - ML service integration for comic processing
    - file: apps/backend/src/__tests__/integration/auth-flow.test.ts
      scenarios:
      - Supabase authentication integration
      - Protected route access control
    manual_testing:
    - step: Use Postman/Thunder Client to test all API endpoints with various payloads
      expected: All routes respond correctly with proper status codes and response
        formats
    - step: Test rate limiting by sending rapid requests to /v1/novels/upload
      expected: Requests are throttled after configured limit with 429 status
    - step: Verify Swagger UI loads at /documentation with interactive endpoint testing
      expected: Complete API documentation with working 'Try it out' functionality
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup Fastify plugins and dependencies in package.json
      done: false
    - task: Create base Zod schemas for all domains (auth, projects, novels, comics)
      done: false
    - task: Implement authentication plugin with Supabase JWT validation
      done: false
    - task: Build core route handlers for each domain with proper validation
      done: false
    - task: Configure rate limiting, CORS, and security middleware
      done: false
    - task: Integrate OpenAPI/Swagger documentation generation
      done: false
    - task: Implement error handling and response standardization
      done: false
    - task: Write comprehensive unit and integration tests
      done: false
    - task: Test ML service integration endpoints for async workflows
      done: false
    - task: Document API endpoints and update technical documentation
      done: false
- key: T26
  title: Authentication Integration
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 3
  area: backend
  dependsOn:
  - T24
  agent_notes:
    research_findings: '**Context:**

      Authentication is the foundational security layer for Morpheus, enabling user
      registration, login, and secure access control across the platform. This directly
      supports user management for the comic creation pipeline, subscription handling,
      and API access control. Since Morpheus uses Supabase as the database, leveraging
      Supabase Auth provides a comprehensive, production-ready authentication system
      with minimal custom implementation.


      **Technical Approach:**

      Implement Supabase Auth as the primary authentication provider, integrated with
      Fastify 5 backend. Use JWT-based authentication with refresh token rotation.
      Implement role-based access control (RBAC) with roles like ''user'', ''premium'',
      ''admin''. Create authentication middleware for route protection, session management
      utilities, and user profile management endpoints. Follow OAuth 2.0 patterns
      for third-party integrations (Google, GitHub).


      **Dependencies:**

      - External: @supabase/supabase-js, @fastify/jwt, @fastify/cookie, @fastify/cors,
      bcrypt, zod (validation)

      - Internal: Database schemas (users, profiles, roles), error handling service,
      logging service, rate limiting middleware


      **Risks:**

      - Session management complexity: Use Supabase''s built-in session handling with
      proper refresh token rotation

      - JWT security vulnerabilities: Implement short-lived access tokens (15min)
      with secure refresh mechanism

      - Rate limiting bypass: Implement strict rate limiting on auth endpoints to
      prevent brute force

      - Data consistency across services: Use Supabase RLS (Row Level Security) for
      consistent access control


      **Complexity Notes:**

      Medium complexity - Supabase Auth significantly reduces implementation overhead
      compared to custom auth. Main complexity lies in proper middleware integration
      with Fastify 5''s new plugin system and ensuring seamless frontend integration.


      **Key Files:**

      - packages/backend/src/plugins/auth.ts: Fastify auth plugin

      - packages/backend/src/middleware/authenticate.ts: JWT verification middleware

      - packages/backend/src/routes/auth/: Authentication route handlers

      - packages/backend/src/services/user.service.ts: User management logic

      - packages/backend/src/types/auth.ts: Authentication type definitions

      - supabase/migrations/: User tables and RLS policies

      '
    design_decisions:
    - decision: Use Supabase Auth as primary authentication provider
      rationale: Leverages existing Supabase infrastructure, provides enterprise-grade
        security, handles complex auth flows, and reduces maintenance overhead
      alternatives_considered:
      - Custom JWT implementation
      - Auth0
      - Firebase Auth
    - decision: Implement middleware-based route protection in Fastify
      rationale: Provides clean separation of concerns, reusable across routes, and
        integrates well with Fastify's plugin architecture
      alternatives_considered:
      - Decorator-based auth
      - Route-level auth checks
      - Gateway-level auth
    - decision: Use Row Level Security (RLS) for database-level access control
      rationale: Ensures data security at the database layer, prevents data leaks
        even if application logic fails, and scales automatically
      alternatives_considered:
      - Application-level access control
      - View-based security
      - Manual query filtering
    researched_at: '2026-02-07T18:54:06.881219'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:31:28.881005'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a Fastify plugin that integrates Supabase Auth with JWT token
      verification middleware. Implement user registration/login endpoints that leverage
      Supabase''s built-in auth methods. Set up automatic JWT refresh handling and
      secure cookie-based session management. Create role-based middleware decorators
      for protecting routes based on user permissions. Establish user profile management
      with proper validation using Zod schemas.

      '
    external_dependencies:
    - name: '@supabase/supabase-js'
      version: ^2.39.0
      reason: Official Supabase client for authentication and database operations
    - name: '@fastify/jwt'
      version: ^8.0.0
      reason: JWT token generation and verification for Fastify
    - name: '@fastify/cookie'
      version: ^9.3.1
      reason: Secure cookie handling for refresh tokens
    - name: '@fastify/cors'
      version: ^9.0.1
      reason: CORS configuration for frontend authentication requests
    - name: bcrypt
      version: ^5.1.1
      reason: Password hashing for additional security layers
    - name: zod
      version: ^3.22.4
      reason: Runtime validation for authentication payloads
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register auth plugin, add CORS configuration for auth endpoints
    - path: packages/backend/package.json
      changes: 'Add dependencies: @supabase/supabase-js, @fastify/jwt, @fastify/cookie,
        @fastify/cors, bcrypt, zod'
    - path: packages/backend/.env.example
      changes: Add Supabase URL, anon key, service role key, JWT secret
    new_files:
    - path: packages/backend/src/plugins/auth.ts
      purpose: Fastify plugin for Supabase Auth integration and JWT handling
    - path: packages/backend/src/middleware/authenticate.ts
      purpose: JWT verification middleware with role-based access control
    - path: packages/backend/src/middleware/rate-limit.ts
      purpose: Rate limiting middleware for auth endpoints
    - path: packages/backend/src/routes/auth/register.ts
      purpose: User registration endpoint with validation
    - path: packages/backend/src/routes/auth/login.ts
      purpose: User login endpoint with JWT token generation
    - path: packages/backend/src/routes/auth/logout.ts
      purpose: User logout endpoint with token invalidation
    - path: packages/backend/src/routes/auth/refresh.ts
      purpose: JWT refresh token endpoint
    - path: packages/backend/src/routes/auth/profile.ts
      purpose: User profile management endpoints
    - path: packages/backend/src/services/user.service.ts
      purpose: User management service interfacing with Supabase
    - path: packages/backend/src/services/supabase.service.ts
      purpose: Supabase client configuration and utilities
    - path: packages/backend/src/types/auth.ts
      purpose: TypeScript interfaces for authentication objects
    - path: packages/backend/src/schemas/auth.schemas.ts
      purpose: Zod validation schemas for auth endpoints
    - path: supabase/migrations/001_create_auth_tables.sql
      purpose: Database schema for user profiles and roles
    - path: supabase/migrations/002_setup_rls_policies.sql
      purpose: Row Level Security policies for user data access
  acceptance_criteria:
  - criterion: Users can register, login, and logout with email/password through Supabase
      Auth
    verification: POST /auth/register, POST /auth/login, POST /auth/logout return
      appropriate status codes and valid JWT tokens
  - criterion: JWT tokens expire after 15 minutes and refresh tokens rotate properly
    verification: Test token expiration at 15min mark and verify refresh endpoint
      returns new access/refresh token pair
  - criterion: Protected routes require valid JWT and return 401 for unauthorized
      requests
    verification: Access protected endpoints without token returns 401, with valid
      token returns expected data
  - criterion: Role-based access control prevents unauthorized access to admin/premium
      features
    verification: User role cannot access admin routes, premium features require premium
      role
  - criterion: Rate limiting prevents brute force attacks on authentication endpoints
    verification: Exceed rate limit (5 attempts/minute) on /auth/login returns 429
      status
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/plugins/auth.test.ts
      coverage_target: 90%
      scenarios:
      - JWT token generation and validation
      - User role verification
      - Token refresh logic
      - Error handling for invalid tokens
    - file: packages/backend/src/__tests__/middleware/authenticate.test.ts
      coverage_target: 85%
      scenarios:
      - Valid token allows access
      - Expired token returns 401
      - Missing token returns 401
      - Role-based access validation
    - file: packages/backend/src/__tests__/services/user.service.test.ts
      coverage_target: 85%
      scenarios:
      - User profile creation/updates
      - Password validation
      - Role assignment logic
    integration_tests:
    - file: packages/backend/src/__tests__/integration/auth.test.ts
      scenarios:
      - Complete registration flow with Supabase
      - Login flow with JWT generation
      - Token refresh workflow
      - Protected route access with middleware
      - Rate limiting enforcement
    manual_testing:
    - step: Register new user via Postman/curl
      expected: 201 status, user created in Supabase, JWT tokens returned
    - step: Login with registered credentials
      expected: 200 status, valid access/refresh tokens in secure cookies
    - step: Access protected route with token
      expected: Successful access to protected resource
    - step: Wait 15+ minutes and try accessing protected route
      expected: 401 unauthorized, token expired
    - step: Use refresh token to get new access token
      expected: New token pair generated, old refresh token invalidated
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup Supabase project and obtain API keys
      done: false
    - task: Install required dependencies and configure environment variables
      done: false
    - task: Create database migrations for user profiles and roles
      done: false
    - task: Implement Supabase service and auth plugin
      done: false
    - task: Create JWT middleware with role-based access control
      done: false
    - task: Implement authentication route handlers (register/login/logout/refresh)
      done: false
    - task: Add rate limiting middleware to auth endpoints
      done: false
    - task: Create user profile management endpoints
      done: false
    - task: Write comprehensive unit and integration tests
      done: false
    - task: Update API documentation and create usage examples
      done: false
- key: T27
  title: Backend Unit Tests (Vitest)
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 5
  area: backend
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      Backend unit tests are critical for the Morpheus platform''s reliability, especially
      given the complex AI/ML integrations and data transformations involved in novel-to-comic
      conversion. This task establishes comprehensive test coverage for business logic,
      API endpoints, data models, and external service integrations (OpenAI/Anthropic,
      RunPod). Unit tests provide rapid feedback during development, prevent regressions,
      and enable confident refactoring as the platform scales.


      **Technical Approach:**

      - Use Vitest as the primary test runner (already in tech stack) with TypeScript
      support

      - Implement test organization following Fastify 5 plugin structure

      - Mock external services (LLMs, image generation APIs) using vi.mock()

      - Use supertest-like testing for HTTP endpoints via Fastify''s inject method

      - Implement repository pattern testing with in-memory databases or transaction
      rollback

      - Follow AAA pattern (Arrange, Act, Assert) with descriptive test names

      - Create test fixtures for novel parsing, character extraction, and scene generation

      - Use dependency injection for better testability of services


      **Dependencies:**

      - External: @vitest/ui, @faker-js/faker, supertest, testcontainers (optional)

      - Internal: Database schemas, authentication middleware, AI service wrappers,
      comic generation pipeline


      **Risks:**

      - Flaky tests: External API mocks may not reflect real behavior changes

      - Slow tests: Database operations and file I/O could slow CI/CD pipeline

      - Mock drift: Mocked AI responses may diverge from actual API behavior

      - Test maintenance: Complex business logic changes requiring extensive test
      updates


      **Complexity Notes:**

      Higher complexity than initially expected due to:

      - Complex AI/ML integration points requiring sophisticated mocking strategies

      - Novel parsing and comic generation pipelines with multiple data transformation
      stages

      - Supabase-specific testing patterns and database transaction handling

      - File upload/storage testing for images and generated comics


      **Key Files:**

      - packages/backend/src/**/*.test.ts: Unit test files alongside source

      - packages/backend/test/: Shared test utilities, fixtures, and helpers

      - packages/backend/vitest.config.ts: Vitest configuration

      - packages/backend/src/plugins/: Plugin-specific test files

      - packages/backend/src/services/: Service layer unit tests

      - packages/backend/src/lib/: Utility and helper function tests

      '
    design_decisions:
    - decision: Co-locate unit tests alongside source files using .test.ts suffix
      rationale: Improves discoverability and maintainability, follows modern testing
        practices, easier refactoring
      alternatives_considered:
      - Separate /test directory
      - /tests folder structure
      - __tests__ folders
    - decision: Use Fastify's built-in inject() method instead of supertest
      rationale: Native Fastify testing approach, better TypeScript support, no additional
        HTTP server startup
      alternatives_considered:
      - supertest library
      - Manual HTTP requests
      - Custom test helpers
    - decision: Mock external AI services at the service layer boundary
      rationale: Allows testing business logic without API calls, predictable test
        data, faster execution
      alternatives_considered:
      - Integration tests with real APIs
      - Record/replay approach
      - Stub entire service classes
    researched_at: '2026-02-07T18:54:30.518661'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:31:51.797002'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement comprehensive unit test suite using Vitest with co-located
      test files alongside source code. Create mock factories for AI service responses
      and use Fastify''s inject method for endpoint testing. Establish database testing
      patterns using transactions or in-memory alternatives. Build reusable test fixtures
      for novel content, character data, and comic generation scenarios. Focus on
      testing business logic, data transformations, and error handling paths while
      mocking external dependencies.

      '
    external_dependencies:
    - name: '@vitest/ui'
      version: ^1.0.0
      reason: Web UI for running and debugging tests during development
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate realistic test data for novels, characters, and user profiles
    - name: testcontainers
      version: ^10.0.0
      reason: 'Optional: Spin up real PostgreSQL containers for integration-style
        tests'
    - name: '@types/supertest'
      version: ^6.0.0
      reason: TypeScript definitions if using supertest as fallback option
    files_to_modify:
    - path: packages/backend/package.json
      changes: Add test scripts, vitest dependencies, and coverage configuration
    - path: packages/backend/vitest.config.ts
      changes: Configure test environment, coverage thresholds, and mock patterns
    - path: packages/backend/src/plugins/auth/index.ts
      changes: Add dependency injection support for better testability
    - path: packages/backend/src/services/ai-service.ts
      changes: Refactor for dependency injection and mock-friendly interfaces
    new_files:
    - path: packages/backend/test/fixtures/novel-samples.ts
      purpose: Sample novel content for testing parsing and character extraction
    - path: packages/backend/test/fixtures/ai-responses.ts
      purpose: Mock AI service responses for consistent testing
    - path: packages/backend/test/helpers/database.ts
      purpose: Database testing utilities and transaction management
    - path: packages/backend/test/helpers/mock-factories.ts
      purpose: Factory functions for generating test data
    - path: packages/backend/src/plugins/auth/__tests__/auth.test.ts
      purpose: Authentication and authorization unit tests
    - path: packages/backend/src/services/__tests__/ai-service.test.ts
      purpose: AI service integration testing with mocks
    - path: packages/backend/src/services/__tests__/comic-generator.test.ts
      purpose: Comic generation pipeline unit tests
    - path: packages/backend/src/services/__tests__/file-storage.test.ts
      purpose: File upload and storage service tests
    - path: packages/backend/src/plugins/database/__tests__/repository.test.ts
      purpose: Database repository pattern unit tests
    - path: packages/backend/src/lib/__tests__/validators.test.ts
      purpose: Input validation and schema testing
    - path: packages/backend/src/routes/__tests__/novels.test.ts
      purpose: Novel management API endpoint tests
    - path: packages/backend/src/routes/__tests__/comics.test.ts
      purpose: Comic generation API endpoint tests
    - path: packages/backend/src/routes/__tests__/users.test.ts
      purpose: User management API endpoint tests
  acceptance_criteria:
  - criterion: All core backend services achieve minimum 85% test coverage with comprehensive
      unit tests
    verification: Run `npm run test:coverage` in backend package and verify coverage
      report meets threshold
  - criterion: AI service integrations are properly mocked with realistic test data
      and error scenarios
    verification: Execute AI service tests in isolation with `npm run test -- ai`
      and verify no external API calls
  - criterion: Database operations use transaction rollback or in-memory patterns
      for test isolation
    verification: Run database tests multiple times with `npm run test -- --reporter=verbose
      db` and verify no test pollution
  - criterion: Comic generation pipeline has end-to-end unit test coverage for all
      transformation stages
    verification: Execute `npm run test -- comic` and verify novel parsing, character
      extraction, and scene generation are tested
  - criterion: Test suite runs in under 30 seconds locally and passes consistently
      in CI
    verification: Time test execution with `time npm run test` and verify CI pipeline
      shows green status
  testing:
    unit_tests:
    - file: packages/backend/src/plugins/auth/__tests__/auth.test.ts
      coverage_target: 90%
      scenarios:
      - JWT token validation
      - User session management
      - Authentication middleware
      - Permission checking
    - file: packages/backend/src/services/__tests__/ai-service.test.ts
      coverage_target: 85%
      scenarios:
      - OpenAI API integration mocking
      - Anthropic API responses
      - Rate limiting behavior
      - Error handling and retries
    - file: packages/backend/src/services/__tests__/comic-generator.test.ts
      coverage_target: 90%
      scenarios:
      - Novel text parsing
      - Character extraction
      - Scene generation
      - Image generation coordination
    - file: packages/backend/src/plugins/database/__tests__/repository.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations
      - Transaction handling
      - Query optimization
      - Connection pooling
    integration_tests:
    - file: packages/backend/src/__tests__/integration/api-endpoints.test.ts
      scenarios:
      - Complete novel upload and processing flow
      - User authentication and authorization flow
      - Comic generation end-to-end pipeline
    manual_testing:
    - step: Run test suite with coverage reporting
      expected: All tests pass with coverage above 85%
    - step: Execute tests in CI environment
      expected: Pipeline completes successfully without flaky test failures
  estimates:
    development: 4
    code_review: 1
    testing: 1
    documentation: 0.5
    total: 6.5
  progress:
    status: not-started
    checklist:
    - task: Setup Vitest configuration and test infrastructure
      done: false
    - task: Create test fixtures and mock factories for AI services
      done: false
    - task: Implement database testing patterns with transaction rollback
      done: false
    - task: Write unit tests for authentication and authorization
      done: false
    - task: Create comprehensive tests for AI service integrations
      done: false
    - task: Build comic generation pipeline test coverage
      done: false
    - task: Implement API endpoint tests using Fastify inject method
      done: false
    - task: Add file storage and upload testing
      done: false
    - task: Configure CI/CD pipeline integration
      done: false
    - task: Verify coverage thresholds and optimize test performance
      done: false
- key: T28
  title: Mock Mode for Backend Services
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 2
  area: backend
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      Mock Mode is essential for development and testing environments where external
      services (OpenAI/Anthropic LLMs, RunPod Stable Diffusion, Supabase) are expensive,
      unreliable, or unavailable. This solves several critical problems: eliminates
      API costs during development, enables offline development, provides deterministic
      responses for testing, prevents hitting rate limits, and allows simulation of
      error scenarios. For Morpheus, this is crucial given the high cost of AI services
      and the need for predictable comic generation workflows during development.


      **Technical Approach:**

      Implement a configuration-driven mock system using environment variables and
      dependency injection. Create mock implementations for each external service
      that return realistic, deterministic data. Use factories to switch between real
      and mock services based on NODE_ENV or MOCK_MODE flags. Store mock data as JSON
      fixtures organized by service and scenario (success, error, timeout). Implement
      a mock server for webhook testing and provide a development dashboard to control
      mock responses.


      **Dependencies:**

      - External: msw (Mock Service Worker), nock (HTTP mocking), faker-js (realistic
      test data)

      - Internal: Service abstraction layer, configuration management, logging system


      **Risks:**

      - Mock drift: Mock responses diverging from real API responses over time. Mitigation:
      Regular sync with actual API documentation and response validation

      - Incomplete error simulation: Missing edge cases in mock responses. Mitigation:
      Comprehensive error scenario mapping

      - Performance differences: Mocks being too fast compared to real services, hiding
      timing issues. Mitigation: Configurable delays in mock responses


      **Complexity Notes:**

      Initially seems straightforward but becomes complex when considering all the
      edge cases, error scenarios, and maintaining parity with real services. The
      challenge lies in creating realistic mock data for complex AI responses (story
      analysis, image generation) that maintains narrative coherence.


      **Key Files:**

      - packages/backend/src/services/mocks/: Mock service implementations

      - packages/backend/src/config/mock-config.ts: Mock mode configuration

      - packages/backend/src/factories/service-factory.ts: Service instantiation logic

      - packages/backend/src/fixtures/: Mock response data

      '
    design_decisions:
    - decision: Use dependency injection with service factories to switch between
        real and mock implementations
      rationale: Allows clean separation of mock logic from business logic, makes
        testing easier, and enables runtime switching of implementations
      alternatives_considered:
      - Direct mocking with nock/msw
      - Compile-time switching with webpack
      - Proxy pattern wrapper
    - decision: Store mock data as structured JSON fixtures with scenario-based organization
      rationale: Provides version control for mock data, allows easy editing by non-developers,
        and enables scenario-based testing
      alternatives_considered:
      - Generated mock data with faker
      - In-memory hardcoded responses
      - Database-stored mock data
    researched_at: '2026-02-07T18:54:50.954096'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:32:16.160579'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a service abstraction layer with interfaces for all external
      dependencies (LLM, image generation, database). Implement mock versions of each
      service that return realistic, deterministic responses from JSON fixtures. Use
      a service factory pattern controlled by environment variables to instantiate
      either real or mock services. Include configurable delays and error injection
      to simulate real-world conditions. Provide a development endpoint to switch
      mock scenarios at runtime.

      '
    external_dependencies:
    - name: msw
      version: ^2.0.0
      reason: Intercept and mock HTTP requests at the network level
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate realistic test data for user profiles, content, etc.
    - name: nock
      version: ^13.0.0
      reason: HTTP server mocking for API integration tests
    files_to_modify:
    - path: packages/backend/src/config/index.ts
      changes: Add mock mode configuration options and validation
    - path: packages/backend/src/services/llm/base-llm-service.ts
      changes: Extract interface for dependency injection
    - path: packages/backend/src/services/image/stable-diffusion-service.ts
      changes: Extract interface for mock implementation
    - path: packages/backend/src/middleware/service-injection.ts
      changes: Add service factory integration for request context
    new_files:
    - path: packages/backend/src/services/mocks/mock-llm-service.ts
      purpose: Mock implementation for OpenAI/Anthropic LLM services
    - path: packages/backend/src/services/mocks/mock-image-service.ts
      purpose: Mock implementation for RunPod Stable Diffusion API
    - path: packages/backend/src/services/mocks/mock-database-service.ts
      purpose: Mock implementation for Supabase database operations
    - path: packages/backend/src/factories/service-factory.ts
      purpose: Factory for instantiating real vs mock services based on config
    - path: packages/backend/src/fixtures/llm-responses.json
      purpose: Realistic LLM response data for story analysis and generation
    - path: packages/backend/src/fixtures/image-metadata.json
      purpose: Mock image generation responses with URLs and metadata
    - path: packages/backend/src/fixtures/comic-scenarios.json
      purpose: Pre-defined comic generation scenarios for testing
    - path: packages/backend/src/config/mock-config.ts
      purpose: Mock mode configuration schema and defaults
    - path: packages/backend/src/routes/dev/mock-dashboard.ts
      purpose: Development endpoint for controlling mock behavior
    - path: packages/backend/src/utils/mock-delay.ts
      purpose: Utility for adding realistic delays to mock responses
    - path: packages/backend/src/types/mock-types.ts
      purpose: TypeScript interfaces for mock service contracts
  acceptance_criteria:
  - criterion: Mock mode can be enabled/disabled via environment variables and configuration
    verification: Set MOCK_MODE=true and verify all external services return mock
      responses via integration tests
  - criterion: All external services (OpenAI, Anthropic, RunPod, Supabase) have realistic
      mock implementations
    verification: Run npm test -- --grep 'mock services' and verify 100% of external
      API calls return deterministic responses
  - criterion: Mock responses include configurable delays and error scenarios
    verification: Set MOCK_DELAY=2000 and MOCK_ERROR_RATE=0.1, verify responses take
      ~2s and 10% fail appropriately
  - criterion: Development dashboard allows runtime switching of mock scenarios
    verification: Access /dev/mocks endpoint, switch to 'error' scenario, verify subsequent
      API calls return error responses
  - criterion: Mock data maintains narrative coherence for comic generation workflows
    verification: Generate complete comic using mocks, verify story elements (characters,
      themes) remain consistent across panels
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/mocks/mock-llm-service.test.ts
      coverage_target: 90%
      scenarios:
      - Returns deterministic story analysis responses
      - Simulates API errors and timeouts
      - Respects configured delays
      - Maintains character consistency across requests
    - file: packages/backend/src/__tests__/services/mocks/mock-image-service.test.ts
      coverage_target: 90%
      scenarios:
      - Returns mock image URLs with metadata
      - Simulates generation failures
      - Handles different art styles consistently
    - file: packages/backend/src/__tests__/factories/service-factory.test.ts
      coverage_target: 95%
      scenarios:
      - Returns mock services when MOCK_MODE=true
      - Returns real services when MOCK_MODE=false
      - Throws error for invalid configuration
    integration_tests:
    - file: packages/backend/src/__tests__/integration/mock-comic-generation.test.ts
      scenarios:
      - Complete comic generation flow using mocks
      - Error handling during mock service failures
      - Performance with configured delays
    manual_testing:
    - step: Set MOCK_MODE=true, generate comic via API
      expected: Comic generated with consistent mock data, no external API calls
    - step: Access /dev/mocks dashboard, switch scenarios
      expected: Mock behavior changes reflect immediately in API responses
    - step: Enable error injection, attempt comic generation
      expected: Appropriate error responses with realistic failure modes
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Create service interfaces and extract from existing implementations
      done: false
    - task: Implement mock service classes with realistic response data
      done: false
    - task: Build service factory with environment-based switching
      done: false
    - task: Create JSON fixtures with coherent comic generation data
      done: false
    - task: Add mock configuration management and validation
      done: false
    - task: Implement development dashboard for runtime control
      done: false
    - task: Add configurable delays and error injection mechanisms
      done: false
    - task: Integrate mock services into existing API endpoints
      done: false
    - task: Write comprehensive test suite covering all scenarios
      done: false
    - task: Create documentation and development setup guide
      done: false
- key: T29
  title: Error Handling Strategy
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 2
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      A comprehensive error handling strategy is critical for Morpheus as it processes
      expensive ML operations (OpenAI/Anthropic API calls, RunPod Stable Diffusion)
      and handles user payments. Poor error handling leads to lost revenue, frustrated
      users, and difficult debugging. The platform needs to gracefully handle API
      failures, rate limits, timeout scenarios, and provide meaningful feedback to
      users while maintaining system reliability.


      **Technical Approach:**

      Implement a layered error handling strategy using:

      - Custom error classes with error codes and contextual data

      - Fastify error handlers with proper HTTP status mapping

      - Circuit breaker pattern for external ML APIs

      - Structured logging with correlation IDs for request tracing

      - Graceful degradation strategies for non-critical failures

      - Error boundaries in React components for frontend resilience

      - Dead letter queues for failed async operations


      **Dependencies:**

      - External: [@fastify/error](^3.0.0), [pino](^8.0.0), [opossum](^7.0.0), [@sentry/node](^7.0.0)

      - Internal: Database connection pooling, ML service wrappers, authentication
      middleware


      **Risks:**

      - Over-engineering error handling: Start simple, add complexity as needed

      - Inconsistent error formats across services: Establish clear error response
      schemas

      - Information leakage in error messages: Sanitize errors in production environments

      - Performance impact from excessive error logging: Implement log levels and
      sampling


      **Complexity Notes:**

      More complex than initially expected due to the distributed nature of ML operations
      and the need for user-friendly error recovery flows. Requires coordination between
      backend services, frontend components, and external API integrations.


      **Key Files:**

      - packages/backend/src/lib/errors/: Error class definitions and utilities

      - packages/backend/src/plugins/error-handler.ts: Fastify error handling plugin

      - packages/backend/src/lib/circuit-breaker.ts: ML API resilience wrapper

      - packages/shared/src/types/errors.ts: Shared error type definitions

      - packages/frontend/src/components/ErrorBoundary.tsx: React error boundaries

      '
    design_decisions:
    - decision: Use custom error classes with structured error codes instead of generic
        Error objects
      rationale: Enables consistent error handling across services, better debugging,
        and allows frontend to provide specific user guidance
      alternatives_considered:
      - Generic Error with message strings
      - HTTP-only error responses
      - Domain-specific error per service
    - decision: Implement circuit breaker pattern for ML API calls
      rationale: Prevents cascade failures when external APIs are down and provides
        faster failure detection
      alternatives_considered:
      - Simple retry with exponential backoff
      - Manual service health checks
      - No resilience patterns
    - decision: Use correlation IDs for request tracing across services
      rationale: Essential for debugging complex novel-to-comic transformation workflows
        that span multiple services
      alternatives_considered:
      - Service-specific logging only
      - Session-based tracking
      - No cross-service correlation
    researched_at: '2026-02-07T18:55:10.747750'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:32:42.780021'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a hierarchical error system with base error classes for different
      categories (ValidationError, ExternalServiceError, ResourceNotFoundError). Implement
      Fastify plugins for consistent error serialization and HTTP status mapping.
      Add circuit breakers around ML API calls with fallback strategies. Establish
      correlation ID propagation through request headers and include in all log entries
      for end-to-end traceability.

      '
    external_dependencies:
    - name: '@fastify/error'
      version: ^3.4.0
      reason: Fastify-native error handling with proper serialization
    - name: pino
      version: ^8.16.0
      reason: High-performance structured logging with correlation support
    - name: opossum
      version: ^7.0.0
      reason: Circuit breaker implementation for ML API resilience
    - name: '@sentry/node'
      version: ^7.81.0
      reason: Production error monitoring and alerting
    - name: nanoid
      version: ^5.0.0
      reason: Generate correlation IDs for request tracing
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register error handler plugin and correlation ID middleware
    - path: packages/backend/src/services/ml/openai-service.ts
      changes: Wrap API calls with circuit breaker and structured error handling
    - path: packages/backend/src/services/ml/runpod-service.ts
      changes: Add circuit breaker pattern and timeout handling
    - path: packages/frontend/src/App.tsx
      changes: Add root-level ErrorBoundary component
    - path: packages/frontend/src/components/ImageGeneration/ImageGenerationForm.tsx
      changes: Add component-level error boundary and error state handling
    new_files:
    - path: packages/backend/src/lib/errors/base-error.ts
      purpose: Base error class with correlation ID and context support
    - path: packages/backend/src/lib/errors/validation-error.ts
      purpose: Input validation specific error handling
    - path: packages/backend/src/lib/errors/external-service-error.ts
      purpose: ML API and external service error wrapper
    - path: packages/backend/src/lib/errors/resource-not-found-error.ts
      purpose: Database and resource lookup error handling
    - path: packages/backend/src/lib/errors/payment-error.ts
      purpose: Stripe and payment processing error handling
    - path: packages/backend/src/plugins/error-handler.ts
      purpose: Fastify plugin for centralized error handling and response formatting
    - path: packages/backend/src/plugins/correlation-id.ts
      purpose: Request correlation ID generation and propagation middleware
    - path: packages/backend/src/lib/circuit-breaker.ts
      purpose: Circuit breaker implementation for ML API resilience
    - path: packages/backend/src/lib/logger.ts
      purpose: Structured Pino logger configuration with correlation ID support
    - path: packages/backend/src/lib/dead-letter-queue.ts
      purpose: Failed operation queuing and retry mechanism
    - path: packages/shared/src/types/errors.ts
      purpose: Shared error type definitions and response schemas
    - path: packages/frontend/src/components/ErrorBoundary.tsx
      purpose: React error boundary component with user-friendly fallback UI
    - path: packages/frontend/src/hooks/useErrorHandler.ts
      purpose: Custom hook for consistent frontend error handling
    - path: packages/frontend/src/utils/error-reporting.ts
      purpose: Frontend error reporting and user notification utilities
  acceptance_criteria:
  - criterion: All API endpoints return consistent error response format with error
      code, message, and correlation ID
    verification: Run integration tests and verify error responses match schema in
      packages/shared/src/types/errors.ts
  - criterion: Circuit breakers activate after 5 consecutive failures to ML APIs and
      return graceful fallback responses
    verification: Mock ML API failures and verify circuit breaker state transitions
      with monitoring dashboard
  - criterion: All errors are logged with structured format including correlation
      ID, user context, and error classification
    verification: Check Pino logs contain required fields and correlation IDs trace
      through request lifecycle
  - criterion: React error boundaries catch component errors and display user-friendly
      fallback UI without crashing the app
    verification: Trigger frontend errors and verify ErrorBoundary components render
      fallback UI
  - criterion: Failed async operations are queued in dead letter queue for retry or
      manual intervention
    verification: Simulate queue processing failures and verify messages appear in
      DLQ with proper metadata
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/lib/errors/error-classes.test.ts
      coverage_target: 95%
      scenarios:
      - Custom error class instantiation with context
      - Error serialization and HTTP status mapping
      - Error inheritance and type checking
    - file: packages/backend/src/__tests__/lib/circuit-breaker.test.ts
      coverage_target: 90%
      scenarios:
      - Circuit breaker state transitions
      - Timeout and failure counting
      - Recovery and half-open state testing
    - file: packages/backend/src/__tests__/plugins/error-handler.test.ts
      coverage_target: 85%
      scenarios:
      - Fastify error handler registration
      - Error response formatting
      - Production vs development error details
    integration_tests:
    - file: packages/backend/src/__tests__/integration/error-flows.test.ts
      scenarios:
      - End-to-end error propagation from ML API to frontend
      - Correlation ID tracing through multiple services
      - Dead letter queue processing
    - file: packages/frontend/src/__tests__/integration/error-boundary.test.ts
      scenarios:
      - Error boundary component behavior
      - Error recovery and user feedback
    manual_testing:
    - step: Simulate OpenAI API rate limit by setting invalid API key
      expected: Circuit breaker activates, user sees 'AI service temporarily unavailable'
        message
    - step: Trigger database connection failure during image generation
      expected: Error logged with correlation ID, user notified, operation queued
        for retry
    - step: Cause React component error in image gallery
      expected: Error boundary catches error, shows fallback UI, logs error to monitoring
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Install and configure error handling dependencies (@fastify/error, pino,
        opossum, @sentry/node)
      done: false
    - task: Create base error class hierarchy and custom error types
      done: false
    - task: Implement Fastify error handler plugin with HTTP status mapping
      done: false
    - task: Add correlation ID middleware and logger configuration
      done: false
    - task: Implement circuit breaker wrapper for ML API services
      done: false
    - task: Create dead letter queue system for failed async operations
      done: false
    - task: Build React ErrorBoundary components and error handling hooks
      done: false
    - task: Add comprehensive unit and integration tests
      done: false
    - task: Update API documentation with error response schemas
      done: false
    - task: Configure Sentry error monitoring and alerting
      done: false
- key: T30
  title: Logging & Observability Setup
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p0
  effort: 2
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Morpheus processes complex ML workloads (LLM text generation, Stable Diffusion
      image generation) with external API dependencies and async operations. Without
      proper logging and observability, debugging production issues, monitoring performance
      bottlenecks, tracking API costs, and ensuring SLA compliance becomes nearly
      impossible. This is critical for M1 as it establishes the foundation for reliable
      backend services before adding more complex features.


      **Technical Approach:**

      - Structured logging with correlation IDs for request tracing across microservices

      - OpenTelemetry for distributed tracing (especially important for ML pipeline
      observability)

      - Prometheus metrics + Grafana dashboards for real-time monitoring

      - Custom Fastify plugins for request logging, error tracking, and performance
      metrics

      - Supabase integration for application-level audit logs

      - Health check endpoints with dependency status monitoring

      - Log aggregation with different levels (DEBUG for dev, INFO+ for production)


      **Dependencies:**

      - External: @opentelemetry/*, pino, prometheus-client, @fastify/sensible

      - Internal: Database connection pool monitoring, ML service status checks, authentication
      middleware integration


      **Risks:**

      - Performance overhead: excessive logging can impact ML processing latency

      - Log volume explosion: unstructured logs from ML operations could overwhelm
      storage

      - Sensitive data leakage: novel content, API keys, user data in logs

      - Missing context: async ML jobs losing trace correlation

      - Cost implications: verbose logging increasing infrastructure costs


      **Complexity Notes:**

      Higher complexity than typical logging setup due to:

      1. ML pipeline observability requirements (RunPod, OpenAI/Anthropic API monitoring)

      2. Cross-service correlation (backend → ML services → external APIs)

      3. Performance-sensitive operations requiring selective instrumentation

      4. Multi-tenant data isolation in logs


      **Key Files:**

      - packages/backend/src/plugins/logging.ts: Core logging plugin

      - packages/backend/src/plugins/metrics.ts: Prometheus metrics

      - packages/backend/src/plugins/tracing.ts: OpenTelemetry setup

      - packages/backend/src/lib/logger.ts: Logger configuration

      - packages/backend/src/routes/health.ts: Health check endpoint

      - packages/shared/src/types/telemetry.ts: Shared telemetry types

      '
    design_decisions:
    - decision: Use Pino for structured logging with OpenTelemetry for tracing
      rationale: Pino offers excellent performance for high-throughput ML operations,
        OpenTelemetry provides vendor-neutral observability for complex async workflows
      alternatives_considered:
      - Winston + Jaeger
      - Built-in console logging
      - DataDog APM only
    - decision: Implement correlation ID propagation through ML pipeline
      rationale: Essential for debugging failed transformations across novel→analysis→image
        generation→comic assembly stages
      alternatives_considered:
      - Service-level logging only
      - Database-based tracking
      - External correlation service
    - decision: Custom Fastify plugins for telemetry integration
      rationale: Provides seamless integration with existing Fastify 5 architecture
        and allows fine-grained control over what gets instrumented
      alternatives_considered:
      - Express.js middleware patterns
      - Global instrumentation
      - Route-level manual logging
    researched_at: '2026-02-07T18:55:35.684313'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:33:09.533689'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a comprehensive observability stack using Fastify plugins that
      integrate Pino structured logging, OpenTelemetry tracing, and Prometheus metrics.
      Implement correlation ID propagation from HTTP requests through ML pipeline
      stages, with custom instrumentation for RunPod and LLM API calls. Build health
      check endpoints that monitor all dependencies (Supabase, external APIs, ML services)
      and provide detailed system status. Use environment-based configuration to control
      log levels and sampling rates for optimal performance in production.

      '
    external_dependencies:
    - name: pino
      version: ^8.17.0
      reason: High-performance structured logging with minimal overhead for ML workloads
    - name: '@opentelemetry/api'
      version: ^1.7.0
      reason: Distributed tracing across novel transformation pipeline stages
    - name: '@opentelemetry/auto-instrumentations-node'
      version: ^0.40.0
      reason: Automatic instrumentation for HTTP, database, and external API calls
    - name: prom-client
      version: ^15.1.0
      reason: Prometheus metrics collection for performance monitoring and alerting
    - name: '@fastify/under-pressure'
      version: ^8.3.0
      reason: Health checks and circuit breaker functionality for ML service dependencies
    - name: pino-pretty
      version: ^10.3.0
      reason: Development-friendly log formatting (dev dependency)
    files_to_modify:
    - path: packages/backend/src/server.ts
      changes: Register logging, metrics, and tracing plugins before routes
    - path: packages/backend/src/services/ml/text-generation.ts
      changes: Add correlation ID propagation and metrics tracking to LLM calls
    - path: packages/backend/src/services/ml/image-generation.ts
      changes: Instrument RunPod API calls with tracing and error logging
    - path: packages/backend/src/services/database.ts
      changes: Add database connection health checks and query performance metrics
    new_files:
    - path: packages/backend/src/plugins/logging.ts
      purpose: Fastify plugin for Pino structured logging with correlation IDs
    - path: packages/backend/src/plugins/metrics.ts
      purpose: Prometheus metrics collection for ML operations and system health
    - path: packages/backend/src/plugins/tracing.ts
      purpose: OpenTelemetry distributed tracing setup for microservice correlation
    - path: packages/backend/src/lib/logger.ts
      purpose: Logger configuration and sensitive data redaction utilities
    - path: packages/backend/src/routes/health.ts
      purpose: Health check endpoint with dependency monitoring
    - path: packages/backend/src/lib/observability/correlation.ts
      purpose: Correlation ID management and async context propagation
    - path: packages/backend/src/lib/observability/metrics.ts
      purpose: Custom metrics definitions for ML pipeline operations
    - path: packages/shared/src/types/telemetry.ts
      purpose: Shared types for telemetry data structures and configuration
    - path: packages/backend/src/config/observability.ts
      purpose: Environment-based configuration for logging, metrics, and tracing
  acceptance_criteria:
  - criterion: All HTTP requests have correlation IDs that propagate through ML pipeline
      stages and external API calls
    verification: Check logs for correlation_id field in request/response pairs, verify
      same ID appears in RunPod/OpenAI API call logs
  - criterion: Health check endpoint returns detailed status of all dependencies (Supabase,
      RunPod, OpenAI/Anthropic APIs) with <100ms response time
    verification: GET /health returns 200 with dependency statuses, measure response
      time with wrk or similar tool
  - criterion: Prometheus metrics are exposed for ML pipeline performance (request
      duration, API costs, error rates) with proper labels
    verification: GET /metrics shows morpheus_* metrics with service/operation/status
      labels, verify in Grafana dashboard
  - criterion: Structured logs exclude sensitive data (API keys, user content, novel
      text) while maintaining debugging capability
    verification: Search logs for patterns matching API keys/PII, verify no sensitive
      data leakage in production logs
  - criterion: Log levels are configurable per environment with production using INFO+
      and development using DEBUG
    verification: Set LOG_LEVEL=debug in dev, verify debug logs appear; set LOG_LEVEL=info
      in prod, verify debug logs filtered
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/plugins/logging.test.ts
      coverage_target: 90%
      scenarios:
      - Correlation ID generation and propagation
      - Sensitive data redaction
      - Log level filtering
      - Request/response logging format
    - file: packages/backend/src/__tests__/plugins/metrics.test.ts
      coverage_target: 85%
      scenarios:
      - Prometheus metrics registration
      - Custom metrics for ML operations
      - Metric labels and values
      - Performance overhead measurement
    - file: packages/backend/src/__tests__/lib/logger.test.ts
      coverage_target: 95%
      scenarios:
      - Logger configuration per environment
      - Structured log format validation
      - Child logger creation with context
    integration_tests:
    - file: packages/backend/src/__tests__/integration/observability.test.ts
      scenarios:
      - End-to-end request tracing through ML pipeline
      - Health check with real dependency calls
      - Metrics collection during ML operations
      - OpenTelemetry span creation and propagation
    manual_testing:
    - step: Start backend with LOG_LEVEL=debug, make text generation request
      expected: See correlation ID in all log entries, no sensitive content in logs
    - step: Access /metrics endpoint and check for morpheus_* metrics
      expected: Prometheus format metrics with proper labels and values
    - step: Access /health endpoint while Supabase is down
      expected: Partial degradation status with specific dependency failures
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Install dependencies (@opentelemetry/*, pino, prometheus-client, fastify
        plugins)
      done: false
    - task: Create logger configuration with environment-based levels and sensitive
        data redaction
      done: false
    - task: Implement Fastify logging plugin with correlation ID generation and request/response
        logging
      done: false
    - task: Setup Prometheus metrics plugin with custom ML operation metrics
      done: false
    - task: Configure OpenTelemetry tracing for distributed request correlation
      done: false
    - task: Build health check endpoint with dependency monitoring (Supabase, external
        APIs)
      done: false
    - task: Instrument ML services with correlation ID propagation and performance
        metrics
      done: false
    - task: Add sensitive data redaction to all log outputs
      done: false
    - task: Create comprehensive test suite for all observability components
      done: false
    - task: Document observability configuration and debugging procedures
      done: false
- key: T31
  title: Query Performance Optimization
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 3
  area: backend
  dependsOn:
  - T24
  agent_notes:
    research_findings: '**Context:**

      Query Performance Optimization is critical for Morpheus as the platform will
      handle computationally expensive operations like novel parsing, comic panel
      generation, and user management at scale. Poor database performance will directly
      impact user experience during story processing, dashboard loading, and storefront
      browsing. This task establishes foundational performance patterns before M1
      backend services go live, preventing costly refactoring later.


      **Technical Approach:**

      - Implement Supabase query optimization with proper indexing strategies for
      novel content, user data, and comic metadata

      - Use Fastify 5''s built-in caching mechanisms with Redis for frequent queries
      (user sessions, comic thumbnails, processing status)

      - Implement connection pooling with pg-pool for PostgreSQL connections

      - Add query monitoring with @supabase/postgrest-js optimization patterns

      - Implement cursor-based pagination for large datasets (novel chapters, comic
      panels)

      - Use prepared statements and parameterized queries to prevent injection and
      improve performance

      - Add database query logging and performance monitoring with Fastify metrics


      **Dependencies:**

      - External: ioredis, @fastify/redis, @supabase/supabase-js, pg-pool, @fastify/caching

      - Internal: Database schema design, authentication service, file storage service


      **Risks:**

      - Over-optimization: Premature optimization could add complexity without measurable
      benefits

      - Cache invalidation: Stale data in Redis could show outdated processing status
      or comic content

      - Index bloat: Too many indexes could slow down write operations for novel uploads

      - Connection limits: PostgreSQL connection exhaustion under high concurrent
      loads


      **Complexity Notes:**

      Higher complexity than initially estimated due to Morpheus''s unique data patterns
      - novel text processing creates variable-length content, comic generation involves
      large binary data, and real-time status updates require careful cache management.
      The ML integration adds another layer of query complexity for training data
      and model outputs.


      **Key Files:**

      - packages/backend/src/plugins/database.ts: Connection pooling and query optimization
      setup

      - packages/backend/src/services/cache.ts: Redis caching layer implementation

      - packages/backend/src/routes/novels/: Query optimization for novel content
      endpoints

      - packages/backend/src/routes/comics/: Pagination and caching for comic data

      - packages/backend/src/middleware/metrics.ts: Query performance monitoring

      - supabase/migrations/: Database indexes and constraints optimization

      '
    design_decisions:
    - decision: Use Redis for caching with Fastify's built-in caching plugin
      rationale: Fastify 5's native caching integrates seamlessly with the existing
        architecture and provides better performance than external solutions
      alternatives_considered:
      - Memcached
      - In-memory Map-based caching
      - Supabase Edge Functions caching
    - decision: Implement cursor-based pagination over offset-based
      rationale: Cursor pagination performs consistently well with large datasets
        and prevents page drift during novel processing
      alternatives_considered:
      - Offset-based pagination
      - Numbered pagination
      - Infinite scroll without pagination
    - decision: Use Supabase RLS policies for query optimization
      rationale: Row Level Security policies push filtering to the database level,
        reducing data transfer and improving security
      alternatives_considered:
      - Application-level filtering
      - View-based access control
      - Separate read/write databases
    researched_at: '2026-02-07T18:56:01.715427'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:33:35.870988'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a three-layer optimization strategy: database-level indexing
      and RLS policies in Supabase, application-level caching with Redis integrated
      through Fastify plugins, and connection pooling for efficient resource utilization.
      Focus on optimizing the most frequent queries first (user authentication, novel
      parsing status, comic thumbnails) using cursor-based pagination and prepared
      statements. Add comprehensive monitoring to identify bottlenecks before they
      impact user experience.

      '
    external_dependencies:
    - name: ioredis
      version: ^5.3.2
      reason: High-performance Redis client for caching novel processing status and
        comic metadata
    - name: '@fastify/redis'
      version: ^6.1.1
      reason: Official Fastify Redis plugin for seamless integration with existing
        backend architecture
    - name: '@fastify/caching'
      version: ^8.0.1
      reason: Built-in Fastify caching mechanisms for HTTP response caching
    - name: pg-pool
      version: ^3.6.1
      reason: PostgreSQL connection pooling to prevent connection exhaustion during
        high load
    - name: '@fastify/metrics'
      version: ^10.3.0
      reason: Performance monitoring and query timing metrics for optimization insights
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register database and cache plugins, add metrics collection
    - path: apps/backend/src/routes/novels/index.ts
      changes: Implement cursor pagination, add caching for novel metadata
    - path: apps/backend/src/routes/novels/[id]/status.ts
      changes: Add Redis caching for processing status with 30s TTL
    - path: apps/backend/src/routes/comics/index.ts
      changes: Implement cursor pagination for comic gallery, cache thumbnails
    - path: apps/backend/src/routes/users/profile.ts
      changes: Add session caching and optimized user data queries
    - path: supabase/migrations/001_initial_schema.sql
      changes: Add performance indexes for novels.title, comics.created_at, users.email
    new_files:
    - path: apps/backend/src/plugins/database.ts
      purpose: Database connection pooling, prepared statements, query optimization
        setup
    - path: apps/backend/src/plugins/cache.ts
      purpose: Redis integration plugin with Fastify, caching strategies
    - path: apps/backend/src/services/cache.ts
      purpose: Cache service with invalidation patterns, TTL management
    - path: apps/backend/src/middleware/metrics.ts
      purpose: Query performance monitoring, slow query detection
    - path: apps/backend/src/utils/pagination.ts
      purpose: Cursor-based pagination utilities for large datasets
    - path: apps/backend/src/types/cache.ts
      purpose: TypeScript interfaces for cache keys, TTL configurations
    - path: supabase/migrations/002_performance_indexes.sql
      purpose: Database indexes for optimized query performance
    - path: apps/backend/src/__tests__/fixtures/large-dataset.ts
      purpose: Test data generators for performance testing scenarios
    - path: tests/load/query-performance.js
      purpose: K6 load testing scripts for database performance validation
  acceptance_criteria:
  - criterion: 'Database queries execute within performance thresholds: <100ms for
      simple queries, <500ms for complex joins, <2s for full-text search'
    verification: Run performance test suite with `npm run test:perf` and check metrics
      dashboard shows all queries under thresholds
  - criterion: Redis caching reduces database load by >70% for frequently accessed
      data (user sessions, comic thumbnails, novel metadata)
    verification: Monitor cache hit ratio in Redis CLI with `redis-cli info stats`
      and verify hit_rate > 0.7
  - criterion: Connection pooling handles concurrent load without connection exhaustion
      up to 100 simultaneous users
    verification: Load test with `k6 run tests/load/concurrent-users.js` and verify
      no connection timeout errors
  - criterion: Cursor-based pagination efficiently handles large datasets (>10k novels,
      >50k comic panels) with consistent response times
    verification: Test pagination endpoints with large datasets and verify response
      times remain <300ms across all pages
  - criterion: Query monitoring captures performance metrics and identifies slow queries
      automatically
    verification: Check Fastify metrics endpoint `/metrics` shows query duration histograms
      and slow query alerts trigger for queries >1s
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/cache.test.ts
      coverage_target: 90%
      scenarios:
      - Cache hit/miss scenarios
      - Cache invalidation patterns
      - Redis connection failures
      - TTL expiration handling
    - file: apps/backend/src/__tests__/plugins/database.test.ts
      coverage_target: 85%
      scenarios:
      - Connection pool initialization
      - Query execution with prepared statements
      - Connection pool exhaustion handling
      - Database reconnection logic
    integration_tests:
    - file: apps/backend/src/__tests__/integration/query-optimization.test.ts
      scenarios:
      - Novel content queries with caching
      - Comic pagination with cursor-based navigation
      - User authentication with session caching
      - Real-time status updates with cache invalidation
    - file: apps/backend/src/__tests__/integration/performance.test.ts
      scenarios:
      - Concurrent query execution under load
      - Memory usage during large dataset operations
      - Cache performance with high throughput
    manual_testing:
    - step: Upload novel and monitor processing status queries
      expected: Status updates appear within 100ms, cached after first query
    - step: Browse comic gallery with 1000+ items using pagination
      expected: Consistent load times <300ms per page, smooth cursor navigation
    - step: Simulate 50 concurrent user sessions
      expected: No connection timeouts, cache hit ratio >70%
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup Redis configuration and Fastify plugins integration
      done: false
    - task: Implement database connection pooling with pg-pool
      done: false
    - task: Create cache service with invalidation strategies
      done: false
    - task: Add performance indexes to Supabase schema
      done: false
    - task: Implement cursor-based pagination for novels and comics routes
      done: false
    - task: Add query monitoring middleware with metrics collection
      done: false
    - task: Optimize authentication and user session queries with caching
      done: false
    - task: Create performance testing suite with load scenarios
      done: false
    - task: Document caching strategies and performance guidelines
      done: false
    - task: Code review and performance validation
      done: false
- key: T32
  title: API Documentation & OpenAPI
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 2
  area: backend
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      API documentation and OpenAPI specification is crucial for Morpheus as a platform
      that will have multiple consumers - the Next.js dashboard/storefront frontends,
      potential mobile apps, third-party integrations, and developer partners. Given
      that Morpheus transforms novels to comics using ML services, having well-documented
      APIs ensures smooth integration between services and enables future scaling.
      This task provides the foundation for API contracts, client SDK generation,
      and developer onboarding.


      **Technical Approach:**

      Leverage Fastify''s excellent OpenAPI ecosystem with @fastify/swagger and @fastify/swagger-ui
      for automatic spec generation from route schemas. Use JSON Schema for request/response
      validation that doubles as documentation. Implement a documentation-first approach
      where schemas drive both validation and API docs. Generate TypeScript types
      from OpenAPI specs to ensure type safety across frontend/backend boundaries.
      Consider using @apidevtools/swagger-parser for spec validation and potentially
      @openapitools/openapi-generator-cli for client SDK generation.


      **Dependencies:**

      - External: @fastify/swagger, @fastify/swagger-ui, @apidevtools/swagger-parser,
      @openapitools/openapi-generator-cli

      - Internal: All existing Fastify route handlers, authentication middleware,
      database schemas (Supabase types), ML service integration endpoints


      **Risks:**

      - Schema drift: API implementations diverging from docs over time - mitigation:
      automated schema validation in CI/CD

      - Performance overhead: Large OpenAPI specs affecting startup time - mitigation:
      lazy loading and spec caching

      - Breaking changes: Unintentional API changes breaking frontends - mitigation:
      semantic versioning and contract testing

      - Security exposure: Accidentally documenting internal endpoints - mitigation:
      separate public/internal API documentation


      **Complexity Notes:**

      This is moderately complex due to the need to retrofit existing routes with
      proper schemas and the interconnected nature of Morpheus APIs (novel processing,
      comic generation, user management, ML orchestration). The task becomes more
      complex when considering client SDK generation and maintaining documentation
      across multiple API versions.


      **Key Files:**

      - packages/backend/src/app.ts: Register swagger plugins and configure OpenAPI
      options

      - packages/backend/src/routes/**: Add comprehensive JSON schemas to all route
      definitions

      - packages/backend/src/schemas/**: Create shared schema definitions for common
      types

      - packages/backend/src/plugins/swagger.ts: Custom swagger configuration and
      transformations

      - packages/shared/types/**: Generated TypeScript types from OpenAPI specs

      '
    design_decisions:
    - decision: Use Fastify's native JSON Schema integration with @fastify/swagger
      rationale: Provides automatic OpenAPI generation from existing route schemas,
        reduces maintenance burden, and ensures documentation stays in sync with implementation
      alternatives_considered:
      - Manual OpenAPI spec writing
      - External tools like Stoplight
      - GraphQL with introspection
    - decision: Generate TypeScript client types from OpenAPI specs
      rationale: Ensures type safety between frontend and backend, catches breaking
        changes at compile time, and improves developer experience
      alternatives_considered:
      - Manual type definitions
      - Runtime type checking only
      - Separate frontend/backend types
    - decision: Implement API versioning through URL paths (/v1/, /v2/)
      rationale: Explicit versioning supports backward compatibility as Morpheus evolves,
        essential for platform stability
      alternatives_considered:
      - Header-based versioning
      - No versioning
      - Query parameter versioning
    researched_at: '2026-02-07T18:56:24.592912'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:34:02.572407'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement comprehensive API documentation by integrating @fastify/swagger
      with existing Fastify routes, adding JSON Schema definitions to all endpoints
      for automatic OpenAPI spec generation. Create a shared schema library for common
      types (User, Novel, Comic, etc.) that can be reused across routes. Set up automated
      TypeScript type generation from OpenAPI specs to maintain type safety between
      frontend and backend. Establish API versioning strategy and documentation hosting
      through Fastify''s swagger-ui plugin.

      '
    external_dependencies:
    - name: '@fastify/swagger'
      version: ^8.0.0
      reason: Automatic OpenAPI 3.0 specification generation from Fastify JSON schemas
    - name: '@fastify/swagger-ui'
      version: ^4.0.0
      reason: Interactive API documentation interface hosted within the application
    - name: '@apidevtools/swagger-parser'
      version: ^10.1.0
      reason: OpenAPI spec validation and dereferencing for build-time checks
    - name: openapi-typescript
      version: ^7.0.0
      reason: Generate TypeScript types from OpenAPI specs for frontend consumption
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register @fastify/swagger and @fastify/swagger-ui plugins with configuration
    - path: packages/backend/src/routes/auth/login.ts
      changes: Add comprehensive JSON schemas for login request/response with examples
    - path: packages/backend/src/routes/novels/upload.ts
      changes: Add multipart file upload schema, progress tracking response schema
    - path: packages/backend/src/routes/novels/process.ts
      changes: Add novel processing request schema, ML pipeline status response schemas
    - path: packages/backend/src/routes/comics/generate.ts
      changes: Add comic generation parameters schema, generation status and result
        schemas
    - path: packages/backend/src/routes/user/profile.ts
      changes: Add user profile schemas with proper field validation and examples
    - path: packages/backend/package.json
      changes: Add type generation scripts and OpenAPI tooling dependencies
    new_files:
    - path: packages/backend/src/plugins/swagger.ts
      purpose: Fastify plugin for OpenAPI configuration, custom transformations, and
        security schemes
    - path: packages/backend/src/schemas/index.ts
      purpose: Centralized JSON schema definitions for User, Novel, Comic, ML job
        status, etc.
    - path: packages/backend/src/schemas/auth.ts
      purpose: Authentication-related schemas (login, register, tokens, permissions)
    - path: packages/backend/src/schemas/novel.ts
      purpose: Novel entity schemas (metadata, content, processing status, validation
        rules)
    - path: packages/backend/src/schemas/comic.ts
      purpose: Comic generation schemas (parameters, panels, pages, output formats)
    - path: packages/backend/src/schemas/common.ts
      purpose: Shared/common schemas (pagination, error responses, file uploads, timestamps)
    - path: packages/backend/src/utils/schema-validation.ts
      purpose: Custom schema validators and format checkers for business logic
    - path: packages/shared/types/api.ts
      purpose: Generated TypeScript types from OpenAPI specs for frontend consumption
    - path: scripts/generate-types.js
      purpose: Script to generate TypeScript types from OpenAPI spec using openapi-typescript
  acceptance_criteria:
  - criterion: All API endpoints expose comprehensive OpenAPI 3.0 specifications with
      request/response schemas
    verification: Visit /docs endpoint and verify all routes have complete schema
      definitions with examples
  - criterion: TypeScript types are automatically generated from OpenAPI specs and
      used in frontend
    verification: Run `npm run generate:types` and verify packages/shared/types/api.ts
      contains current API types
  - criterion: API documentation is accessible through interactive Swagger UI with
      working examples
    verification: Navigate to /docs, test API endpoints directly through the UI with
      valid authentication
  - criterion: Schema validation enforces documented API contracts on all endpoints
    verification: Send malformed requests to any endpoint and verify proper validation
      errors with schema references
  - criterion: Public and internal APIs are properly separated in documentation
    verification: Verify /docs shows only public endpoints while /internal/docs shows
      internal service APIs
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/plugins/swagger.test.ts
      coverage_target: 90%
      scenarios:
      - OpenAPI spec generation from route schemas
      - Schema validation for request/response objects
      - Custom transformations and security definitions
      - Error handling for malformed schemas
    - file: packages/backend/src/__tests__/schemas/index.test.ts
      coverage_target: 95%
      scenarios:
      - Shared schema definitions validate correctly
      - Schema composition and inheritance
      - Custom format validators
    integration_tests:
    - file: packages/backend/src/__tests__/integration/api-docs.test.ts
      scenarios:
      - Generated OpenAPI spec matches actual API behavior
      - Swagger UI renders correctly with authentication
      - Type generation produces valid TypeScript
      - API versioning works across different spec versions
    manual_testing:
    - step: Open /docs in browser and test novel upload endpoint
      expected: Interactive form with file upload, proper validation messages, working
        authentication
    - step: Generate TypeScript types and verify in frontend usage
      expected: No TypeScript errors, autocomplete works for API responses
    - step: Test API with invalid payloads using different content types
      expected: Proper 400 errors with detailed schema validation messages
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Install and configure @fastify/swagger, @fastify/swagger-ui, openapi-typescript
        dependencies
      done: false
    - task: Create swagger plugin with OpenAPI 3.0 configuration, security schemes,
        and custom transforms
      done: false
    - task: Build comprehensive schema library for all entity types (User, Novel,
        Comic, etc.)
      done: false
    - task: Retrofit all existing route handlers with proper JSON schemas for validation
        and documentation
      done: false
    - task: Implement API versioning strategy and separate public/internal documentation
        endpoints
      done: false
    - task: Set up automated TypeScript type generation from OpenAPI specs with build
        scripts
      done: false
    - task: Add comprehensive examples and descriptions to all schema definitions
      done: false
    - task: Implement schema validation testing and contract testing between frontend/backend
      done: false
    - task: Configure CI/CD pipeline to validate OpenAPI specs and detect breaking
        changes
      done: false
    - task: Update developer documentation with API usage examples and SDK generation
        instructions
      done: false
- key: T33
  title: Code Review & Standards Process
  type: Task
  milestone: M1 - Backend Services
  iteration: I2
  priority: p1
  effort: 1
  area: backend
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      A robust code review and standards process is critical for Morpheus as we scale
      the development team and ensure consistent, maintainable code quality across
      our multi-service architecture. This task establishes automated and manual review
      processes that catch bugs early, enforce TypeScript/JavaScript standards, maintain
      security practices, and ensure performance considerations for our ML-heavy workloads.
      Without proper standards, our complex monorepo with backend services, frontend
      apps, and ML integrations could quickly become unmaintainable.


      **Technical Approach:**

      Implement a multi-layered approach: (1) Pre-commit hooks with Husky + lint-staged
      for immediate feedback, (2) ESLint/Prettier configuration with custom rules
      for our stack, (3) GitHub Actions workflows for automated PR checks, (4) SonarCloud
      integration for code quality metrics, (5) Automated security scanning with Snyk,
      (6) Performance budgets for bundle analysis, (7) Custom ESLint rules for our
      specific patterns (Fastify routes, Supabase queries, ML service calls). Use
      conventional commits and semantic versioning across the monorepo.


      **Dependencies:**

      - External: eslint, prettier, husky, lint-staged, @typescript-eslint/*, commitizen,
      semantic-release, sonarcloud, snyk

      - Internal: All existing services need linting configuration, CI/CD pipeline
      setup, documentation updates


      **Risks:**

      - Developer friction: too strict rules slow development - mitigation: gradual
      rollout with team feedback

      - Performance overhead: extensive linting on large codebase - mitigation: incremental
      linting, caching strategies

      - Configuration drift: different standards across packages - mitigation: shared
      config packages, automated synchronization

      - False positives in ML code: AI-generated content might trigger unusual patterns
      - mitigation: custom rules for ML workflows


      **Complexity Notes:**

      More complex than initially estimated due to our diverse stack (Fastify + Next.js
      + ML services) requiring different linting strategies. The monorepo structure
      adds complexity for shared configurations. However, existing tooling maturity
      (ESLint 9+, TypeScript 5+) makes implementation more straightforward than building
      custom solutions.


      **Key Files:**

      - packages/eslint-config/: shared ESLint configurations

      - .github/workflows/code-quality.yml: automated PR checks

      - apps/backend/.eslintrc.js: Fastify-specific rules

      - apps/frontend/.eslintrc.js: Next.js-specific rules

      - turbo.json: add lint/format tasks to pipeline

      - .husky/: git hooks configuration

      - sonar-project.properties: code quality metrics

      '
    design_decisions:
    - decision: Shared ESLint configuration package with service-specific overrides
      rationale: Ensures consistency across monorepo while allowing flexibility for
        different application types (Fastify vs Next.js vs ML services)
      alternatives_considered:
      - Single root ESLint config
      - Completely separate configs per service
      - External preset like Airbnb
    - decision: Husky + lint-staged for pre-commit hooks with escape hatch
      rationale: Catches issues early without blocking emergency fixes, provides immediate
        developer feedback
      alternatives_considered:
      - Server-side only validation
      - IDE-only linting
      - Manual review process only
    - decision: SonarCloud integration for technical debt tracking
      rationale: Provides metrics for code coverage, complexity, and security vulnerabilities
        with good GitHub integration
      alternatives_considered:
      - CodeClimate
      - Codacy
      - Self-hosted SonarQube
    researched_at: '2026-02-07T18:56:49.941791'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:34:28.890487'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a shared ESLint configuration package that extends TypeScript-ESLint
      with custom rules for our Fastify/Next.js patterns. Set up pre-commit hooks
      using Husky and lint-staged to run incremental checks. Implement GitHub Actions
      workflows that run comprehensive linting, type checking, and security scans
      on PRs. Integrate SonarCloud for ongoing code quality metrics and technical
      debt tracking. Establish conventional commit standards with automated semantic
      versioning.

      '
    external_dependencies:
    - name: '@typescript-eslint/eslint-plugin'
      version: ^6.0.0
      reason: TypeScript-specific linting rules for our TS-heavy codebase
    - name: eslint-plugin-security
      version: ^1.7.0
      reason: Security vulnerability detection for our API endpoints
    - name: husky
      version: ^8.0.0
      reason: Git hooks management for pre-commit linting
    - name: lint-staged
      version: ^14.0.0
      reason: Run linters only on staged files for performance
    - name: prettier
      version: ^3.0.0
      reason: Code formatting consistency across team
    - name: commitizen
      version: ^4.3.0
      reason: Standardized commit message format
    - name: '@sonarjs/eslint-plugin'
      version: ^0.23.0
      reason: Code smell detection and complexity analysis
    files_to_modify:
    - path: turbo.json
      changes: Add lint, format, type-check tasks to pipeline configuration
    - path: package.json
      changes: Add root-level scripts for lint:all, format:all, prepare (husky install)
    - path: apps/backend/package.json
      changes: Add lint, format, type-check scripts and eslint-config dependency
    - path: apps/frontend/package.json
      changes: Add lint, format, type-check scripts and eslint-config dependency
    - path: .gitignore
      changes: Add SonarCloud and lint cache directories
    new_files:
    - path: packages/eslint-config/package.json
      purpose: Shared ESLint configuration package definition
    - path: packages/eslint-config/src/base.js
      purpose: Base ESLint configuration with TypeScript and common rules
    - path: packages/eslint-config/src/backend.js
      purpose: Backend-specific rules for Fastify, Node.js patterns
    - path: packages/eslint-config/src/frontend.js
      purpose: Frontend-specific rules for React, Next.js patterns
    - path: packages/eslint-config/src/rules/fastify.js
      purpose: Custom ESLint rules for Fastify route patterns
    - path: packages/eslint-config/src/rules/supabase.js
      purpose: Custom rules for Supabase query patterns
    - path: .husky/pre-commit
      purpose: Git pre-commit hook running lint-staged
    - path: .husky/commit-msg
      purpose: Conventional commit message validation
    - path: .github/workflows/code-quality.yml
      purpose: Comprehensive PR checks (lint, type-check, security, quality)
    - path: .github/workflows/release.yml
      purpose: Automated semantic release workflow
    - path: sonar-project.properties
      purpose: SonarCloud project configuration
    - path: .lintstagedrc.js
      purpose: Lint-staged configuration for pre-commit checks
    - path: .commitlintrc.js
      purpose: Conventional commit validation rules
    - path: apps/backend/.eslintrc.js
      purpose: Backend ESLint configuration extending shared config
    - path: apps/frontend/.eslintrc.js
      purpose: Frontend ESLint configuration extending shared config
    - path: .prettierrc.js
      purpose: Shared Prettier configuration
    - path: .prettierignore
      purpose: Files to exclude from Prettier formatting
  acceptance_criteria:
  - criterion: Pre-commit hooks prevent commits with linting errors, type errors,
      or formatting issues
    verification: Attempt to commit code with ESLint errors - should fail with specific
      error messages
  - criterion: GitHub Actions PR workflow runs comprehensive checks (lint, type-check,
      security scan) and reports status
    verification: Create PR with intentional code issues - workflow should fail with
      detailed reports
  - criterion: SonarCloud integration provides code quality metrics with <5% technical
      debt ratio
    verification: Check SonarCloud dashboard shows project metrics and quality gate
      passes
  - criterion: Shared ESLint config enforces consistent standards across backend/frontend
      with custom rules
    verification: Run 'npm run lint' in apps/backend and apps/frontend - both use
      shared config with zero errors
  - criterion: Conventional commits are enforced with automated semantic versioning
    verification: Attempt commit with invalid format - should fail; valid commits
      trigger correct version bumps
  testing:
    unit_tests:
    - file: packages/eslint-config/src/__tests__/rules.test.ts
      coverage_target: 90%
      scenarios:
      - Custom Fastify route patterns detected correctly
      - Supabase query patterns enforced
      - ML service call patterns validated
      - TypeScript strict mode violations caught
    - file: packages/eslint-config/src/__tests__/config.test.ts
      coverage_target: 85%
      scenarios:
      - Backend config extends base with Fastify rules
      - Frontend config extends base with React/Next.js rules
      - Shared config loads without conflicts
    integration_tests:
    - file: __tests__/integration/code-quality-pipeline.test.ts
      scenarios:
      - Full lint pipeline runs on sample backend code
      - Pre-commit hooks integrate with git workflow
      - GitHub Actions workflow completes successfully
    manual_testing:
    - step: Create PR with various code quality issues (formatting, linting, security)
      expected: GitHub Actions fails with specific error reports and comments on PR
    - step: Run semantic-release on main branch after conventional commits
      expected: Version bumped correctly and changelog generated
    - step: Verify SonarCloud dashboard shows metrics after PR merge
      expected: Code coverage, complexity, and quality metrics displayed
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Create packages/eslint-config with base, backend, frontend configurations
      done: false
    - task: Implement custom ESLint rules for Fastify and Supabase patterns
      done: false
    - task: Set up Husky pre-commit hooks with lint-staged
      done: false
    - task: Configure GitHub Actions workflows for PR checks and releases
      done: false
    - task: Integrate SonarCloud for code quality metrics
      done: false
    - task: Add Snyk security scanning to pipeline
      done: false
    - task: Configure conventional commits with semantic-release
      done: false
    - task: Update all package.json files with lint/format scripts
      done: false
    - task: Test pre-commit hooks and GitHub Actions workflows
      done: false
    - task: Document code standards and review process in README
      done: false
- key: T37
  title: Wolne Lektury API Integration
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: ingestion
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      Wolne Lektury (wolnelektury.pl) is Poland''s largest free digital library containing
      public domain Polish literature classics. This integration enables Morpheus
      to automatically ingest high-quality literary works as source material for comic
      transformation, providing users with a curated catalog of classic novels, poems,
      and stories. This solves the cold-start problem by pre-populating the platform
      with culturally significant content while ensuring copyright compliance.


      **Technical Approach:**

      Implement a robust ingestion service using Fastify''s plugin architecture with
      dedicated routes for Wolne Lektury API integration. Use their REST API (wolnelektury.pl/api/)
      to fetch book metadata, full text content, and cover images. Design an ETL pipeline
      with queue-based processing using Redis/BullMQ for handling bulk imports. Store
      normalized book data in Supabase with proper indexing for search and filtering.
      Implement incremental sync to handle updates and new releases from their catalog.


      **Dependencies:**

      - External: axios/node-fetch for API calls, zod for API response validation,
      bull/bullmq for job queuing, cheerio for HTML parsing if needed

      - Internal: Database schemas in Supabase, content ingestion service, search
      indexing service, file storage service for cover images


      **Risks:**

      - Rate limiting: Wolne Lektury API may have undocumented limits; implement exponential
      backoff and respect headers

      - Data format changes: API responses may evolve; use schema validation and graceful
      degradation

      - Large content volumes: Full book texts can be massive; implement streaming
      and chunked processing

      - Character encoding: Polish diacritics require proper UTF-8 handling throughout
      the pipeline


      **Complexity Notes:**

      Initially seems straightforward but complexity increases with scale. The API
      is well-documented but lacks official rate limits documentation. Text processing
      for comic adaptation will require sophisticated chunking strategies. Integration
      touches multiple system components (ingestion, storage, search, user content).


      **Key Files:**

      - apps/backend/src/plugins/ingestion/wolne-lektury.ts: Main API integration
      service

      - apps/backend/src/routes/admin/ingestion.ts: Admin endpoints for triggering
      imports

      - packages/database/src/schemas/books.sql: Book metadata schema

      - apps/backend/src/services/content-processor.ts: Text processing and chunking
      logic

      '
    design_decisions:
    - decision: Use incremental sync with last-modified tracking
      rationale: Wolne Lektury updates their catalog regularly; full re-imports would
        be wasteful and slow
      alternatives_considered:
      - Full periodic sync
      - Webhook-based updates (not available)
      - Manual imports only
    - decision: Store full text in PostgreSQL with proper chunking
      rationale: Enables fast search and comic generation without external dependencies;
        PostgreSQL handles large text well
      alternatives_considered:
      - External text storage service
      - File-based storage
      - Vector database for embeddings
    - decision: Queue-based processing with job priorities
      rationale: Large catalog requires async processing; priorities allow featuring
        popular books first
      alternatives_considered:
      - Synchronous processing
      - Batch processing without queues
      - Stream processing
    researched_at: '2026-02-07T18:57:15.054822'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:34:54.418228'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a Fastify plugin that wraps the Wolne Lektury REST API with
      proper error handling, rate limiting, and response validation using Zod schemas.
      Implement a job queue system using BullMQ to process book imports asynchronously,
      storing metadata in Supabase and full text content in chunked format optimized
      for LLM processing. Build admin endpoints for triggering imports and monitoring
      progress, with incremental sync capabilities to efficiently handle catalog updates.

      '
    external_dependencies:
    - name: axios
      version: ^1.6.0
      reason: HTTP client for Wolne Lektury API calls with retry capabilities
    - name: zod
      version: ^3.22.0
      reason: Runtime validation of API responses and data transformation
    - name: bullmq
      version: ^5.0.0
      reason: Redis-based job queue for async book processing
    - name: cheerio
      version: ^1.0.0-rc.12
      reason: HTML parsing if API returns formatted content that needs cleaning
    - name: iconv-lite
      version: ^0.6.3
      reason: Character encoding handling for Polish text content
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register wolne-lektury ingestion plugin and admin routes
    - path: packages/database/src/schema.sql
      changes: Add books, book_chunks, and sync_metadata tables with indexes
    - path: apps/backend/package.json
      changes: 'Add dependencies: axios, bullmq, cheerio, ioredis, zod'
    new_files:
    - path: apps/backend/src/plugins/ingestion/wolne-lektury.ts
      purpose: Main Fastify plugin with API client, rate limiting, and job queue setup
    - path: apps/backend/src/services/wolne-lektury-api.ts
      purpose: API client service with request/response handling and validation
    - path: apps/backend/src/services/content-processor.ts
      purpose: Text processing, chunking, and content optimization for LLM consumption
    - path: apps/backend/src/routes/admin/ingestion.ts
      purpose: Admin endpoints for triggering imports, monitoring progress, and managing
        sync
    - path: apps/backend/src/jobs/wolne-lektury-import.ts
      purpose: BullMQ job processors for handling book import tasks
    - path: apps/backend/src/schemas/wolne-lektury.ts
      purpose: Zod schemas for API response validation and type safety
    - path: packages/database/src/migrations/003_wolne_lektury_tables.sql
      purpose: Database migration for book-related tables and indexes
    - path: apps/frontend/src/pages/admin/ingestion.tsx
      purpose: Admin UI for managing ingestion processes and monitoring status
  acceptance_criteria:
  - criterion: Successfully fetch and store book metadata from Wolne Lektury API with
      proper schema validation
    verification: 'Run `npm test -- wolne-lektury.test.ts` and verify API integration
      tests pass. Check Supabase books table contains expected fields: title, author,
      publication_date, genre, language, etc.'
  - criterion: Process and chunk full-text content for books with progress tracking
      and error handling
    verification: Trigger import via POST /admin/ingestion/wolne-lektury endpoint,
      monitor job queue status, verify book content is stored in chunks table with
      proper text segmentation
  - criterion: Implement rate limiting and exponential backoff to respect API limits
      without failures
    verification: Run load test importing 100+ books simultaneously, verify no 429
      errors and exponential backoff logs appear. Check Redis rate limiter keys.
  - criterion: Support incremental sync to update existing books and add new releases
    verification: Run initial import, then run sync again - verify only new/updated
      books are processed. Check sync_metadata table for last_sync timestamps.
  - criterion: Admin interface displays import progress, statistics, and error handling
    verification: Access /admin/ingestion dashboard, trigger import, verify real-time
      progress updates, job status display, and error reporting UI
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/wolne-lektury-api.test.ts
      coverage_target: 90%
      scenarios:
      - API response parsing and validation
      - Rate limiting logic
      - Error handling for network failures
      - Text chunking algorithms
      - Incremental sync logic
    - file: apps/backend/src/__tests__/services/content-processor.test.ts
      coverage_target: 85%
      scenarios:
      - Text chunking with proper boundaries
      - Polish character encoding handling
      - Large content streaming
    integration_tests:
    - file: apps/backend/src/__tests__/integration/wolne-lektury-ingestion.test.ts
      scenarios:
      - End-to-end book import flow
      - Job queue processing with Redis
      - Database transactions and rollbacks
      - File storage integration for covers
    - file: apps/backend/src/__tests__/integration/admin-ingestion-routes.test.ts
      scenarios:
      - Admin endpoint authentication
      - Import triggering and status monitoring
      - Bulk operations handling
    manual_testing:
    - step: Access admin panel at /admin/ingestion and trigger Wolne Lektury import
      expected: Progress bar appears, job status updates in real-time, books appear
        in database
    - step: Import a book with Polish diacritics (ą, ć, ę, ł, ń, ó, ś, ź, ż)
      expected: Characters display correctly in database and API responses
    - step: Simulate network failure during import
      expected: Jobs retry with exponential backoff, graceful error handling, no data
        corruption
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup database schema and migrations for books, chunks, and sync metadata
      done: false
    - task: Implement Wolne Lektury API client with rate limiting and error handling
      done: false
    - task: Create content processor service for text chunking and optimization
      done: false
    - task: Build BullMQ job queue system for async processing
      done: false
    - task: Develop admin API endpoints for import management
      done: false
    - task: Create admin UI for monitoring and controlling imports
      done: false
    - task: Implement incremental sync logic and scheduling
      done: false
    - task: Add comprehensive error handling and logging
      done: false
    - task: Write unit and integration tests with high coverage
      done: false
    - task: Document API endpoints and admin procedures
      done: false
- key: T38
  title: Book Upload Handler
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: ingestion
  dependsOn:
  - T25
  agent_notes:
    research_findings: '**Context:**

      The Book Upload Handler is a critical component for content ingestion that enables
      users to upload novel files (PDF, EPUB, TXT, DOCX) through the dashboard and
      processes them for comic transformation. This solves the core problem of getting
      user content into the Morpheus pipeline, validating file integrity, extracting
      text content, and preparing it for chapter segmentation and comic generation.
      Without this, users cannot feed their novels into the transformation system.


      **Technical Approach:**

      Implement a multi-part file upload endpoint using Fastify''s multipart plugin
      with streaming support. Use a service-oriented architecture with dedicated classes
      for FileValidator, ContentExtractor, and BookProcessor. Leverage Supabase Storage
      for file persistence and PostgreSQL for metadata storage. Implement async processing
      with job queues for large files to prevent request timeouts. Use strong typing
      with Zod schemas for validation and proper error handling with custom error
      classes.


      **Dependencies:**

      - External: @fastify/multipart, pdf-parse, epub2, mammoth (DOCX), zod, mime-types,
      sharp (thumbnails)

      - Internal: database service, storage service, job queue system, user authentication
      middleware


      **Risks:**

      - Large file uploads (>100MB): Implement streaming uploads with progress tracking
      and chunked processing

      - Memory exhaustion from PDF/EPUB parsing: Use streaming parsers and implement
      memory limits

      - File format security vulnerabilities: Strict MIME type validation, file signature
      verification, sandboxed processing

      - Concurrent upload limits: Implement rate limiting and queue management per
      user

      - Storage costs from abandoned uploads: Implement cleanup jobs and upload expiration


      **Complexity Notes:**

      More complex than initially estimated due to multiple file format support requirements
      and the need for robust content extraction. The streaming aspects and proper
      error recovery add significant complexity. However, the well-defined scope and
      existing Fastify/Supabase infrastructure reduce integration complexity.


      **Key Files:**

      - packages/api/src/routes/books/upload.ts: Main upload endpoint

      - packages/api/src/services/BookUploadService.ts: Core upload logic

      - packages/api/src/services/ContentExtractorService.ts: File parsing logic

      - packages/api/src/lib/validators/book-upload.ts: Zod validation schemas

      - packages/shared/types/book.ts: Type definitions

      '
    design_decisions:
    - decision: Use streaming multipart uploads with Fastify native support
      rationale: Handles large files efficiently, integrates well with existing Fastify
        setup, provides progress tracking capabilities
      alternatives_considered:
      - Direct base64 upload
      - External upload service (AWS S3 direct)
      - Chunked upload implementation
    - decision: Implement synchronous upload + asynchronous processing pattern
      rationale: Provides immediate feedback to users while preventing timeout issues
        for large file processing
      alternatives_considered:
      - Fully synchronous processing
      - Fully asynchronous with webhooks
      - WebSocket real-time updates
    - decision: Store original files in Supabase Storage with extracted content in
        PostgreSQL
      rationale: Leverages existing infrastructure, keeps large binary data separate
        from searchable text content, enables efficient queries
      alternatives_considered:
      - Store everything in PostgreSQL
      - Use separate S3 bucket
      - Store extracted text as files
    researched_at: '2026-02-07T18:57:39.374147'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:35:20.429745'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a Fastify multipart upload route that streams files to temporary
      storage while validating format and size. Implement dedicated service classes
      for content extraction using format-specific parsers (pdf-parse, epub2, mammoth).
      Store original files in Supabase Storage and extracted metadata/content in PostgreSQL
      with proper foreign key relationships. Use async job processing for heavy operations
      like full-text extraction and initial chapter detection to prevent request timeouts.

      '
    external_dependencies:
    - name: '@fastify/multipart'
      version: ^8.0.0
      reason: Native Fastify multipart form support with streaming
    - name: pdf-parse
      version: ^1.1.1
      reason: Reliable PDF text extraction with metadata support
    - name: epub2
      version: ^3.0.2
      reason: EPUB file parsing and chapter extraction
    - name: mammoth
      version: ^1.6.0
      reason: DOCX file processing with formatting preservation
    - name: mime-types
      version: ^2.1.35
      reason: File type validation and MIME type detection
    - name: file-type
      version: ^19.0.0
      reason: File signature validation for security
    files_to_modify:
    - path: packages/api/src/app.ts
      changes: Register multipart plugin and book upload routes
    - path: packages/api/src/middleware/auth.ts
      changes: Add user context extraction for file ownership
    - path: packages/shared/types/api.ts
      changes: Add upload response types and error codes
    - path: packages/database/schema.sql
      changes: Add books table and file_uploads tracking table
    new_files:
    - path: packages/api/src/routes/books/upload.ts
      purpose: Main upload endpoint with multipart handling
    - path: packages/api/src/routes/books/status.ts
      purpose: Upload job status checking endpoint
    - path: packages/api/src/services/BookUploadService.ts
      purpose: Core upload orchestration and file management
    - path: packages/api/src/services/ContentExtractorService.ts
      purpose: Format-specific content parsing and extraction
    - path: packages/api/src/services/FileValidatorService.ts
      purpose: Security validation and format verification
    - path: packages/api/src/lib/validators/book-upload.ts
      purpose: Zod schemas for upload validation
    - path: packages/api/src/lib/storage/SupabaseStorageAdapter.ts
      purpose: File storage abstraction layer
    - path: packages/api/src/lib/jobs/BookProcessingJob.ts
      purpose: Async job handler for content processing
    - path: packages/shared/types/book.ts
      purpose: Book entity types and upload interfaces
    - path: packages/api/src/lib/errors/UploadErrors.ts
      purpose: Custom error classes for upload scenarios
  acceptance_criteria:
  - criterion: System accepts and processes PDF, EPUB, TXT, and DOCX files up to 100MB
    verification: Upload test files of each format via POST /api/books/upload and
      verify 200 response with extracted content
  - criterion: File validation rejects invalid formats and oversized files with appropriate
      error messages
    verification: Upload invalid file types and >100MB files, verify 400 responses
      with specific error codes
  - criterion: Extracted text content is stored in database with proper metadata (title,
      author, word count)
    verification: Query books table after upload to verify content field populated
      and metadata fields accurate
  - criterion: Original files are securely stored in Supabase Storage with proper
      access controls
    verification: Check storage bucket for uploaded files and verify they're only
      accessible to authenticated users
  - criterion: Large file processing happens asynchronously without blocking the upload
      response
    verification: Upload 50MB+ file and verify immediate response with job_id, then
      check job completion via status endpoint
  testing:
    unit_tests:
    - file: packages/api/src/__tests__/services/BookUploadService.test.ts
      coverage_target: 90%
      scenarios:
      - Valid file upload flow
      - File size validation
      - MIME type validation
      - Storage service integration
      - Database persistence
    - file: packages/api/src/__tests__/services/ContentExtractorService.test.ts
      coverage_target: 85%
      scenarios:
      - PDF text extraction
      - EPUB content parsing
      - DOCX document processing
      - TXT file handling
      - Malformed file error handling
    - file: packages/api/src/__tests__/lib/validators/book-upload.test.ts
      coverage_target: 95%
      scenarios:
      - Valid upload payload validation
      - File type validation
      - Size limit validation
      - Required field validation
    integration_tests:
    - file: packages/api/src/__tests__/integration/book-upload.test.ts
      scenarios:
      - Complete upload flow with database persistence
      - File upload with Supabase Storage integration
      - Authentication middleware integration
      - Error handling across service boundaries
    e2e_tests:
    - file: packages/e2e/tests/book-upload.spec.ts
      scenarios:
      - Dashboard file upload workflow
      - Upload progress tracking
      - File processing status updates
    manual_testing:
    - step: Upload each supported file format through dashboard
      expected: Files process successfully with extracted content visible
    - step: Upload files at size limits (99MB, 101MB)
      expected: 99MB succeeds, 101MB fails with clear error message
    - step: Upload corrupted/malicious files
      expected: System rejects with security error, no system compromise
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    security_review: 0.5
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup database schema and migrations for books and file_uploads tables
      done: false
    - task: Install and configure required dependencies (@fastapi/multipart, pdf-parse,
        epub2, mammoth)
      done: false
    - task: Implement FileValidatorService with MIME type and size validation
      done: false
    - task: Create ContentExtractorService with format-specific parsers
      done: false
    - task: Build BookUploadService orchestrating validation, extraction, and storage
      done: false
    - task: Implement upload route with streaming multipart support
      done: false
    - task: Add Supabase Storage integration for file persistence
      done: false
    - task: Create job queue system for async processing of large files
      done: false
    - task: Implement comprehensive error handling and custom error classes
      done: false
    - task: Add authentication middleware integration and user context
      done: false
    - task: Create status endpoint for tracking upload job progress
      done: false
    - task: Write comprehensive test suites (unit, integration, e2e)
      done: false
    - task: Add API documentation and usage examples
      done: false
    - task: Performance testing with large files and concurrent uploads
      done: false
    - task: Security review and penetration testing
      done: false
- key: T39
  title: Chapter Extraction & Parsing
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T38
  agent_notes:
    research_findings: "**Context:**\nChapter extraction and parsing is fundamental\
      \ to the novel-to-comic transformation pipeline. This service needs to intelligently\
      \ segment uploaded novel text into logical chapters, handling various formats\
      \ (plain text, EPUB, DOCX, PDF) and inconsistent chapter markers. It's critical\
      \ for M1 as downstream services (scene analysis, character extraction, comic\
      \ generation) depend on properly segmented content. Without this, the entire\
      \ transformation pipeline fails.\n\n**Technical Approach:**\nImplement a multi-stage\
      \ parsing pipeline using Fastify plugins:\n1. File format detection and content\
      \ extraction (different strategies per format)\n2. Text preprocessing and normalization\
      \ \n3. Chapter boundary detection using regex patterns + LLM validation\n4.\
      \ Metadata extraction (chapter titles, numbers, word counts)\n5. Content validation\
      \ and error handling\n\nUse streaming for large files, implement caching for\
      \ repeated operations, and design for extensibility as chapter formats vary\
      \ widely across novels.\n\n**Dependencies:**\n- External: mammoth (DOCX), pdf-parse\
      \ (PDF), epub2 (EPUB), natural (NLP), zod (validation)\n- Internal: file-upload\
      \ service, database models, queue system for async processing\n\n**Risks:**\n\
      - Memory issues with large novels: Use streaming and chunked processing\n- Inconsistent\
      \ chapter formats: Implement fallback strategies and LLM-assisted detection\n\
      - Performance bottlenecks: Add Redis caching and background job processing\n\
      - Character encoding issues: Normalize to UTF-8 early in pipeline\n\n**Complexity\
      \ Notes:**\nInitially appears straightforward but complexity emerges from format\
      \ diversity and edge cases. Novels have inconsistent chapter markers (\"Chapter\
      \ 1\", \"Ch. 1\", \"ONE\", etc.) and some lack clear boundaries. The LLM integration\
      \ for ambiguous cases adds async complexity but is necessary for quality.\n\n\
      **Key Files:**\n- packages/backend/src/plugins/chapter-parser.ts: Main parser\
      \ plugin\n- packages/backend/src/services/ingestion/: Core extraction logic\n\
      - packages/backend/src/models/chapter.ts: Database schema\n- packages/backend/src/routes/ingestion/chapters.ts:\
      \ API endpoints\n"
    design_decisions:
    - decision: Use multi-format parser with strategy pattern
      rationale: Different file formats require different extraction methods. Strategy
        pattern allows easy extension and testing of each format independently.
      alternatives_considered:
      - Single parser for all formats
      - External service for parsing
    - decision: Hybrid regex + LLM approach for chapter detection
      rationale: Regex handles common patterns efficiently, LLM provides fallback
        for ambiguous cases and validation. Balances performance with accuracy.
      alternatives_considered:
      - Pure regex approach
      - LLM-only approach
      - Rule-based heuristics
    - decision: Streaming processing with chunked analysis
      rationale: Prevents memory issues with large novels while maintaining processing
        speed. Critical for scalability.
      alternatives_considered:
      - Load entire file in memory
      - External processing service
    researched_at: '2026-02-07T18:58:01.374007'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:35:46.843938'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a Fastify plugin that accepts file uploads and returns structured
      chapter data. Use strategy pattern for different file formats (TXT/DOCX/PDF/EPUB),
      each with specific extraction logic. Implement a two-pass system: first pass
      uses regex to identify likely chapter boundaries, second pass validates with
      LLM for ambiguous cases. Stream large files in chunks to prevent memory issues
      and store results in Supabase with proper indexing.

      '
    external_dependencies:
    - name: mammoth
      version: ^1.6.0
      reason: Extract text content from DOCX files while preserving structure
    - name: pdf-parse
      version: ^1.1.1
      reason: Parse PDF files and extract text content reliably
    - name: epub2
      version: ^3.0.2
      reason: Extract chapters and metadata from EPUB format novels
    - name: natural
      version: ^6.10.0
      reason: Text processing utilities for sentence tokenization and content analysis
    - name: iconv-lite
      version: ^0.6.3
      reason: Handle various text encodings and normalize to UTF-8
    - name: file-type
      version: ^18.7.0
      reason: Detect file formats reliably from buffer content
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Register chapter-parser plugin
    - path: packages/backend/src/lib/database.types.ts
      changes: Add Chapter table types from Supabase schema
    new_files:
    - path: packages/backend/src/plugins/chapter-parser.ts
      purpose: Main Fastify plugin for chapter extraction API endpoints
    - path: packages/backend/src/services/ingestion/chapter-extractor.ts
      purpose: Core extraction service with strategy pattern for file formats
    - path: packages/backend/src/services/ingestion/extractors/txt-extractor.ts
      purpose: Plain text file extraction strategy
    - path: packages/backend/src/services/ingestion/extractors/docx-extractor.ts
      purpose: DOCX file extraction using mammoth library
    - path: packages/backend/src/services/ingestion/extractors/pdf-extractor.ts
      purpose: PDF text extraction using pdf-parse library
    - path: packages/backend/src/services/ingestion/extractors/epub-extractor.ts
      purpose: EPUB file extraction using epub2 library
    - path: packages/backend/src/services/ingestion/chapter-detector.ts
      purpose: Chapter boundary detection with regex patterns and LLM fallback
    - path: packages/backend/src/models/chapter.ts
      purpose: Zod schemas and database models for chapter data
    - path: packages/backend/src/routes/ingestion/chapters.ts
      purpose: HTTP route handlers for chapter ingestion endpoints
    - path: packages/backend/src/lib/streaming-utils.ts
      purpose: Utilities for chunked file processing and memory management
    - path: packages/backend/src/jobs/chapter-processing.ts
      purpose: Background job handlers for async chapter processing
  acceptance_criteria:
  - criterion: System successfully extracts and parses chapters from TXT, DOCX, PDF,
      and EPUB files up to 50MB
    verification: Upload test files via POST /api/v1/ingestion/chapters and verify
      200 response with structured chapter data
  - criterion: Chapter detection accuracy >90% for common formats (Chapter N, Ch.
      N, numbered sections)
    verification: Run test suite with 20+ sample novels, measure detected vs expected
      chapter count
  - criterion: Memory usage stays under 512MB when processing large files (20MB+)
    verification: Monitor memory with --max-old-space-size=512 during load tests
  - criterion: Processing completes within 30 seconds for files under 5MB, 2 minutes
      for files under 50MB
    verification: Automated performance tests measuring end-to-end processing time
  - criterion: API returns proper error responses for unsupported formats, corrupted
      files, and oversized uploads
    verification: Test error scenarios and verify appropriate HTTP status codes and
      error messages
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/chapter-extractor.test.ts
      coverage_target: 90%
      scenarios:
      - Text extraction from each file format
      - Chapter boundary detection with various markers
      - Metadata extraction (titles, numbers, word counts)
      - Error handling for corrupted files
      - Memory management with large inputs
    - file: packages/backend/src/__tests__/plugins/chapter-parser.test.ts
      coverage_target: 85%
      scenarios:
      - Plugin registration and configuration
      - Request validation and sanitization
      - Response formatting
    integration_tests:
    - file: packages/backend/src/__tests__/integration/chapter-ingestion.test.ts
      scenarios:
      - End-to-end file upload and chapter extraction
      - Database persistence of chapter data
      - Queue integration for async processing
      - Error propagation through pipeline
    manual_testing:
    - step: Upload a 10MB novel PDF via Postman to /api/v1/ingestion/chapters
      expected: Returns structured JSON with chapters array, processing time logged
    - step: Upload file with no clear chapter markers
      expected: Falls back to paragraph-based segmentation, returns warning in response
    - step: Upload corrupted EPUB file
      expected: Returns 400 error with descriptive message
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup dependencies (mammoth, pdf-parse, epub2, natural) and update package.json
      done: false
    - task: Create database schema for chapters table in Supabase migration
      done: false
    - task: Implement file format detection and base extractor interface
      done: false
    - task: Build format-specific extractors (TXT, DOCX, PDF, EPUB)
      done: false
    - task: Implement chapter boundary detection with regex patterns
      done: false
    - task: Add LLM integration for ambiguous chapter detection
      done: false
    - task: Create streaming utilities and memory management
      done: false
    - task: Build main chapter-extractor service with strategy pattern
      done: false
    - task: Implement Fastify plugin and API routes with validation
      done: false
    - task: Add Redis caching for repeated operations
      done: false
    - task: Setup background job processing for large files
      done: false
    - task: Write comprehensive test suites and run coverage analysis
      done: false
    - task: Performance testing with large files and memory profiling
      done: false
    - task: Create API documentation and usage examples
      done: false
    - task: Code review and refactoring based on feedback
      done: false
- key: T40
  title: Text Preprocessing Pipeline
  type: Task
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T39
  agent_notes:
    research_findings: '**Context:**

      The text preprocessing pipeline is crucial for transforming raw novel text into
      structured, ML-ready content for comic generation. This involves cleaning text,
      segmenting into scenes/panels, extracting dialogue vs narrative, identifying
      characters, and preparing prompts for LLM scene analysis. Without proper preprocessing,
      downstream AI models receive inconsistent input, leading to poor comic panel
      generation and character continuity issues. This is a foundational component
      that directly impacts the quality of the entire novel-to-comic transformation
      process.


      **Technical Approach:**

      - Use a modular pipeline architecture with discrete processing stages

      - Implement text chunking with overlap for context preservation using tiktoken
      for token counting

      - Natural Language Processing via spaCy for NER (character detection) and sentence
      segmentation

      - Regex patterns for dialogue extraction and formatting cleanup

      - Scene boundary detection using ML-based sentence similarity (sentence-transformers)

      - Streaming processing for large novels to manage memory efficiently

      - Cache intermediate results in PostgreSQL for resumability

      - Expose via Fastify REST endpoints with real-time WebSocket progress updates


      **Dependencies:**

      - External: spacy, tiktoken, sentence-transformers, compromise, pdf-parse, mammoth
      (for docx)

      - Internal: Database models for storing processed segments, WebSocket service,
      file upload service


      **Risks:**

      - Memory exhaustion with large files: Use streaming and chunked processing

      - Character name variations/aliases: Build fuzzy matching with Levenshtein distance

      - Context loss at chunk boundaries: Implement overlapping windows with smart
      boundary detection

      - Processing time for large novels: Implement background job queue with progress
      tracking

      - Text encoding issues: Normalize to UTF-8 early in pipeline


      **Complexity Notes:**

      This is more complex than initially apparent due to the need for semantic understanding
      rather than just text manipulation. Character consistency across scenes, maintaining
      narrative flow, and handling various input formats (PDF, DOCX, TXT) adds significant
      complexity. The ML components for scene segmentation require careful prompt
      engineering and model selection.


      **Key Files:**

      - apps/api/src/services/text-processor.ts: Main preprocessing service

      - apps/api/src/models/processed-text.ts: Database schema for segments

      - apps/api/src/routes/ingestion.ts: API endpoints for text upload/processing

      - packages/shared/types/text-processing.ts: Shared TypeScript interfaces

      - apps/api/src/utils/text-chunker.ts: Token-aware text segmentation

      '
    design_decisions:
    - decision: Use spaCy for NLP tasks instead of lighter alternatives
      rationale: Provides robust NER for character detection, dependency parsing for
        dialogue attribution, and proven performance with literary text
      alternatives_considered:
      - Natural (compromise.js)
      - NLTK.js
      - Custom regex-based solution
    - decision: Implement streaming pipeline with PostgreSQL staging tables
      rationale: Enables processing of large novels without memory issues, provides
        resumability, and allows real-time progress tracking for frontend
      alternatives_considered:
      - In-memory processing
      - File-based intermediate storage
      - Redis-based caching
    - decision: Use sentence-transformers for semantic scene boundary detection
      rationale: Provides better scene transitions than rule-based approaches, understands
        context shifts that indicate panel boundaries
      alternatives_considered:
      - Rule-based chapter/paragraph splitting
      - Custom BERT fine-tuning
      - OpenAI embeddings API
    researched_at: '2026-02-07T18:58:27.542920'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:36:15.676630'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a multi-stage pipeline service that accepts raw text files, processes
      them through cleaning/normalization, NLP analysis for character/dialogue extraction,
      semantic segmentation into scenes, and outputs structured data ready for LLM
      comic generation prompts. Use PostgreSQL for intermediate storage, WebSocket
      for progress updates, and implement as background jobs for large files. Each
      stage is modular and resumable to handle failures gracefully.

      '
    external_dependencies:
    - name: spacy
      version: ^3.7.0
      reason: NER for character detection, sentence segmentation, and dialogue attribution
    - name: tiktoken
      version: ^1.0.10
      reason: Accurate token counting for LLM context window management
    - name: sentence-transformers
      version: ^2.2.2
      reason: Semantic similarity for intelligent scene boundary detection
    - name: pdf-parse
      version: ^1.1.1
      reason: Extract text content from PDF novel uploads
    - name: mammoth
      version: ^1.6.0
      reason: Convert DOCX files to clean text while preserving structure
    - name: compromise
      version: ^14.10.0
      reason: Lightweight NLP for text normalization and basic entity recognition
    files_to_modify:
    - path: apps/api/src/models/index.ts
      changes: Add imports for new ProcessedText, TextSegment, and CharacterEntity
        models
    - path: apps/api/src/services/index.ts
      changes: Export TextProcessorService and register with dependency injection
    - path: apps/api/src/routes/index.ts
      changes: Register ingestion routes with main router
    - path: packages/shared/types/index.ts
      changes: Export text processing types for frontend consumption
    new_files:
    - path: apps/api/src/services/text-processor.ts
      purpose: Main preprocessing pipeline orchestrator with stage management
    - path: apps/api/src/models/processed-text.ts
      purpose: Database schema for storing processed text segments and metadata
    - path: apps/api/src/models/character-entity.ts
      purpose: Character extraction and management with alias resolution
    - path: apps/api/src/models/text-segment.ts
      purpose: Individual text segments with type classification and scene boundaries
    - path: apps/api/src/routes/ingestion.ts
      purpose: REST endpoints for file upload, processing triggers, and status queries
    - path: apps/api/src/utils/text-chunker.ts
      purpose: Token-aware text segmentation with overlap and boundary detection
    - path: apps/api/src/utils/nlp-processor.ts
      purpose: NLP utilities for character extraction, dialogue detection, and scene
        analysis
    - path: apps/api/src/utils/file-parser.ts
      purpose: Multi-format file parsing (PDF, DOCX, TXT) with encoding normalization
    - path: packages/shared/types/text-processing.ts
      purpose: TypeScript interfaces for text processing pipeline data structures
    - path: apps/api/src/jobs/text-processing-job.ts
      purpose: Background job implementation for large file processing
    - path: apps/api/src/services/websocket-progress.ts
      purpose: Real-time progress updates via WebSocket for processing status
  acceptance_criteria:
  - criterion: Pipeline processes uploaded text files (TXT, PDF, DOCX) and extracts
      structured content including characters, dialogue, narrative segments, and scene
      boundaries
    verification: Upload test novel file via POST /api/ingestion/process-text, verify
      response contains extracted characters array, dialogue/narrative segments with
      proper tags, and scene boundary markers
  - criterion: System handles large files (>10MB) without memory exhaustion using
      streaming and background job processing
    verification: Upload 50MB+ text file, monitor memory usage stays under 500MB,
      verify WebSocket progress updates, confirm job completes successfully
  - criterion: Character detection identifies main characters with 90%+ accuracy and
      handles name variations/aliases through fuzzy matching
    verification: Process test novel with known character list, verify detected characters
      match expected list within 90% accuracy, test aliases like 'John' vs 'Johnny'
      are grouped correctly
  - criterion: Scene segmentation preserves context at boundaries using overlapping
      windows and semantic similarity scoring
    verification: Verify adjacent scenes share contextual elements, check overlap
      regions contain relevant context, validate scene transitions maintain narrative
      flow
  - criterion: Pipeline is resumable from any stage failure and caches intermediate
      results for re-processing
    verification: Interrupt processing mid-stage, restart pipeline, verify it resumes
      from cached checkpoint without re-processing completed stages
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/text-processor.test.ts
      coverage_target: 90%
      scenarios:
      - Text cleaning and normalization
      - Character extraction with NER
      - Dialogue vs narrative classification
      - Scene boundary detection
      - Error handling for malformed input
      - Memory management for large texts
    - file: apps/api/src/__tests__/utils/text-chunker.test.ts
      coverage_target: 85%
      scenarios:
      - Token-aware chunking with tiktoken
      - Overlap window calculations
      - Boundary detection accuracy
      - Memory efficient streaming
    integration_tests:
    - file: apps/api/src/__tests__/integration/text-pipeline.test.ts
      scenarios:
      - 'Full pipeline: file upload to structured output'
      - Background job processing with progress tracking
      - WebSocket progress updates
      - Database persistence of processed segments
      - Resume from failure scenarios
    manual_testing:
    - step: Upload various file formats (PDF with complex layout, DOCX with formatting,
        large TXT file)
      expected: All formats processed correctly, maintaining text structure and extracting
        content accurately
    - step: Test with novels containing complex dialogue, multiple POVs, and scene
        transitions
      expected: Characters identified consistently, dialogue properly tagged, scene
        boundaries logical
    - step: Monitor system performance during concurrent file processing
      expected: Multiple files processed simultaneously without performance degradation
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup NLP dependencies (spacy, sentence-transformers, tiktoken) and configure
        model downloads
      done: false
    - task: Create database models and migrations for processed text storage
      done: false
    - task: Implement file parsing utilities for PDF, DOCX, TXT formats
      done: false
    - task: Build text chunking service with token counting and overlap logic
      done: false
    - task: Develop NLP processor for character extraction and dialogue classification
      done: false
    - task: Create scene segmentation using semantic similarity
      done: false
    - task: Implement main text processor service with pipeline orchestration
      done: false
    - task: Build REST API endpoints and WebSocket progress service
      done: false
    - task: Add background job queue integration for large file processing
      done: false
    - task: Create comprehensive test suite and performance benchmarks
      done: false
    - task: Write API documentation and usage examples
      done: false
    - task: Conduct code review and security analysis
      done: false
- key: T44
  title: Semantic Search Implementation
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T43
  agent_notes:
    research_findings: '**Context:**

      Semantic search enables users to find novels, comics, and chapters using natural
      language queries rather than exact keyword matching. In the context of Morpheus,
      users should be able to search for "dark fantasy novels with dragons" or "romance
      stories set in medieval times" and get relevant results based on meaning rather
      than just text matching. This is crucial for content discovery as the platform
      scales, especially since novels and comics have rich descriptive metadata (genres,
      themes, character descriptions, plot summaries) that traditional search can''t
      effectively utilize.


      **Technical Approach:**

      Implement vector-based semantic search using OpenAI''s text-embedding-3-small
      model for generating embeddings, PostgreSQL''s pgvector extension for vector
      storage and similarity search, and a hybrid approach combining semantic similarity
      with traditional text search (using PostgreSQL''s full-text search). The architecture
      should include an embedding service for generating vectors, a search service
      with query expansion capabilities, and background jobs for indexing content
      as it''s ingested.


      **Dependencies:**

      - External: [@supabase/supabase-js, openai, pgvector (PostgreSQL extension),
      @fastify/schedule for background jobs]

      - Internal: Novel/Comic ingestion pipeline, Content metadata extraction service,
      Authentication service for user-specific search results


      **Risks:**

      - Embedding generation costs: OpenAI charges per token, could get expensive
      with large content volumes - mitigate by batching requests and caching embeddings

      - Vector search performance: Large vector datasets can slow queries - mitigate
      with proper indexing (HNSW) and result pagination

      - Embedding model changes: OpenAI model updates could invalidate existing vectors
      - mitigate by versioning embeddings and planning migration strategies

      - Query latency: Vector similarity + traditional search could be slow - mitigate
      with proper database indexing and result caching


      **Complexity Notes:**

      More complex than initially estimated due to the hybrid search approach needed.
      Pure semantic search often misses exact matches users expect, so combining with
      traditional search requires careful result ranking and fusion algorithms. The
      embedding generation and storage pipeline also adds operational complexity for
      content updates.


      **Key Files:**

      - apps/backend/src/services/embedding.service.ts: Generate and manage embeddings

      - apps/backend/src/services/search.service.ts: Hybrid search implementation

      - apps/backend/src/routes/search.ts: Search API endpoints

      - packages/database/migrations/: Add vector columns and indexes

      - apps/backend/src/workers/embedding-indexer.ts: Background embedding generation

      '
    design_decisions:
    - decision: Use OpenAI text-embedding-3-small for embedding generation
      rationale: Cost-effective, good performance for general text, integrates with
        existing OpenAI usage in the platform
      alternatives_considered:
      - Sentence Transformers (self-hosted)
      - Cohere embeddings
      - OpenAI text-embedding-3-large
    - decision: Implement hybrid search combining vector similarity with PostgreSQL
        full-text search
      rationale: Pure semantic search can miss exact keyword matches that users expect,
        hybrid approach provides better user experience
      alternatives_considered:
      - Pure vector search
      - Elasticsearch with vector plugin
      - Pure PostgreSQL FTS
    - decision: Use Supabase/PostgreSQL with pgvector extension for vector storage
      rationale: Keeps vector data co-located with relational data, reduces infrastructure
        complexity, leverages existing Supabase setup
      alternatives_considered:
      - Pinecone vector database
      - Weaviate
      - Qdrant
    researched_at: '2026-02-07T18:58:51.773353'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:36:38.705636'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a three-layer approach: (1) Embedding generation service
      that creates vectors for novel/comic content during ingestion, storing them
      in PostgreSQL with pgvector, (2) Search service that performs hybrid queries
      combining vector similarity (cosine distance) with traditional full-text search,
      and (3) Result fusion algorithm that ranks and merges results from both approaches.
      Include background workers for batch embedding generation and API endpoints
      for real-time search with proper caching and pagination.

      '
    external_dependencies:
    - name: openai
      version: ^4.28.0
      reason: Generate text embeddings for semantic search
    - name: pgvector
      version: ^0.5.0
      reason: PostgreSQL extension for vector similarity search
    - name: '@fastify/schedule'
      version: ^2.0.0
      reason: Background job scheduling for embedding generation
    - name: postgres
      version: ^3.4.0
      reason: Direct PostgreSQL queries for vector operations
    files_to_modify:
    - path: packages/database/src/schema.sql
      changes: Add embedding columns (vector(1536)) to novels and comics tables, create
        HNSW indexes
    - path: apps/backend/src/server.ts
      changes: Register search routes and embedding worker scheduler
    - path: packages/database/src/types.ts
      changes: Add embedding fields to Novel and Comic type definitions
    new_files:
    - path: apps/backend/src/services/embedding.service.ts
      purpose: Generate and manage text embeddings using OpenAI API, handle batching
        and caching
    - path: apps/backend/src/services/search.service.ts
      purpose: Implement hybrid semantic + text search with result fusion algorithms
    - path: apps/backend/src/routes/search.ts
      purpose: RESTful search endpoints with pagination and filtering
    - path: apps/backend/src/workers/embedding-indexer.ts
      purpose: Background worker to generate embeddings for newly ingested content
    - path: packages/database/migrations/202401_add_vector_search.sql
      purpose: Database migration to add pgvector extension and embedding columns
    - path: apps/backend/src/lib/search-fusion.ts
      purpose: Algorithm to merge and rank results from semantic and text search
    - path: apps/backend/src/config/search.config.ts
      purpose: Search-specific configuration including OpenAI API keys and vector
        dimensions
  acceptance_criteria:
  - criterion: Semantic search API returns relevant results for natural language queries
    verification: POST /api/v1/search with query 'dark fantasy novels with dragons'
      returns novels tagged with fantasy/dragon genres, measured by precision@10 >
      0.7
  - criterion: Hybrid search combines semantic and text-based results effectively
    verification: Search for exact title matches appear in top 3 results, while thematically
      similar content fills remaining positions
  - criterion: Search performance meets latency requirements
    verification: 95th percentile response time < 500ms for queries with 1000+ indexed
      items, measured via load testing
  - criterion: Background embedding generation processes new content automatically
    verification: New novels/comics get embeddings within 5 minutes of ingestion,
      verified by checking embedding_status in database
  - criterion: Vector storage and retrieval functions correctly
    verification: pgvector similarity queries return expected nearest neighbors with
      cosine similarity scores, tested via direct database queries
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/embedding.service.test.ts
      coverage_target: 90%
      scenarios:
      - Generate embeddings for novel metadata
      - Handle OpenAI API errors gracefully
      - Batch embedding requests efficiently
      - Cache embedding results
    - file: apps/backend/src/__tests__/services/search.service.test.ts
      coverage_target: 85%
      scenarios:
      - Semantic search returns ranked results
      - Hybrid search merges vector and text results
      - Query preprocessing and expansion
      - Empty/invalid query handling
    integration_tests:
    - file: apps/backend/src/__tests__/integration/search-flow.test.ts
      scenarios:
      - End-to-end search from API to database
      - Background embedding worker processes queue
      - Database vector operations with pgvector
    manual_testing:
    - step: Search for 'romantic comedy manga' via API
      expected: Returns manga/comics with romance and comedy genres in top results
    - step: Search for exact novel title
      expected: Exact match appears as first result
    - step: Add new novel and wait 5 minutes
      expected: New content becomes searchable via semantic queries
  estimates:
    development: 4.5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Setup pgvector extension and create database migration
      done: false
    - task: Implement embedding service with OpenAI integration
      done: false
    - task: Create vector storage operations and HNSW indexes
      done: false
    - task: Build hybrid search service with result fusion
      done: false
    - task: Implement search API endpoints with pagination
      done: false
    - task: Create background worker for embedding generation
      done: false
    - task: Add comprehensive unit and integration tests
      done: false
    - task: Performance testing and optimization
      done: false
    - task: API documentation and usage examples
      done: false
    - task: Code review and security audit
      done: false
- key: T45
  title: Book Status & Progress Tracking
  type: Feature
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: ingestion
  dependsOn:
  - T38
  agent_notes:
    research_findings: '**Context:**

      Book status & progress tracking is critical for the novel-to-comic transformation
      pipeline. Users need visibility into where their book is in the processing workflow
      (uploaded, analyzing, generating scenes, creating artwork, etc.), while the
      system needs robust state management to handle failures, retries, and resume
      operations. This enables better UX through progress indicators and reliable
      processing through state persistence.


      **Technical Approach:**

      Implement a state machine pattern using a PostgreSQL-backed status system with
      real-time updates via Supabase subscriptions. Create an event-driven architecture
      where each processing stage emits progress events that update both database
      state and notify frontend clients. Use enum-based status types for type safety
      and implement progress tracking with percentage completion and detailed substep
      information.


      **Dependencies:**

      - External: [@xstate/fsm, ioredis, bull/bullmq]

      - Internal: [supabase client, ML processing services, ingestion pipeline, dashboard
      components]


      **Risks:**

      - State inconsistency: Implement atomic updates and event sourcing with rollback
      capabilities

      - Real-time update performance: Use Redis for high-frequency updates, batch
      database writes

      - Complex state transitions: Define clear FSM with validation rules and transition
      guards

      - Long-running process failures: Implement heartbeat mechanism and timeout detection


      **Complexity Notes:**

      More complex than initially estimated due to need for robust failure handling,
      real-time updates, and coordination across multiple async ML services. The state
      machine aspect adds architectural complexity but provides better reliability.


      **Key Files:**

      - packages/backend/src/services/BookStatusService.ts: Core status management
      logic

      - packages/backend/src/models/BookStatus.ts: Database schema and types

      - packages/backend/src/routes/books/status.ts: REST API endpoints

      - packages/database/migrations/: New status and progress tables

      - packages/frontend/src/components/ProgressTracker.tsx: Real-time progress UI

      '
    design_decisions:
    - decision: Use finite state machine pattern with PostgreSQL persistence
      rationale: Provides predictable state transitions, easy debugging, and reliable
        persistence with ACID guarantees
      alternatives_considered:
      - Event sourcing only
      - Simple status flags
      - Redis-only state
    - decision: Hybrid real-time updates (Redis + Supabase subscriptions)
      rationale: Redis handles high-frequency ML progress updates, Supabase provides
        reliable real-time UI updates
      alternatives_considered:
      - Supabase only
      - WebSocket implementation
      - Polling-based updates
    - decision: Hierarchical progress tracking (stage + substeps)
      rationale: Provides detailed progress visibility while maintaining simple high-level
        status
      alternatives_considered:
      - Simple percentage only
      - Event log based
      - Milestone checkpoints
    researched_at: '2026-02-07T18:59:13.894894'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:37:06.244940'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a BookStatusService that manages a finite state machine with
      states like UPLOADED, ANALYZING, GENERATING_SCENES, CREATING_ARTWORK, COMPLETED,
      FAILED. Each ML processing service publishes progress events to Redis, which
      the status service consumes to update PostgreSQL state and trigger Supabase
      real-time notifications. Implement progress tracking with both overall percentage
      and detailed substep information, using database transactions for atomic state
      updates and retry mechanisms for failed transitions.

      '
    external_dependencies:
    - name: '@xstate/fsm'
      version: ^2.0.0
      reason: Lightweight finite state machine for managing book processing states
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for high-frequency progress updates and pub/sub
    - name: bullmq
      version: ^4.15.0
      reason: Queue management with built-in progress tracking and job state persistence
    - name: zod
      version: ^3.22.4
      reason: Runtime validation for status transitions and progress data
    files_to_modify:
    - path: packages/backend/src/app.ts
      changes: Add BookStatusService initialization and Redis connection setup
    - path: packages/backend/src/routes/books/index.ts
      changes: Import and mount status routes at /books/:id/status
    - path: packages/backend/src/services/MLProcessingService.ts
      changes: Add progress event publishing to Redis at each processing step
    - path: packages/shared/src/types/Book.ts
      changes: Add BookStatus enum and BookProgress interface definitions
    - path: packages/frontend/src/hooks/useBookStatus.ts
      changes: Add Supabase subscription for real-time status updates
    new_files:
    - path: packages/backend/src/services/BookStatusService.ts
      purpose: Core status management with finite state machine logic and Redis integration
    - path: packages/backend/src/models/BookStatus.ts
      purpose: PostgreSQL schema definitions and database operations for book status
    - path: packages/backend/src/routes/books/status.ts
      purpose: REST API endpoints for status queries and manual status operations
    - path: packages/backend/src/events/BookStatusEvents.ts
      purpose: Event type definitions and Redis pub/sub message schemas
    - path: packages/database/migrations/20240115_add_book_status_tables.sql
      purpose: Create book_status and book_progress tables with indexes
    - path: packages/frontend/src/components/ProgressTracker.tsx
      purpose: Real-time progress visualization with progress bars and status messages
    - path: packages/frontend/src/components/StatusIndicator.tsx
      purpose: Book status badge/indicator component with state-specific styling
    - path: packages/backend/src/lib/StateMachine.ts
      purpose: Generic finite state machine implementation with validation and guards
  acceptance_criteria:
  - criterion: Book status progresses through defined states (UPLOADED → ANALYZING
      → GENERATING_SCENES → CREATING_ARTWORK → COMPLETED) with atomic state transitions
    verification: Run integration test `npm run test -- BookStatusService.integration.test.ts`
      and verify state machine transitions work correctly
  - criterion: Real-time progress updates are delivered to frontend clients within
      500ms of backend state changes
    verification: 'Manual test: Upload book, monitor WebSocket messages in browser
      dev tools, measure time between backend log and frontend update'
  - criterion: System recovers from failures and resumes processing from last known
      good state without data loss
    verification: Integration test simulating service crashes at each state, verify
      resume functionality with `npm run test -- failure-recovery.test.ts`
  - criterion: Progress tracking shows overall percentage (0-100%) and detailed substep
      information for each processing stage
    verification: Check API response at `GET /api/books/{id}/status` contains `overall_progress`,
      `current_stage`, and `substeps` fields
  - criterion: System handles concurrent status updates for multiple books without
      race conditions
    verification: Load test with `k6 run tests/load/concurrent-status-updates.js`
      processing 10+ books simultaneously
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/BookStatusService.test.ts
      coverage_target: 90%
      scenarios:
      - State machine transitions (happy path)
      - Invalid state transition rejection
      - Progress update calculations
      - Event emission on state changes
      - Error handling and rollback
    - file: packages/backend/src/__tests__/models/BookStatus.test.ts
      coverage_target: 85%
      scenarios:
      - Database schema validation
      - Status enum type safety
      - Progress percentage bounds checking
    integration_tests:
    - file: packages/backend/src/__tests__/integration/book-status-flow.test.ts
      scenarios:
      - Complete book processing pipeline with status updates
      - Redis event publishing and consumption
      - Supabase real-time notification delivery
      - Database transaction rollback on failures
      - Concurrent book processing status isolation
    - file: packages/backend/src/__tests__/integration/status-api.test.ts
      scenarios:
      - REST API endpoints return correct status data
      - WebSocket subscription receives real-time updates
    manual_testing:
    - step: Upload a book via frontend and monitor progress in real-time
      expected: Progress bar updates smoothly, status messages are clear, completion
        triggers success notification
    - step: Simulate ML service failure during ANALYZING stage
      expected: Status shows FAILED with error details, retry button appears and works
    - step: Test with multiple books uploading simultaneously
      expected: Each book shows independent progress, no cross-contamination of status
  estimates:
    development: 4.5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Create database migration for book_status and book_progress tables
      done: false
    - task: Implement BookStatus model with PostgreSQL operations
      done: false
    - task: Build BookStatusService with finite state machine logic
      done: false
    - task: Create Redis event publishing/subscription system
      done: false
    - task: Implement REST API endpoints for status queries
      done: false
    - task: Add Supabase real-time subscription integration
      done: false
    - task: Build frontend ProgressTracker component with WebSocket updates
      done: false
    - task: Integrate status updates into existing ML processing services
      done: false
    - task: Write comprehensive unit and integration tests
      done: false
    - task: Add error handling and failure recovery mechanisms
      done: false
    - task: Create API documentation and usage examples
      done: false
    - task: Conduct load testing and performance optimization
      done: false
- key: T46
  title: Error Recovery & Retries
  type: Task
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 3
  area: backend
  dependsOn:
  - T29
  agent_notes:
    research_findings: "**Context:**\nError recovery & retries are critical for Morpheus\
      \ due to its heavy reliance on external services (OpenAI/Anthropic APIs, RunPod\
      \ Stable Diffusion, Supabase) and the long-running nature of novel-to-comic\
      \ transformations. Users expect resilient processing when API rate limits are\
      \ hit, network issues occur, or ML services are temporarily unavailable. Without\
      \ proper retry mechanisms, failed transformations would require manual intervention\
      \ and create poor user experience.\n\n**Technical Approach:**\nImplement a multi-layered\
      \ retry strategy using exponential backoff with jitter:\n1. **HTTP Client Level**:\
      \ Axios interceptors for transient failures (network, 5xx errors)\n2. **Service\
      \ Level**: Custom retry decorators for business logic (rate limits, quota exceeded)\n\
      3. **Job Queue Level**: Bull Queue with job retry configuration for long-running\
      \ tasks\n4. **Circuit Breaker**: Prevent cascade failures when external services\
      \ are down\n5. **Dead Letter Queue**: Capture permanently failed jobs for manual\
      \ review\n6. **Observability**: Structured logging and metrics for retry patterns\n\
      \n**Dependencies:**\n- External: axios-retry, p-retry, bullmq, ioredis, pino\
      \ logger\n- Internal: Existing Fastify plugins, Supabase client, OpenAI/Anthropic\
      \ service wrappers\n\n**Risks:**\n- **API Cost Explosion**: Aggressive retries\
      \ could multiply API costs; mitigation via retry budgets and circuit breakers\n\
      - **Resource Exhaustion**: Too many concurrent retries; mitigation via queue\
      \ concurrency limits and backpressure\n- **Data Inconsistency**: Partial failures\
      \ in multi-step operations; mitigation via idempotent operations and transaction\
      \ boundaries\n- **Observability Gaps**: Hard to debug retry storms; mitigation\
      \ via detailed metrics and distributed tracing\n\n**Complexity Notes:**\nHigher\
      \ complexity than initially estimated due to the heterogeneous nature of external\
      \ services (each has different failure modes, rate limits, and optimal retry\
      \ strategies). The async nature of comic generation adds complexity as retries\
      \ need to maintain job state and user notifications.\n\n**Key Files:**\n- packages/backend/src/lib/retry/:\
      \ Core retry utilities and decorators\n- packages/backend/src/services/ai/:\
      \ Add retry logic to LLM clients  \n- packages/backend/src/services/image/:\
      \ Add retry logic to Stable Diffusion client\n- packages/backend/src/queues/:\
      \ Configure Bull queue retry strategies\n- packages/backend/src/plugins/http-client.ts:\
      \ Axios retry configuration\n"
    design_decisions:
    - decision: Use Bull Queue built-in retry with custom exponential backoff
      rationale: Leverages existing queue infrastructure, provides job persistence,
        and allows per-job retry configuration
      alternatives_considered:
      - Custom retry service
      - AWS SQS with DLQ
      - Simple setTimeout recursion
    - decision: Implement service-specific retry strategies rather than generic approach
      rationale: OpenAI rate limits differ from RunPod availability issues - each
        needs tailored backoff and circuit breaking
      alternatives_considered:
      - Single unified retry service
      - Client-side retry only
      - No retries, fail fast
    - decision: Use circuit breaker pattern with Redis state storage
      rationale: Prevents retry storms across multiple backend instances and provides
        fast failure when services are known to be down
      alternatives_considered:
      - In-memory circuit breaker
      - No circuit breaking
      - Database-backed state
    researched_at: '2026-02-07T18:59:38.005698'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:37:37.626015'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a layered retry system starting with HTTP-level retries via
      axios-retry for transient failures, service-level retries with custom decorators
      for business logic failures, and job queue retries for long-running comic generation
      tasks. Implement circuit breakers to prevent cascade failures and use Redis
      for shared state across backend instances. Add comprehensive observability with
      structured logs and metrics to monitor retry patterns and identify problematic
      external services.

      '
    external_dependencies:
    - name: axios-retry
      version: ^4.0.0
      reason: HTTP-level retry with exponential backoff for API calls
    - name: p-retry
      version: ^6.2.0
      reason: Promise-based retry utility for custom service logic
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for circuit breaker state and job queue backend
    - name: '@bull-board/api'
      version: ^5.10.2
      reason: Queue monitoring dashboard for retry job inspection
    - name: prom-client
      version: ^15.1.0
      reason: Prometheus metrics for retry rates, success/failure ratios
    files_to_modify:
    - path: packages/backend/src/plugins/http-client.ts
      changes: Add axios-retry configuration with exponential backoff, service-specific
        retry conditions
    - path: packages/backend/src/services/ai/openai-client.ts
      changes: Add @Retry decorator with OpenAI-specific retry strategy (rate limits,
        quota exceeded)
    - path: packages/backend/src/services/ai/anthropic-client.ts
      changes: Add @Retry decorator with Anthropic-specific retry strategy
    - path: packages/backend/src/services/image/runpod-client.ts
      changes: Add @Retry decorator for Stable Diffusion API failures and timeouts
    - path: packages/backend/src/queues/comic-generation-queue.ts
      changes: Configure Bull queue retry settings, add job state persistence, error
        handling
    - path: packages/backend/src/plugins/observability.ts
      changes: Add retry metrics, structured logging for retry events, cost tracking
    new_files:
    - path: packages/backend/src/lib/retry/retry-decorator.ts
      purpose: Decorator factory for service-level retry logic with provider-specific
        strategies
    - path: packages/backend/src/lib/retry/circuit-breaker.ts
      purpose: Circuit breaker implementation with Redis state sharing across instances
    - path: packages/backend/src/lib/retry/backoff-strategies.ts
      purpose: Exponential backoff, jitter, and provider-specific timing strategies
    - path: packages/backend/src/lib/retry/dead-letter-queue.ts
      purpose: Handler for permanently failed jobs, admin review interface
    - path: packages/backend/src/lib/retry/retry-config.ts
      purpose: Centralized retry configuration for all services and HTTP clients
    - path: packages/backend/src/lib/retry/cost-tracker.ts
      purpose: Track API costs from retry attempts, budget enforcement
    - path: packages/backend/src/__tests__/fixtures/mock-ai-servers.ts
      purpose: Mock HTTP servers simulating various AI service failure modes for testing
  acceptance_criteria:
  - criterion: HTTP client automatically retries transient failures (network, 5xx
      errors) with exponential backoff up to 3 attempts
    verification: Unit test simulating network failures shows retry attempts with
      increasing delays. Integration test with mock server returning 503 then 200
      succeeds on retry.
  - criterion: Service-level retries handle API rate limits (429) and quota exceeded
      errors with appropriate backoff strategies for each provider
    verification: Integration tests with OpenAI/Anthropic/RunPod mock servers returning
      rate limit errors show provider-specific retry behavior. Verify OpenAI uses
      60s backoff, Anthropic uses exponential backoff.
  - criterion: Job queue retries failed comic generation tasks up to 5 times with
      exponential backoff, maintaining job state and user notifications
    verification: E2E test creating comic generation job that fails 3 times then succeeds
      shows job retries, state persistence in Supabase, and websocket notifications
      to user.
  - criterion: Circuit breaker opens after 5 consecutive failures to external service,
      stays open for 30s, then allows single test request
    verification: Integration test simulating external service downtime shows circuit
      breaker state transitions logged with metrics. Manual verification of circuit
      breaker dashboard showing open/closed states.
  - criterion: Comprehensive observability captures retry patterns, failure rates,
      and costs with structured logs and Prometheus metrics
    verification: Load test generating 100 comics with 20% artificial failure rate
      produces detailed retry metrics dashboard. Verify retry_attempts_total, circuit_breaker_state,
      and retry_cost_estimate metrics.
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/lib/retry.test.ts
      coverage_target: 95%
      scenarios:
      - Exponential backoff calculation with jitter
      - Retry decorator success after failures
      - Circuit breaker state transitions
      - Dead letter queue job handling
    - file: packages/backend/src/__tests__/services/ai-client.test.ts
      coverage_target: 90%
      scenarios:
      - OpenAI rate limit retry with 60s backoff
      - Anthropic exponential backoff retry
      - Permanent failure after max retries
      - Successful retry after transient failure
    integration_tests:
    - file: packages/backend/src/__tests__/integration/retry-system.test.ts
      scenarios:
      - End-to-end comic generation with AI service failures and recovery
      - Circuit breaker prevents cascade failures during service outage
      - Dead letter queue captures permanently failed jobs
      - Job queue maintains state across retries
    - file: packages/backend/src/__tests__/integration/external-services.test.ts
      scenarios:
      - HTTP client retry behavior with real external service failures
      - Service-specific retry strategies for each AI provider
    manual_testing:
    - step: Disable external AI service, trigger comic generation, re-enable service
      expected: Circuit breaker opens, jobs queue up, service recovery triggers retries,
        comics complete successfully
    - step: Generate load with 50 concurrent comic requests during peak API hours
      expected: Rate limit retries handled gracefully, no user-facing errors, retry
        metrics captured
  estimates:
    development: 4
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 7.5
  progress:
    status: not-started
    checklist:
    - task: Install retry dependencies (axios-retry, p-retry, ioredis circuit breaker)
      done: false
    - task: Implement core retry utilities and backoff strategies
      done: false
    - task: Create retry decorator with provider-specific configurations
      done: false
    - task: Add HTTP client retry configuration to Fastify plugin
      done: false
    - task: Implement circuit breaker with Redis shared state
      done: false
    - task: Configure Bull queue retry strategies for comic generation
      done: false
    - task: Add service-level retry decorators to AI clients
      done: false
    - task: Implement dead letter queue and cost tracking
      done: false
    - task: Add comprehensive observability (metrics, structured logs)
      done: false
    - task: Create unit and integration tests with mock failure scenarios
      done: false
    - task: Perform manual testing with real external service failures
      done: false
    - task: Update API documentation with retry behavior details
      done: false
- key: T47
  title: M2 Integration Testing
  type: Task
  milestone: M1 - Backend Services
  iteration: I3
  priority: p0
  effort: 5
  area: ingestion
  dependsOn:
  - T44
  - T45
  - T46
  agent_notes:
    research_findings: '**Context:**

      M2 Integration Testing refers to comprehensive end-to-end testing for Milestone
      2 features, likely focusing on the complete novel-to-comic transformation pipeline.
      This task ensures that all components (ingestion, LLM processing, image generation,
      storage) work together seamlessly. Given the p0 priority and "ingestion" area,
      this specifically tests the data flow from novel input through the entire transformation
      process, validating that the backend services can handle real-world scenarios
      with proper error handling, data consistency, and performance requirements.


      **Technical Approach:**

      - Use Vitest for unit/integration tests with test containers for isolated database
      testing

      - Implement Playwright for full E2E workflows simulating real user journeys

      - Create test fixtures with sample novels, expected outputs, and mock ML responses

      - Build integration test harnesses using Fastify''s testing utilities

      - Implement database seeding/cleanup strategies with Supabase test environments

      - Use MSW (Mock Service Worker) for mocking external API calls (OpenAI/Anthropic/RunPod)

      - Create comprehensive test data factories for consistent test scenarios


      **Dependencies:**

      - External: @testcontainers/postgresql, msw, @faker-js/faker, dotenv-cli

      - Internal: All M1 backend services, database schema, authentication middleware,
      ingestion pipeline, LLM integration services


      **Risks:**

      - Flaky tests due to external API dependencies: Use comprehensive mocking strategies

      - Test data management complexity: Implement proper seeding/teardown with isolated
      test DBs

      - Performance bottlenecks in CI: Parallelize tests and use selective test running

      - Mock drift from real APIs: Regular validation of mocks against actual API
      responses


      **Complexity Notes:**

      This is more complex than typical integration testing due to the multi-service
      architecture involving ML APIs, file processing, and real-time data transformations.
      The async nature of comic generation and potential webhook integrations add
      coordination complexity.


      **Key Files:**

      - packages/backend/src/__tests__/integration/: Test suite structure

      - packages/backend/src/__tests__/fixtures/: Sample novels and expected outputs

      - packages/backend/src/__tests__/helpers/: Test utilities and database helpers

      - apps/backend/vitest.config.ts: Vitest configuration for integration tests

      - packages/shared/src/test-utils/: Shared testing utilities across packages

      '
    design_decisions:
    - decision: Use Testcontainers for database isolation
      rationale: Ensures clean test environment without affecting development DB,
        enables parallel test execution
      alternatives_considered:
      - Shared test database
      - In-memory database
      - Database transactions rollback
    - decision: MSW for external API mocking
      rationale: Provides realistic HTTP mocking at network level, works across different
        test environments
      alternatives_considered:
      - Jest mocks
      - Nock
      - Manual mock implementations
    - decision: Separate integration test environment
      rationale: Isolates integration tests from unit tests, allows different configuration
        and longer timeouts
      alternatives_considered:
      - Mixed test approach
      - Only E2E tests
      - Only unit tests
    researched_at: '2026-02-07T19:00:00.270151'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:38:03.017759'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a comprehensive integration testing suite that validates the
      complete novel ingestion and processing pipeline. Use Testcontainers for isolated
      PostgreSQL instances, MSW for mocking external ML APIs, and Vitest for the test
      runner. Implement test fixtures with sample novels and expected comic outputs,
      ensuring all error scenarios and edge cases are covered. Structure tests to
      run in parallel while maintaining data isolation, with proper setup/teardown
      for database state management.

      '
    external_dependencies:
    - name: '@testcontainers/postgresql'
      version: ^10.0.0
      reason: Isolated PostgreSQL instances for integration testing
    - name: msw
      version: ^2.0.0
      reason: Mock external API calls to OpenAI, Anthropic, RunPod
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate realistic test data for novels and user scenarios
    - name: dotenv-cli
      version: ^7.0.0
      reason: Environment variable management for test configurations
    files_to_modify:
    - path: apps/backend/vitest.config.ts
      changes: Add integration test configuration with testcontainers setup and MSW
        integration
    - path: packages/backend/src/services/ingestion/novel-processor.ts
      changes: Add test hooks and instrumentation for integration testing
    - path: packages/backend/src/lib/database/client.ts
      changes: Add test database connection handling and transaction utilities
    new_files:
    - path: packages/backend/src/__tests__/integration/novel-pipeline.test.ts
      purpose: End-to-end testing of complete novel processing workflow
    - path: packages/backend/src/__tests__/integration/database-operations.test.ts
      purpose: Database transaction and consistency testing across services
    - path: packages/backend/src/__tests__/fixtures/sample-novels.ts
      purpose: Test data fixtures with various novel formats and edge cases
    - path: packages/backend/src/__tests__/helpers/test-database.ts
      purpose: Database setup/teardown utilities with testcontainers
    - path: packages/backend/src/__tests__/helpers/api-mocks.ts
      purpose: MSW handlers for OpenAI, Anthropic, and RunPod API mocking
    - path: packages/backend/src/__tests__/helpers/test-factories.ts
      purpose: Data factory functions for consistent test object generation
    - path: packages/backend/src/__tests__/setup/integration-setup.ts
      purpose: Global test setup for integration test environment
    - path: packages/shared/src/test-utils/performance-helpers.ts
      purpose: Shared performance testing utilities and assertions
    - path: .github/workflows/integration-tests.yml
      purpose: CI/CD pipeline configuration for integration test execution
  acceptance_criteria:
  - criterion: Complete novel ingestion pipeline processes sample novels end-to-end
      with proper data flow validation
    verification: Run `npm run test:integration -- --grep 'novel ingestion pipeline'`
      and verify all test scenarios pass
  - criterion: Integration tests achieve >90% code coverage for ingestion services
      and error handling paths
    verification: Run `npm run test:coverage:integration` and verify coverage reports
      show >90% for ingestion modules
  - criterion: Mock external ML APIs (OpenAI/Anthropic/RunPod) respond correctly to
      all integration test scenarios
    verification: Integration tests pass with MSW mocks active and log validation
      shows proper API call patterns
  - criterion: Database transactions and cleanup work correctly across all test scenarios
      with isolated test environments
    verification: Run integration test suite 3 times consecutively without failures,
      verify test database isolation
  - criterion: 'Performance benchmarks meet requirements: <2s for novel upload, <30s
      for chapter processing'
    verification: Integration tests include timing assertions and performance regression
      detection
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/unit/ingestion-service.test.ts
      coverage_target: 95%
      scenarios:
      - Novel file parsing and validation
      - Chapter extraction and segmentation
      - Database transaction rollback on errors
      - File upload size and format validation
    - file: packages/backend/src/__tests__/unit/llm-integration.test.ts
      coverage_target: 90%
      scenarios:
      - OpenAI API response parsing
      - Rate limiting and retry logic
      - Token counting and chunking
    integration_tests:
    - file: packages/backend/src/__tests__/integration/novel-pipeline.test.ts
      scenarios:
      - Complete novel upload to comic generation flow
      - Multi-chapter processing with parallel execution
      - Error recovery and partial processing states
      - Authentication and authorization throughout pipeline
    - file: packages/backend/src/__tests__/integration/database-operations.test.ts
      scenarios:
      - Transaction consistency across multiple services
      - Concurrent user processing isolation
      - Data migration and schema validation
    manual_testing:
    - step: Upload a 50-chapter novel through the API endpoint
      expected: Processing completes within 15 minutes with status updates
    - step: Simulate API failures during processing
      expected: Graceful error handling with proper user notifications
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup testcontainers configuration and Docker test environment
      done: false
    - task: Create MSW mock handlers for all external ML APIs with realistic responses
      done: false
    - task: Build comprehensive test fixtures with sample novels and expected outputs
      done: false
    - task: Implement database helpers with proper seeding and cleanup strategies
      done: false
    - task: Write core integration tests for novel ingestion pipeline
      done: false
    - task: Add database transaction and consistency validation tests
      done: false
    - task: Implement performance benchmarking and regression detection
      done: false
    - task: Setup CI/CD pipeline with parallel test execution and reporting
      done: false
    - task: Create comprehensive test documentation and debugging guides
      done: false
    - task: Conduct thorough code review and integration with existing test suite
      done: false
- key: T19
  title: Dataset Preparation for Dialogue Extraction
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p0
  effort: 5
  area: ml
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      This task involves preparing training datasets for extracting dialogue from
      novel text, which is critical for the novel-to-comic transformation pipeline.
      Comics rely heavily on dialogue bubbles, so we need ML models that can accurately
      identify, extract, and classify different types of speech (dialogue, narration,
      inner thoughts, sound effects). This enables proper comic panel layout and visual
      storytelling structure.


      **Technical Approach:**

      - Create annotated datasets with dialogue/narration labels using spaCy NLP pipelines

      - Implement data preprocessing with Hugging Face Transformers for token classification

      - Use pandas/polars for efficient dataset manipulation and validation

      - Store processed datasets in Supabase with proper indexing for ML training
      retrieval

      - Implement active learning feedback loops to improve dataset quality over time

      - Use OpenAI/Anthropic APIs for initial auto-labeling with human validation


      **Dependencies:**

      - External: spacy, transformers, datasets (HuggingFace), pandas, scikit-learn,
      nltk

      - Internal: Supabase database schema, novel processing service, ML training
      pipeline


      **Risks:**

      - Data quality inconsistency: Implement strict validation schemas and inter-annotator
      agreement metrics

      - Copyright/licensing issues: Ensure training data complies with fair use and
      licensing terms

      - Annotation bias: Use multiple annotators and cross-validation techniques

      - Dataset imbalance: Apply stratified sampling and synthetic data generation
      techniques


      **Complexity Notes:**

      This is more complex than initially estimated due to the nuanced nature of dialogue
      extraction (handling nested quotes, implied speech, narrative voice changes).
      Requires domain expertise in literary analysis and careful consideration of
      different novel genres and writing styles.


      **Key Files:**

      - packages/ml/src/data/dialogue-extractor.ts: Core extraction logic

      - packages/ml/src/datasets/preparation.ts: Dataset preprocessing pipeline

      - packages/database/migrations/: New tables for training data storage

      - packages/ml/src/models/dialogue-classifier.ts: Model training interface

      '
    design_decisions:
    - decision: Use spaCy + custom NER models for dialogue extraction
      rationale: spaCy provides robust NLP pipelines with custom entity training capabilities,
        better than regex-based approaches for handling complex literary text
      alternatives_considered:
      - NLTK-only approach
      - Pure transformer-based solution
      - Rule-based regex system
    - decision: Store training data in Supabase with JSONB metadata
      rationale: Keeps training data co-located with application data, enables real-time
        feedback integration, and JSONB allows flexible annotation schemas
      alternatives_considered:
      - Separate ML database
      - File-based storage
      - External ML platform
    researched_at: '2026-02-07T19:00:20.208307'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:38:29.432459'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a multi-stage pipeline that processes novel text through spaCy
      NLP models to identify dialogue boundaries, then uses transformer-based classification
      to categorize speech types. Implement active learning by integrating user feedback
      from the comic generation process to continuously improve dataset quality. Store
      processed datasets in Supabase with proper schema versioning for ML experiment
      tracking.

      '
    external_dependencies:
    - name: spacy
      version: ^3.7.0
      reason: Advanced NLP processing and custom NER model training
    - name: '@huggingface/transformers'
      version: ^2.17.0
      reason: Pre-trained language models for text classification
    - name: datasets
      version: ^2.14.0
      reason: HuggingFace datasets library for ML-ready data formats
    - name: pandas-js
      version: ^2.0.0
      reason: Data manipulation and preprocessing in TypeScript environment
    - name: natural
      version: ^6.5.0
      reason: Additional NLP utilities for text preprocessing
    files_to_modify:
    - path: packages/database/supabase/migrations/20241201000000_dialogue_training_schema.sql
      changes: Add tables for dialogue_annotations, training_datasets, model_experiments,
        feedback_corrections
    - path: packages/database/src/types/training.ts
      changes: Add TypeScript types for training data schemas and API responses
    - path: apps/api/src/routes/ml/index.ts
      changes: Add routes for dataset upload, annotation, feedback submission
    new_files:
    - path: packages/ml/src/data/dialogue-extractor.ts
      purpose: Core spaCy-based dialogue extraction and boundary detection logic
    - path: packages/ml/src/datasets/preparation.ts
      purpose: Hugging Face transformers preprocessing pipeline for token classification
    - path: packages/ml/src/models/dialogue-classifier.ts
      purpose: Model training interface and active learning implementation
    - path: packages/ml/src/services/annotation-service.ts
      purpose: OpenAI/Anthropic auto-labeling with validation workflows
    - path: packages/ml/src/utils/dataset-validator.ts
      purpose: Quality validation, inter-annotator agreement, bias detection
    - path: packages/ml/src/config/training-config.ts
      purpose: Configuration for model hyperparameters, dataset splits, evaluation
        metrics
    - path: apps/api/src/controllers/dialogue-training.controller.ts
      purpose: API endpoints for dataset management and training pipeline triggers
    - path: packages/ml/src/__tests__/fixtures/sample-novels.ts
      purpose: Test data with various dialogue patterns and edge cases
  acceptance_criteria:
  - criterion: Pipeline can process novel text and extract dialogue with >85% accuracy
      on validation dataset
    verification: Run `npm test packages/ml/src/__tests__/dialogue-extractor.test.ts`
      and check accuracy metrics
  - criterion: Annotated training dataset contains at least 10,000 labeled examples
      across 4 categories (dialogue, narration, inner thoughts, sound effects)
    verification: 'Query Supabase training_datasets table: `SELECT category, COUNT(*)
      FROM dialogue_annotations GROUP BY category`'
  - criterion: Dataset preprocessing pipeline handles edge cases (nested quotes, multi-paragraph
      dialogue, mixed narrative styles)
    verification: Execute integration test suite with edge case scenarios and validate
      100% completion without errors
  - criterion: Active learning feedback system successfully incorporates user corrections
      to improve model performance
    verification: 'Manual test: submit correction through API, verify database update,
      retrain model, measure accuracy improvement >2%'
  - criterion: Complete API documentation and dataset schema documentation available
    verification: Check docs/ml/dialogue-extraction.md exists with API endpoints,
      schema definitions, and usage examples
  testing:
    unit_tests:
    - file: packages/ml/src/__tests__/dialogue-extractor.test.ts
      coverage_target: 90%
      scenarios:
      - Extract simple dialogue with quotes
      - Handle nested dialogue and attribution
      - Classify different speech types correctly
      - Process edge cases (incomplete quotes, formatting issues)
      - Error handling for malformed input
    - file: packages/ml/src/__tests__/dataset-preparation.test.ts
      coverage_target: 85%
      scenarios:
      - Preprocessing pipeline transforms raw text correctly
      - Token classification labeling accuracy
      - Dataset validation and quality checks
      - Batch processing performance
    integration_tests:
    - file: packages/ml/src/__tests__/integration/dialogue-pipeline.test.ts
      scenarios:
      - End-to-end novel processing to labeled dataset
      - Supabase storage and retrieval workflow
      - Active learning feedback integration
      - Model training data pipeline
    manual_testing:
    - step: Upload sample novel chapter via API endpoint
      expected: Dialogue extracted and categorized correctly, stored in database
    - step: Submit feedback correction through active learning interface
      expected: Dataset updated, model retraining triggered, improved predictions
    - step: Test with different novel genres (fantasy, romance, mystery)
      expected: Consistent extraction quality across writing styles
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup Supabase database schema for training datasets and annotations
      done: false
    - task: Implement spaCy-based dialogue extraction with boundary detection
      done: false
    - task: Build Hugging Face transformers preprocessing pipeline for token classification
      done: false
    - task: Create OpenAI/Anthropic auto-labeling service with validation workflows
      done: false
    - task: Implement dataset validation, quality metrics, and bias detection utilities
      done: false
    - task: Build active learning feedback system with model retraining triggers
      done: false
    - task: Develop API endpoints for dataset upload, annotation, and feedback submission
      done: false
    - task: Create comprehensive test suites with edge cases and integration scenarios
      done: false
    - task: Generate sample annotated datasets with 10k+ examples across 4 categories
      done: false
    - task: Write technical documentation and API reference guides
      done: false
    - task: Performance optimization and error handling implementation
      done: false
    - task: Code review and security audit for data handling
      done: false
- key: T20
  title: Polish Language Model Selection & Validation
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p0
  effort: 3
  area: ml
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nThis task involves implementing a sophisticated\
      \ model selection system for choosing the optimal LLM for different comic transformation\
      \ subtasks (scene breakdown, character extraction, dialogue adaptation, panel\
      \ descriptions). Polish refers to refinement/optimization, not the language.\
      \ The system needs to dynamically select between OpenAI GPT-4, Anthropic Claude,\
      \ and potentially other models based on task type, content complexity, cost\
      \ constraints, and quality requirements. This is critical for M2 as it directly\
      \ impacts transformation quality and operational costs.\n\n**Technical Approach:**\n\
      - Implement a Model Router service using strategy pattern with async model evaluation\n\
      - Create model performance benchmarking system with A/B testing capabilities\
      \  \n- Use Zod schemas for model response validation and quality scoring\n-\
      \ Implement fallback chains (primary -> secondary -> tertiary model)\n- Add\
      \ cost tracking and budget constraints per transformation job\n- Use Redis for\
      \ caching model selection decisions and performance metrics\n- Integrate with\
      \ existing ML pipeline through dependency injection\n\n**Dependencies:**\n-\
      \ External: zod, ioredis, openai@^4.0.0, @anthropic-ai/sdk, p-queue, p-retry\n\
      - Internal: Supabase client, existing ML service interfaces, job queue system,\
      \ logging service\n\n**Risks:**\n- Model API rate limits: implement exponential\
      \ backoff and request queuing\n- Cost explosion: add strict budget controls\
      \ and monitoring dashboards\n- Quality degradation: maintain quality thresholds\
      \ with automatic rollback\n- Vendor lock-in: abstract model interfaces for easy\
      \ provider switching\n\n**Complexity Notes:**\nMore complex than initially estimated.\
      \ Requires sophisticated orchestration logic, real-time quality assessment,\
      \ and cost optimization algorithms. The validation component alone needs prompt\
      \ engineering expertise and extensive testing infrastructure.\n\n**Key Files:**\n\
      - packages/ml/src/services/model-router.service.ts: core routing logic\n- packages/ml/src/validators/model-response.validator.ts:\
      \ response validation\n- packages/ml/src/models/model-performance.model.ts:\
      \ performance tracking\n- packages/backend/src/api/ml/model-selection.route.ts:\
      \ API endpoints\n"
    design_decisions:
    - decision: Strategy Pattern with Dynamic Model Selection
      rationale: Allows runtime model switching based on content analysis and performance
        metrics while maintaining clean separation of concerns
      alternatives_considered:
      - Static model assignment
      - Round-robin selection
      - ML-based model recommendation
    - decision: Zod-based Response Validation Pipeline
      rationale: Provides type-safe validation with detailed error reporting and quality
        scoring for model outputs
      alternatives_considered:
      - Custom validation logic
      - JSON Schema validation
      - LLM-based self-validation
    - decision: Redis-backed Performance Caching
      rationale: Enables fast model selection decisions and reduces redundant API
        calls for similar content types
      alternatives_considered:
      - Database-only storage
      - In-memory caching
      - No caching
    researched_at: '2026-02-07T19:00:41.713990'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:38:52.516938'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a hierarchical model selection system that analyzes incoming
      transformation requests, evaluates content complexity, checks performance history,
      and routes to the optimal LLM provider. The system will validate responses against
      strict quality criteria, track costs in real-time, and maintain fallback chains
      for reliability. Performance data feeds back into the selection algorithm for
      continuous improvement.

      '
    external_dependencies:
    - name: zod
      version: ^3.22.0
      reason: Runtime validation and quality scoring of model responses
    - name: '@anthropic-ai/sdk'
      version: ^0.17.0
      reason: Claude API integration for alternative model routing
    - name: p-queue
      version: ^7.4.0
      reason: Request queuing and rate limit management across providers
    - name: p-retry
      version: ^5.1.0
      reason: Robust retry logic with exponential backoff for API failures
    - name: ioredis
      version: ^5.3.0
      reason: Caching model performance metrics and selection decisions
    files_to_modify:
    - path: packages/ml/src/services/ml.service.ts
      changes: Integrate ModelRouter service via dependency injection
    - path: packages/backend/src/config/database.ts
      changes: Add model performance and cost tracking tables schema
    - path: packages/backend/src/middleware/auth.middleware.ts
      changes: Add budget validation middleware for ML endpoints
    new_files:
    - path: packages/ml/src/services/model-router.service.ts
      purpose: Core routing logic with strategy pattern implementation
    - path: packages/ml/src/services/model-benchmark.service.ts
      purpose: A/B testing and performance measurement service
    - path: packages/ml/src/validators/model-response.validator.ts
      purpose: Zod schema validation and quality scoring
    - path: packages/ml/src/models/model-performance.model.ts
      purpose: TypeScript interfaces for performance metrics
    - path: packages/ml/src/providers/openai.provider.ts
      purpose: OpenAI API wrapper with rate limiting and retry logic
    - path: packages/ml/src/providers/anthropic.provider.ts
      purpose: Claude API wrapper with consistent interface
    - path: packages/ml/src/utils/cost-calculator.ts
      purpose: Token counting and cost calculation utilities
    - path: packages/backend/src/api/ml/model-selection.route.ts
      purpose: REST API endpoints for model selection and performance data
    - path: packages/ml/src/config/model-config.ts
      purpose: Model provider configurations and routing rules
    - path: packages/shared/src/types/model-selection.types.ts
      purpose: Shared TypeScript interfaces for model selection
  acceptance_criteria:
  - criterion: Model router correctly selects optimal LLM based on task type and complexity
    verification: Unit tests verify router chooses GPT-4 for complex scene breakdown,
      Claude for dialogue adaptation, with 95% accuracy on test dataset
  - criterion: Cost tracking prevents budget overruns with real-time monitoring
    verification: Integration tests confirm transformation jobs halt when approaching
      budget limits (configurable threshold), cost per request logged to Supabase
  - criterion: Fallback chain maintains service availability during provider outages
    verification: E2E tests simulate API failures, verify automatic failover to secondary/tertiary
      models within 30 seconds
  - criterion: Response validation rejects low-quality outputs and triggers retries
    verification: Manual testing with intentionally malformed responses confirms Zod
      validation catches schema violations and quality scores below threshold
  - criterion: Performance benchmarking system provides actionable metrics for model
      optimization
    verification: Dashboard shows model performance trends, A/B test results accessible
      via API endpoint /api/ml/model-performance
  testing:
    unit_tests:
    - file: packages/ml/src/__tests__/model-router.service.test.ts
      coverage_target: 90%
      scenarios:
      - Route selection based on task complexity
      - Cost constraint enforcement
      - Fallback chain execution
      - Redis cache hit/miss scenarios
    - file: packages/ml/src/__tests__/model-response.validator.test.ts
      coverage_target: 85%
      scenarios:
      - Valid response validation
      - Schema validation failures
      - Quality score calculation
    integration_tests:
    - file: packages/ml/src/__tests__/integration/model-selection-flow.test.ts
      scenarios:
      - End-to-end model selection and validation
      - Multi-provider failover scenarios
      - Performance metric collection
    - file: packages/backend/src/__tests__/integration/ml-api.test.ts
      scenarios:
      - Model selection API endpoints
      - Cost tracking integration with Supabase
    manual_testing:
    - step: Submit comic transformation with complex scene requiring GPT-4
      expected: Router selects GPT-4, logs decision reasoning, tracks cost
    - step: Trigger budget threshold breach during transformation
      expected: Service halts processing, returns budget exceeded error
    - step: Simulate OpenAI API outage during active transformation
      expected: Automatic failover to Claude, transformation completes successfully
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup Redis client and model provider SDK configurations
      done: false
    - task: Implement base ModelProvider interface and concrete provider classes
      done: false
    - task: Create ModelRouter service with strategy pattern and async evaluation
      done: false
    - task: Build response validation system with Zod schemas and quality scoring
      done: false
    - task: Implement cost tracking and budget constraint enforcement
      done: false
    - task: Create benchmarking service with A/B testing capabilities
      done: false
    - task: Build fallback chain logic with exponential backoff
      done: false
    - task: Integrate with existing ML pipeline and job queue system
      done: false
    - task: Create API endpoints for model selection and performance monitoring
      done: false
    - task: Write comprehensive tests and performance benchmarks
      done: false
    - task: Documentation and deployment configuration
      done: false
- key: T21
  title: Dialogue Extraction Model Training
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p0
  effort: 8
  area: ml
  dependsOn:
  - T19
  - T20
  agent_notes:
    research_findings: '**Context:**

      Dialogue extraction is critical for converting novels to comics, as it separates
      character speech from narrative text. This enables proper comic panel layout
      where dialogue appears in speech bubbles while narrative becomes captions or
      is omitted. The model must identify speakers, extract clean dialogue, and preserve
      emotional context for visual storytelling. This is foundational for the comic
      generation pipeline.


      **Technical Approach:**

      Use a two-stage approach: 1) Named Entity Recognition (NER) to identify characters
      and speakers, 2) Dialogue classification to extract speech patterns. Implement
      using spaCy for NER with custom training on literary texts, combined with a
      fine-tuned transformer model (BERT/RoBERTa) for dialogue detection. Create a
      training pipeline using Hugging Face transformers with custom datasets from
      Project Gutenberg novels. Store training data and models in Supabase with versioning.


      **Dependencies:**

      - External: spaCy, transformers, datasets, torch, wandb, nltk

      - Internal: Database models for training data, ML service infrastructure, text
      preprocessing utilities


      **Risks:**

      - Training data bias: Literary styles vary greatly; mitigation through diverse
      dataset curation

      - Speaker attribution accuracy: Complex novels with many characters; use context
      windows and speaker tracking

      - Performance overhead: Large models impact inference speed; implement model
      quantization and caching

      - Training costs: GPU requirements for fine-tuning; use incremental training
      and smaller model variants


      **Complexity Notes:**

      More complex than initially estimated due to nuanced speaker attribution in
      literary text. Unlike screenplay format, novels have inconsistent dialogue patterns,
      indirect speech, and complex narrative structures. Requires sophisticated NLP
      preprocessing and custom training data annotation.


      **Key Files:**

      - apps/ml/src/models/dialogue-extractor.ts: Main model interface

      - apps/ml/src/training/dialogue-trainer.py: Training pipeline

      - packages/shared/src/types/dialogue.ts: Type definitions

      - apps/backend/src/services/ml/dialogue-service.ts: API integration

      '
    design_decisions:
    - decision: Use spaCy + Transformers hybrid approach instead of pure LLM
      rationale: Better control over training data, faster inference, and more accurate
        speaker attribution than prompt-based LLM approaches
      alternatives_considered:
      - Pure OpenAI API calls
      - Custom LSTM model
      - Rule-based regex extraction
    - decision: Implement training pipeline with Weights & Biases tracking
      rationale: Need experiment tracking for model iterations and hyperparameter
        optimization in ML development
      alternatives_considered:
      - MLflow
      - TensorBoard only
      - Custom logging
    researched_at: '2026-02-07T19:01:03.390029'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:39:17.906039'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a dialogue extraction service that preprocesses novel text through
      spaCy NER for character identification, then applies a fine-tuned BERT model
      to classify dialogue vs narrative sentences. Create training datasets from annotated
      literary works, implementing active learning to improve model accuracy over
      time. Deploy the trained model as a FastAPI microservice that integrates with
      the main Morpheus backend, providing structured dialogue data with speaker attribution
      and emotional context tags.

      '
    external_dependencies:
    - name: spacy
      version: ^3.7.0
      reason: Industrial-strength NLP with custom model training capabilities
    - name: transformers
      version: ^4.36.0
      reason: Hugging Face transformers for BERT/RoBERTa fine-tuning
    - name: datasets
      version: ^2.14.0
      reason: Efficient data loading and preprocessing for training
    - name: torch
      version: ^2.1.0
      reason: PyTorch backend for transformer model training
    - name: wandb
      version: ^0.16.0
      reason: Experiment tracking and model versioning
    - name: nltk
      version: ^3.8.0
      reason: Text preprocessing utilities and sentence tokenization
    files_to_modify:
    - path: apps/backend/src/routes/ml.ts
      changes: Add dialogue extraction endpoint with proper request validation and
        error handling
    - path: apps/backend/src/services/ml/base-ml-service.ts
      changes: Extend base service to support dialogue model loading and inference
    - path: packages/shared/src/types/ml.ts
      changes: Add dialogue extraction request/response types
    new_files:
    - path: apps/ml/src/models/dialogue-extractor.ts
      purpose: Main dialogue extraction model interface with NER and classification
        components
    - path: apps/ml/src/training/dialogue-trainer.py
      purpose: Training pipeline for fine-tuning BERT model on dialogue detection
        task
    - path: apps/ml/src/preprocessing/text-processor.ts
      purpose: Text preprocessing utilities for novel format handling and tokenization
    - path: apps/ml/src/data/dataset-builder.py
      purpose: Build training datasets from Project Gutenberg novels with dialogue
        annotation
    - path: apps/backend/src/services/ml/dialogue-service.ts
      purpose: Backend service for dialogue extraction API integration
    - path: packages/shared/src/types/dialogue.ts
      purpose: TypeScript interfaces for dialogue data structures and API contracts
    - path: apps/ml/src/evaluation/dialogue_eval.py
      purpose: Model evaluation scripts for precision/recall metrics on dialogue extraction
    - path: apps/ml/src/evaluation/speaker_eval.py
      purpose: Speaker attribution accuracy evaluation and analysis
    - path: apps/ml/configs/dialogue_config.yaml
      purpose: Training configuration for model hyperparameters and data paths
    - path: apps/ml/src/utils/model-storage.ts
      purpose: Model versioning and artifact management with Supabase integration
  acceptance_criteria:
  - criterion: Model accurately extracts dialogue with >85% precision/recall on test
      dataset
    verification: 'Run evaluation script: python apps/ml/src/evaluation/dialogue_eval.py
      --test-data data/test/annotated_novels.json'
  - criterion: Speaker attribution correctly identifies characters in >80% of dialogue
      instances
    verification: 'Execute speaker evaluation: python apps/ml/src/evaluation/speaker_eval.py
      and check attribution_accuracy metric'
  - criterion: API processes novel chapters and returns structured dialogue data within
      30 seconds for 10k word texts
    verification: 'Load test: curl -X POST /api/ml/extract-dialogue with sample chapter,
      verify response time <30s'
  - criterion: Training pipeline successfully fine-tunes BERT model and saves versioned
      artifacts
    verification: 'Run training: python apps/ml/src/training/dialogue-trainer.py --config
      configs/dialogue_config.yaml, verify model saved to storage'
  - criterion: Integration with backend service provides dialogue extraction endpoint
      with proper error handling
    verification: 'Test API endpoint: POST /api/novels/{id}/extract-dialogue returns
      200 with dialogue array or appropriate error codes'
  testing:
    unit_tests:
    - file: apps/ml/src/__tests__/dialogue-extractor.test.ts
      coverage_target: 90%
      scenarios:
      - Text preprocessing and tokenization
      - NER character extraction
      - Dialogue classification accuracy
      - Speaker attribution logic
      - Error handling for malformed input
    - file: apps/backend/src/__tests__/services/dialogue-service.test.ts
      coverage_target: 85%
      scenarios:
      - Service initialization and model loading
      - API request/response formatting
      - Database interaction for training data
      - Caching mechanism
    integration_tests:
    - file: apps/ml/src/__tests__/integration/training-pipeline.test.ts
      scenarios:
      - End-to-end training from data ingestion to model export
      - Model versioning and artifact storage
      - Integration with Supabase for training data
    - file: apps/backend/src/__tests__/integration/dialogue-api.test.ts
      scenarios:
      - Complete dialogue extraction workflow
      - ML service communication
      - Error propagation and handling
    manual_testing:
    - step: Upload sample novel chapter via API
      expected: Returns structured dialogue with speakers, text, and emotional tags
    - step: Test with various literary styles (modern, classic, different genres)
      expected: Consistent extraction quality across different writing styles
    - step: Verify training dashboard shows model metrics and progress
      expected: WandB dashboard displays training loss, validation metrics, and model
        artifacts
  estimates:
    development: 8
    code_review: 1
    testing: 2
    documentation: 1
    total: 12
  progress:
    status: not-started
    checklist:
    - task: Setup Python ML environment with spaCy, transformers, and dependencies
      done: false
    - task: Create training dataset from Project Gutenberg novels with dialogue annotation
      done: false
    - task: Implement text preprocessing pipeline with NER character identification
      done: false
    - task: Build and train BERT-based dialogue classification model
      done: false
    - task: Develop dialogue extraction service with speaker attribution logic
      done: false
    - task: Create backend API integration with proper error handling and validation
      done: false
    - task: Implement model evaluation pipeline with precision/recall metrics
      done: false
    - task: Add model versioning and artifact storage to Supabase
      done: false
    - task: Setup monitoring and logging with WandB integration
      done: false
    - task: Create comprehensive test suite covering unit and integration scenarios
      done: false
- key: T22
  title: Character Extraction Training (Named Entity Recognition)
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p0
  effort: 5
  area: ml
  dependsOn:
  - T19
  - T20
  agent_notes:
    research_findings: '**Context:**

      Character extraction is critical for Morpheus to identify and track main characters,
      supporting characters, and their relationships throughout a novel. This enables
      consistent visual representation across comic panels, character-specific dialogue
      styling, and narrative continuity. Without accurate character extraction, the
      comic transformation would lose coherence and character development arcs.


      **Technical Approach:**

      Implement a hybrid NER approach combining pre-trained models (spaCy''s transformer
      models) with custom training on literary texts. Use active learning to iteratively
      improve character detection accuracy. Create character relationship graphs and
      coreference resolution to handle pronouns and character aliases. Integration
      with existing LLM pipeline for context-aware character attribute extraction
      (appearance, personality traits).


      **Dependencies:**

      - External: spaCy v3.7+, transformers, datasets, torch, scikit-learn, networkx

      - Internal: Novel preprocessing service, LLM orchestration service, character
      database schema, training data pipeline


      **Risks:**

      - Model drift: Regular retraining needed as literary styles vary significantly

      - Coreference resolution complexity: Pronouns and aliases create ambiguity -
      use graph-based entity linking

      - Training data quality: Limited annotated literary datasets - implement active
      learning with human-in-the-loop

      - Performance at scale: Large novels may exceed context windows - implement
      sliding window approach


      **Complexity Notes:**

      More complex than initially estimated due to literary text nuances (archaic
      names, fantasy characters, multiple aliases). Requires domain-specific training
      data and sophisticated coreference resolution. However, leveraging pre-trained
      models reduces training time significantly.


      **Key Files:**

      - apps/ml-service/src/services/character-extraction.ts: Main NER service

      - apps/ml-service/src/models/character-ner/: Custom model training pipeline

      - apps/ml-service/src/training/: Training data preparation and active learning

      - packages/shared/src/types/character.ts: Character entity types

      - apps/ml-service/src/services/coreference-resolver.ts: Entity linking service

      '
    design_decisions:
    - decision: Use spaCy with custom transformer training rather than pure OpenAI
        API
      rationale: Better control over model behavior, cost efficiency for batch processing,
        and ability to fine-tune on domain-specific literary data
      alternatives_considered:
      - Pure LLM prompting
      - BERT-based custom training
      - Rule-based regex extraction
    - decision: Implement character relationship graph using NetworkX
      rationale: Enables complex relationship modeling, supports graph algorithms
        for character importance scoring, and integrates well with visualization tools
      alternatives_considered:
      - Simple adjacency lists
      - Graph databases like Neo4j
      - Custom graph implementation
    - decision: Use active learning with human feedback loop
      rationale: Literary character extraction requires domain expertise, limited
        labeled data available, and quality is critical for downstream comic generation
      alternatives_considered:
      - Fully automated training
      - Crowdsourced labeling
      - Transfer learning only
    researched_at: '2026-02-07T19:01:26.636934'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-07T19:39:42.583345'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a two-stage pipeline: (1) Initial character detection using spaCy''s
      transformer-based NER with custom literary training data, (2) Coreference resolution
      and relationship mapping using graph-based entity linking. Implement active
      learning to continuously improve model performance through human feedback. Create
      character profiles with extracted attributes (physical descriptions, relationships,
      dialogue patterns) stored in Supabase for comic generation consistency.

      '
    external_dependencies:
    - name: spacy
      version: ^3.7.0
      reason: Production-ready NER with transformer support and custom training capabilities
    - name: '@huggingface/transformers'
      version: ^4.35.0
      reason: Pre-trained transformer models for literary text understanding
    - name: networkx
      version: ^3.2
      reason: Character relationship graph modeling and analysis (Python interop via
        child_process)
    - name: scikit-learn
      version: ^1.3.0
      reason: Active learning algorithms and model evaluation metrics
    - name: datasets
      version: ^2.14.0
      reason: Training data management and preprocessing pipeline
    - name: torch
      version: ^2.1.0
      reason: Custom transformer model fine-tuning and inference
    files_to_modify:
    - path: apps/ml-service/src/routes/character.ts
      changes: Add endpoints for character extraction, training status, and active
        learning feedback
    - path: apps/ml-service/package.json
      changes: Add spaCy, transformers, torch, networkx dependencies
    - path: packages/shared/src/types/api.ts
      changes: Add character extraction request/response types
    - path: apps/ml-service/src/config/ml-config.ts
      changes: Add NER model paths, training parameters, and performance thresholds
    new_files:
    - path: apps/ml-service/src/services/character-extraction.ts
      purpose: Main NER service with spaCy integration and custom model handling
    - path: apps/ml-service/src/services/coreference-resolver.ts
      purpose: Entity linking and coreference resolution using graph algorithms
    - path: apps/ml-service/src/models/character-ner/trainer.ts
      purpose: Custom model training pipeline with active learning capabilities
    - path: apps/ml-service/src/models/character-ner/evaluator.ts
      purpose: Model performance evaluation and benchmarking
    - path: apps/ml-service/src/training/active-learner.ts
      purpose: Active learning strategy implementation for iterative improvement
    - path: apps/ml-service/src/training/data-preprocessor.ts
      purpose: Literary text preprocessing and training data preparation
    - path: apps/ml-service/src/utils/character-graph.ts
      purpose: Character relationship graph construction and manipulation
    - path: packages/shared/src/types/character.ts
      purpose: Character entity types, relationships, and attributes
    - path: apps/ml-service/src/db/character-repository.ts
      purpose: Database operations for character profiles and training data
    - path: apps/ml-service/python/ner_training.py
      purpose: Python training script for spaCy model fine-tuning
    - path: apps/ml-service/python/requirements.txt
      purpose: Python dependencies for ML training pipeline
  acceptance_criteria:
  - criterion: Character extraction achieves >85% precision and recall on literary
      test dataset
    verification: 'Run evaluation script: npm run test:character-extraction -- --benchmark'
  - criterion: System correctly resolves character coreferences (pronouns, aliases)
      with >80% accuracy
    verification: 'Execute coreference test suite: npm test -- coreference-resolver.test.ts'
  - criterion: Character relationship graphs are generated with proper entity linking
    verification: Manual review of relationship visualization and automated graph
      structure validation
  - criterion: Active learning pipeline successfully improves model performance over
      100 training iterations
    verification: Monitor training metrics dashboard showing accuracy improvement
      curve
  - criterion: API handles full-length novels (100k+ words) within 60 seconds processing
      time
    verification: 'Load test with sample novels: npm run test:load -- --target=character-extraction'
  testing:
    unit_tests:
    - file: apps/ml-service/src/__tests__/character-extraction.test.ts
      coverage_target: 90%
      scenarios:
      - Basic character detection from text passages
      - Handling of archaic/fantasy character names
      - Error handling for malformed input text
      - Character attribute extraction accuracy
    - file: apps/ml-service/src/__tests__/coreference-resolver.test.ts
      coverage_target: 85%
      scenarios:
      - Pronoun resolution to correct characters
      - Alias mapping and entity linking
      - Multiple character disambiguation
      - Cross-chapter character tracking
    - file: apps/ml-service/src/__tests__/character-training.test.ts
      coverage_target: 80%
      scenarios:
      - Training data preparation and validation
      - Active learning sample selection
      - Model serialization and loading
    integration_tests:
    - file: apps/ml-service/src/__tests__/integration/character-pipeline.test.ts
      scenarios:
      - End-to-end character extraction from novel upload to character profiles
      - Integration with LLM orchestration service
      - Database storage and retrieval of character data
      - Performance with varying novel lengths and styles
    manual_testing:
    - step: Upload test novels (fantasy, classic literature, modern fiction)
      expected: Characters extracted with relationships mapped correctly
    - step: Review active learning interface for annotation quality
      expected: Human feedback properly incorporated into training loop
    - step: Validate character profile completeness and accuracy
      expected: Physical descriptions, relationships, and traits properly extracted
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup Python environment and install spaCy, transformers dependencies
      done: false
    - task: Implement character extraction service with pre-trained model integration
      done: false
    - task: Build coreference resolution system with graph-based entity linking
      done: false
    - task: Create training data pipeline and active learning infrastructure
      done: false
    - task: Develop custom NER model training with literary domain adaptation
      done: false
    - task: Implement character relationship graph generation and storage
      done: false
    - task: Build evaluation framework and performance benchmarking
      done: false
    - task: Create API endpoints and integrate with existing LLM pipeline
      done: false
    - task: Implement sliding window processing for large novels
      done: false
    - task: Setup monitoring, logging, and error handling for production deployment
      done: false
- key: T34
  title: Training Data Generation Service
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p1
  effort: 5
  area: ml
  dependsOn:
  - T21
  agent_notes:
    research_findings: '**Context:**

      Training Data Generation Service is critical for creating high-quality datasets
      to fine-tune ML models for novel-to-comic transformation. This service will
      generate synthetic training pairs of text descriptions and corresponding comic
      panel descriptions/styles, augment existing data, and create domain-specific
      datasets for character consistency, scene composition, and visual storytelling.
      Without quality training data, our comic generation models will produce inconsistent
      results and poor visual coherence.


      **Technical Approach:**

      Build a dedicated microservice using Fastify 5 that orchestrates data generation
      pipelines. Use OpenAI/Anthropic LLMs for text augmentation and synthetic scene
      generation, integrate with existing Stable Diffusion endpoints for image generation
      validation. Implement queue-based processing with BullMQ for batch operations,
      store generated datasets in Supabase with proper versioning and metadata tracking.
      Use Zod schemas for data validation and implement streaming responses for large
      dataset generation.


      **Dependencies:**

      - External: [@bull-board/fastify, bullmq, zod, @faker-js/faker, csv-parser,
      sharp, node-fetch]

      - Internal: Existing LLM service abstraction, RunPod Stable Diffusion service,
      Supabase database schemas, shared validation schemas


      **Risks:**

      - Data Quality Degradation: Implement multi-stage validation and human-in-the-loop
      review processes

      - Cost Explosion: Set strict API rate limits, implement cost tracking, and use
      cheaper models for bulk generation

      - Storage Bloat: Implement data lifecycle policies, compression strategies,
      and archival systems

      - Bias Amplification: Diversify data sources and implement bias detection metrics


      **Complexity Notes:**

      More complex than initially estimated due to need for sophisticated data validation
      pipelines and integration with multiple ML services. The challenge lies in maintaining
      data quality at scale while managing costs and ensuring reproducibility.


      **Key Files:**

      - apps/backend/src/services/training-data-generator.ts: Core service implementation

      - apps/backend/src/routes/training-data/: API endpoints for dataset management

      - packages/shared/src/schemas/training-data.ts: Validation schemas

      - apps/backend/src/workers/data-generation.worker.ts: Background processing
      logic

      '
    design_decisions:
    - decision: Queue-based architecture with BullMQ for dataset generation
      rationale: Training data generation is inherently batch-oriented and resource-intensive,
        requiring proper job management, retry logic, and progress tracking
      alternatives_considered:
      - Direct API processing
      - Serverless functions
      - Event-driven architecture
    - decision: Multi-tier data validation with LLM-assisted quality scoring
      rationale: Ensures generated training data meets quality thresholds before expensive
        fine-tuning operations
      alternatives_considered:
      - Rule-based validation only
      - Human review only
      - No validation
    - decision: Versioned dataset storage with immutable snapshots
      rationale: Enables reproducible training runs and A/B testing of different dataset
        versions
      alternatives_considered:
      - Mutable datasets
      - Git-based versioning
      - External dataset services
    researched_at: '2026-02-07T19:01:48.720703'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:26:03.056887'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a Fastify-based microservice that orchestrates training data
      generation through configurable pipelines. Use BullMQ for queue management with
      Redis, integrating with existing LLM services for text generation and Stable
      Diffusion for validation. Store datasets in Supabase with proper metadata and
      versioning, implementing streaming APIs for large dataset downloads. Include
      cost tracking, quality metrics, and automated validation workflows.

      '
    external_dependencies:
    - name: bullmq
      version: ^5.0.0
      reason: Redis-based queue system for batch data generation jobs
    - name: '@bull-board/fastify'
      version: ^5.0.0
      reason: Web UI for monitoring data generation job queues
    - name: '@faker-js/faker'
      version: ^8.0.0
      reason: Generate synthetic metadata and augment training examples
    - name: csv-parser
      version: ^3.0.0
      reason: Parse and process external dataset imports
    - name: sharp
      version: ^0.32.0
      reason: Image processing and validation for generated visual data
    files_to_modify:
    - path: apps/backend/src/index.ts
      changes: Register training data routes and BullMQ queue connections
    - path: apps/backend/src/lib/database.ts
      changes: Add dataset storage helpers and versioning utilities
    - path: packages/shared/src/types/index.ts
      changes: Export training data types for frontend consumption
    new_files:
    - path: apps/backend/src/services/training-data-generator.ts
      purpose: Core orchestration service for data generation pipelines
    - path: apps/backend/src/routes/training-data/index.ts
      purpose: API route registration and middleware setup
    - path: apps/backend/src/routes/training-data/generate.ts
      purpose: POST endpoint for initiating dataset generation jobs
    - path: apps/backend/src/routes/training-data/datasets.ts
      purpose: CRUD operations for dataset management and metadata
    - path: apps/backend/src/routes/training-data/download.ts
      purpose: Streaming download endpoint for large datasets
    - path: apps/backend/src/workers/data-generation.worker.ts
      purpose: BullMQ worker for background dataset generation processing
    - path: packages/shared/src/schemas/training-data.ts
      purpose: Zod validation schemas for all training data structures
    - path: apps/backend/src/lib/cost-tracker.ts
      purpose: API usage cost monitoring and budget enforcement
    - path: apps/backend/src/lib/data-templates.ts
      purpose: Configurable templates for different dataset types
    - path: apps/backend/src/lib/quality-validator.ts
      purpose: Multi-stage validation pipeline for generated data quality
    - path: database/migrations/20241201_training_data_tables.sql
      purpose: Supabase schema for datasets, generations, and metadata
  acceptance_criteria:
  - criterion: Service generates high-quality synthetic training pairs (text + comic
      descriptions) with configurable templates and LLM prompts
    verification: POST /api/v1/training-data/generate with template_id returns valid
      dataset with >90% schema compliance rate
  - criterion: Queue-based processing handles batch generation of 1000+ samples without
      memory overflow or timeouts
    verification: BullMQ dashboard shows successful completion of batch job generating
      1000 samples within 30 minutes
  - criterion: Dataset versioning and metadata tracking maintains full lineage from
      generation parameters to final output
    verification: GET /api/v1/training-data/datasets/{id}/metadata returns complete
      generation config, timestamps, and quality metrics
  - criterion: Cost tracking accurately monitors API usage across OpenAI/Anthropic
      services with configurable budget limits
    verification: Cost dashboard shows real-time spend tracking and auto-pauses generation
      when budget threshold exceeded
  - criterion: Streaming API efficiently delivers large datasets (>100MB) without
      server memory issues
    verification: GET /api/v1/training-data/datasets/{id}/download streams 500MB dataset
      with <2GB server memory usage
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/training-data-generator.test.ts
      coverage_target: 90%
      scenarios:
      - Template-based generation with valid schemas
      - LLM service integration with mocked responses
      - Data validation pipeline with edge cases
      - Cost calculation accuracy
      - Error handling for external service failures
    - file: apps/backend/src/__tests__/workers/data-generation.worker.test.ts
      coverage_target: 85%
      scenarios:
      - Queue job processing success/failure
      - Batch processing with memory constraints
      - Progress tracking and status updates
    integration_tests:
    - file: apps/backend/src/__tests__/integration/training-data-flow.test.ts
      scenarios:
      - End-to-end dataset generation from API to storage
      - BullMQ job lifecycle with Redis integration
      - Supabase storage with versioning
      - LLM service integration with rate limiting
    - file: apps/backend/src/__tests__/integration/streaming-api.test.ts
      scenarios:
      - Large dataset streaming performance
      - Concurrent download handling
    manual_testing:
    - step: Generate comic training dataset with character consistency template
      expected: Produces 100 samples with consistent character descriptions across
        scenes
    - step: Monitor cost tracking during high-volume generation
      expected: Real-time cost updates and automatic stopping at budget limit
    - step: Test streaming download of 200MB dataset
      expected: Smooth download with progress indicators and resume capability
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup BullMQ infrastructure and Redis connection
      done: false
    - task: Implement core TrainingDataGenerator service class
      done: false
    - task: Create Zod schemas and validation pipeline
      done: false
    - task: Build API endpoints for generation and dataset management
      done: false
    - task: Implement BullMQ worker for background processing
      done: false
    - task: Add streaming download functionality with memory optimization
      done: false
    - task: Integrate cost tracking and budget enforcement
      done: false
    - task: Create database migrations and Supabase integration
      done: false
    - task: Build quality validation and bias detection metrics
      done: false
    - task: Comprehensive testing and performance optimization
      done: false
- key: T35
  title: Active Learning Loop - User Corrections to Training Data
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p2
  effort: 3
  area: ml
  dependsOn:
  - T34
  agent_notes:
    research_findings: "**Context:**\nActive learning loops are crucial for improving\
      \ ML model quality by incorporating user corrections back into training data.\
      \ For Morpheus, this means when users correct generated scene descriptions,\
      \ character descriptions, or panel compositions, these corrections should be\
      \ captured, validated, and fed back into the training pipeline to improve future\
      \ generations. This creates a continuous improvement cycle that makes the platform\
      \ smarter over time and reduces the need for manual corrections.\n\n**Technical\
      \ Approach:**\nImplement a feedback collection system with PostgreSQL storage,\
      \ asynchronous processing via queues, and integration with the existing ML training\
      \ pipeline. Use a reward/penalty system for training data quality, implement\
      \ data versioning for training datasets, and create validation mechanisms to\
      \ ensure feedback quality. The system should track user corrections at multiple\
      \ levels: text-to-scene conversion, scene-to-image generation, and panel layout\
      \ optimization.\n\n**Dependencies:**\n- External: bull (Redis-based queues),\
      \ zod (validation schemas), pg (PostgreSQL driver), openai SDK\n- Internal:\
      \ Supabase database schema, existing ML services, user authentication system,\
      \ comic generation pipeline\n\n**Risks:**\n- Data quality degradation: Implement\
      \ validation rules and expert review queues for high-impact corrections\n- Training\
      \ data bias: Use stratified sampling and bias detection algorithms to maintain\
      \ dataset balance  \n- Storage costs explosion: Implement data retention policies\
      \ and compress/archive old feedback after model retraining\n- User privacy concerns:\
      \ Anonymize feedback data and provide opt-out mechanisms with clear consent\
      \ flows\n\n**Complexity Notes:**\nMore complex than initially expected due to\
      \ the multi-modal nature (text + images), need for data lineage tracking, and\
      \ integration with multiple ML models. The feedback loop affects both text generation\
      \ (LLMs) and image generation (Stable Diffusion), requiring different training\
      \ approaches for each modality.\n\n**Key Files:**\n- packages/database/schemas/feedback.sql:\
      \ Create feedback tables and indexes\n- apps/api/src/services/feedback-collector.ts:\
      \ Core feedback collection service\n- apps/api/src/workers/training-data-processor.ts:\
      \ Background job for processing corrections\n- packages/ml-training/src/active-learning/feedback-processor.ts:\
      \ ML-specific feedback processing\n"
    design_decisions:
    - decision: Use PostgreSQL JSON columns for flexible feedback storage
      rationale: Allows schema evolution as we discover new feedback types without
        migrations
      alternatives_considered:
      - Separate tables per feedback type
      - MongoDB for document storage
    - decision: Implement asynchronous processing with Redis/Bull queues
      rationale: Prevents blocking user interactions while ensuring reliable processing
      alternatives_considered:
      - Synchronous processing
      - Database-only job queue
      - AWS SQS
    - decision: Create separate feedback pipelines for text and image corrections
      rationale: Different ML models require different training data formats and validation
      alternatives_considered:
      - Single unified feedback pipeline
      - Model-agnostic feedback system
    researched_at: '2026-02-07T19:02:10.459616'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:26:30.940308'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a multi-tier feedback system: (1) Real-time collection APIs that
      capture user corrections during comic editing, (2) Validation layer using Zod
      schemas and business rules to ensure feedback quality, (3) Asynchronous processing
      workers that transform feedback into training-ready formats, (4) Integration
      with existing ML training pipelines to incorporate corrections into model fine-tuning.
      Implement data versioning to track training dataset evolution and A/B testing
      capabilities to validate improvements.

      '
    external_dependencies:
    - name: bull
      version: ^4.12.0
      reason: Redis-based job queues for asynchronous feedback processing
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for Bull queue backend and caching
    - name: '@bull-board/api'
      version: ^5.10.0
      reason: Queue monitoring dashboard for debugging training data processing
    - name: zod
      version: ^3.22.0
      reason: Runtime validation for feedback data schemas and type safety
    files_to_modify:
    - path: apps/api/src/routes/comics.ts
      changes: Add feedback collection endpoints for scene/character corrections
    - path: apps/api/src/middleware/auth.ts
      changes: Add user privacy preference validation for feedback collection
    - path: apps/web/src/components/comic-editor/SceneEditor.tsx
      changes: Integrate feedback collection hooks on user corrections
    - path: packages/database/migrations/002_comics.sql
      changes: Add feedback foreign key relationships to existing tables
    new_files:
    - path: packages/database/schemas/feedback.sql
      purpose: Define feedback tables, indexes, and constraints for corrections storage
    - path: apps/api/src/services/feedback-collector.ts
      purpose: Core service for capturing, validating, and storing user corrections
    - path: apps/api/src/workers/training-data-processor.ts
      purpose: Bull queue worker for async processing of feedback into training format
    - path: packages/ml-training/src/active-learning/feedback-processor.ts
      purpose: ML-specific logic for transforming feedback into model training data
    - path: apps/api/src/routes/feedback.ts
      purpose: REST endpoints for feedback submission, status queries, and privacy
        controls
    - path: packages/shared/src/types/feedback.ts
      purpose: TypeScript interfaces for feedback data structures across packages
    - path: apps/api/src/services/data-quality-validator.ts
      purpose: Validation rules and quality scoring for feedback submissions
    - path: packages/ml-training/src/dataset-versioning/training-dataset-manager.ts
      purpose: Version control and lineage tracking for training datasets
    - path: apps/api/src/queues/feedback-processing.ts
      purpose: Bull queue configuration and job definitions for feedback processing
    - path: packages/database/src/repositories/feedback-repository.ts
      purpose: Data access layer for feedback CRUD operations and queries
  acceptance_criteria:
  - criterion: User corrections are captured and stored when editing comic scenes,
      characters, or panels
    verification: Create comic → make corrections → verify feedback records in database
      with correct user_id, correction_type, and original/corrected data
  - criterion: Feedback processing worker transforms corrections into training-ready
      format within 30 seconds
    verification: Monitor bull queue metrics and database timestamps between feedback
      creation and processed status
  - criterion: Training data versioning tracks dataset evolution with lineage from
      user feedback
    verification: Generate training dataset → verify version increment and feedback_source
      references in training_datasets table
  - criterion: Data validation rejects low-quality feedback and flags suspicious corrections
      for review
    verification: Submit invalid/malicious feedback → verify rejection with appropriate
      error codes and admin queue entries
  - criterion: Privacy controls allow users to opt-out and anonymize their feedback
      data
    verification: Toggle privacy settings → verify feedback anonymization and opt-out
      status in user preferences
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/feedback-collector.test.ts
      coverage_target: 90%
      scenarios:
      - Valid scene correction capture
      - Character description feedback validation
      - Duplicate feedback deduplication
      - Privacy opt-out handling
      - Invalid feedback rejection
    - file: apps/api/src/__tests__/workers/training-data-processor.test.ts
      coverage_target: 85%
      scenarios:
      - Feedback transformation to training format
      - Batch processing optimization
      - Error handling and retry logic
      - Data quality scoring
    - file: packages/ml-training/src/__tests__/active-learning/feedback-processor.test.ts
      coverage_target: 80%
      scenarios:
      - Multi-modal feedback processing
      - Training dataset versioning
      - Bias detection algorithms
      - Stratified sampling validation
    integration_tests:
    - file: apps/api/src/__tests__/integration/feedback-flow.test.ts
      scenarios:
      - End-to-end correction capture to training data
      - Queue processing with Redis integration
      - Database consistency across feedback tables
      - ML pipeline integration
    - file: packages/database/src/__tests__/feedback-schema.test.ts
      scenarios:
      - Feedback table relationships and constraints
      - Data retention policy enforcement
      - Index performance on large datasets
    manual_testing:
    - step: Create comic scene and make text corrections in editor
      expected: Feedback captured with scene_id, correction metadata stored
    - step: Generate character and modify appearance description
      expected: Character feedback logged with before/after image references
    - step: Toggle privacy settings and submit corrections
      expected: Anonymized feedback stored without personally identifiable info
    - step: Submit intentionally bad feedback (spam/offensive)
      expected: Validation rules trigger, feedback flagged for admin review
  estimates:
    development: 6
    code_review: 1
    testing: 2
    documentation: 1
    total: 10
  progress:
    status: not-started
    checklist:
    - task: Design and implement database schema for feedback storage
      done: false
    - task: Create feedback collection service with validation layer
      done: false
    - task: Implement Bull queue workers for async feedback processing
      done: false
    - task: Build ML training pipeline integration for feedback incorporation
      done: false
    - task: Add privacy controls and data anonymization features
      done: false
    - task: Create REST API endpoints for feedback submission and management
      done: false
    - task: Integrate feedback collection into comic editor UI components
      done: false
    - task: Implement data versioning and lineage tracking system
      done: false
    - task: Add monitoring and metrics for feedback processing pipeline
      done: false
    - task: Write comprehensive tests and documentation
      done: false
- key: T36
  title: Dialogue Classification Service - Intent, Tone, Emotion
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I2
  priority: p2
  effort: 5
  area: ml
  dependsOn:
  - T21
  agent_notes:
    research_findings: '**Context:**

      This service analyzes dialogue from novels to classify intent (question, command,
      statement), tone (formal, casual, aggressive), and emotion (happy, sad, angry,
      etc.) for comic generation. This is crucial for visual storytelling as it determines
      character expressions, speech bubble styling, panel composition, and scene atmosphere.
      Without proper dialogue classification, the generated comics would lose emotional
      context and narrative impact.


      **Technical Approach:**

      Implement a hybrid approach using OpenAI''s GPT-4 for complex contextual analysis
      combined with a local classification pipeline using transformers.js for performance
      optimization. Create a service that processes dialogue in batches, caches results,
      and provides structured output for the comic generation pipeline. Use sentiment
      analysis models like cardiffnlp/twitter-roberta-base-sentiment-latest for emotion
      detection and custom prompt engineering for intent classification.


      **Dependencies:**

      - External: @huggingface/transformers, openai, compromise (NLP), sentiment

      - Internal: Depends on text extraction service (likely from novel parsing),
      integrates with comic generation pipeline, uses shared ML utilities


      **Risks:**

      - Model accuracy: Different writing styles may confuse classification - mitigate
      with ensemble methods and confidence scoring

      - Performance bottleneck: Processing large novels could be slow - implement
      batch processing and caching

      - Context loss: Single dialogue analysis without conversation context - maintain
      dialogue history window

      - API costs: OpenAI usage could be expensive - implement smart fallback to local
      models


      **Complexity Notes:**

      More complex than initially estimated due to contextual nuances in literary
      dialogue. Emotion classification especially challenging as literary characters
      often use subtext, sarcasm, and complex emotional states that simple sentiment
      analysis cannot capture.


      **Key Files:**

      - apps/api/src/services/ml/dialogue-classifier.ts: Main classification service

      - packages/shared/src/types/dialogue.ts: Type definitions for classification
      results

      - apps/api/src/routes/ml/dialogue.ts: API endpoints for classification

      - packages/ml-utils/src/dialogue/: Shared utilities and prompt templates

      '
    design_decisions:
    - decision: Hybrid OpenAI + Local Model Approach
      rationale: Balances accuracy (GPT-4 for complex cases) with cost and performance
        (local models for simple cases)
      alternatives_considered:
      - Pure OpenAI approach
      - Pure local transformers
      - Rule-based classification
    - decision: Structured JSON Output Schema
      rationale: Enables reliable parsing for downstream comic generation services
        and allows confidence scoring
      alternatives_considered:
      - Simple string labels
      - Probability arrays
      - Multi-step classification
    researched_at: '2026-02-07T19:02:31.103396'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:26:51.615816'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a dialogue classification service that accepts text input and
      returns structured analysis of intent, tone, and emotion with confidence scores.
      Implement a smart routing system that uses local models for straightforward
      cases and escalates to OpenAI for complex literary dialogue. Create a caching
      layer using Supabase to store classification results and enable batch processing
      for novel-length content. Design the service to maintain conversation context
      windows for improved accuracy.

      '
    external_dependencies:
    - name: '@huggingface/transformers'
      version: ^2.17.2
      reason: Local sentiment analysis and emotion classification models
    - name: compromise
      version: ^14.10.0
      reason: Natural language processing for intent detection and dialogue preprocessing
    - name: sentiment
      version: ^5.0.2
      reason: Lightweight sentiment analysis for tone classification fallback
    - name: string-similarity
      version: ^4.0.4
      reason: Dialogue similarity matching for cache optimization and context analysis
    files_to_modify:
    - path: apps/api/src/app.ts
      changes: Add dialogue classification routes to Express app
    - path: packages/shared/src/types/index.ts
      changes: Export dialogue classification types
    new_files:
    - path: apps/api/src/services/ml/dialogue-classifier.ts
      purpose: Main classification service with hybrid model approach
    - path: packages/shared/src/types/dialogue.ts
      purpose: TypeScript interfaces for classification input/output
    - path: apps/api/src/routes/ml/dialogue.ts
      purpose: REST endpoints for single and batch classification
    - path: packages/ml-utils/src/dialogue/prompt-templates.ts
      purpose: GPT-4 prompt templates for complex dialogue analysis
    - path: packages/ml-utils/src/dialogue/local-classifier.ts
      purpose: Local transformers.js models for basic classification
    - path: packages/ml-utils/src/dialogue/context-manager.ts
      purpose: Maintain conversation history windows for context
    - path: apps/api/src/services/ml/dialogue-cache.ts
      purpose: Supabase caching layer for classification results
    - path: apps/api/src/middleware/batch-processor.ts
      purpose: Handle large novel processing with progress tracking
  acceptance_criteria:
  - criterion: Service correctly classifies dialogue intent (question, command, statement)
      with >80% accuracy on test dataset
    verification: 'Run test suite with labeled dialogue samples: npm test -- dialogue-classifier.test.ts'
  - criterion: Service identifies tone (formal, casual, aggressive) and emotion (happy,
      sad, angry, etc.) with confidence scores 0-1
    verification: API returns structured JSON with intent, tone, emotion fields and
      confidence scores for each
  - criterion: Batch processing handles novel-length content (10,000+ dialogue lines)
      within 5 minutes
    verification: 'Load test with sample novel data: POST /api/ml/dialogue/batch with
      10k samples'
  - criterion: Smart routing uses local models for simple cases and OpenAI for complex
      literary dialogue
    verification: Monitor logs showing model selection logic and verify <30% OpenAI
      usage on mixed dataset
  - criterion: Classification results are cached and retrievable for identical input
    verification: Second identical request returns cached result within 50ms response
      time
  testing:
    unit_tests:
    - file: apps/api/src/services/ml/__tests__/dialogue-classifier.test.ts
      coverage_target: 90%
      scenarios:
      - Intent classification accuracy
      - Tone detection with confidence scores
      - Emotion analysis for complex literary dialogue
      - Caching mechanism
      - Error handling for API failures
      - Batch processing logic
    - file: packages/ml-utils/src/dialogue/__tests__/prompt-templates.test.ts
      coverage_target: 85%
      scenarios:
      - Prompt generation for different dialogue types
      - Context window management
      - Template validation
    integration_tests:
    - file: apps/api/src/__tests__/integration/dialogue-classification.test.ts
      scenarios:
      - End-to-end classification pipeline
      - OpenAI API integration with fallback
      - Supabase caching integration
      - Batch processing with real novel data
    manual_testing:
    - step: Submit complex literary dialogue with subtext via API
      expected: Returns accurate emotion classification despite sarcasm/irony
    - step: Process large batch of dialogue from sample novel
      expected: Completes within time limit with progress tracking
    - step: Verify cache hit/miss behavior in database
      expected: Cached results stored in Supabase with proper TTL
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup project dependencies (@huggingface/transformers, openai, compromise)
      done: false
    - task: Create type definitions and shared interfaces
      done: false
    - task: Implement local classification pipeline with transformers.js
      done: false
    - task: Build OpenAI integration with prompt engineering
      done: false
    - task: Create smart routing logic and confidence thresholding
      done: false
    - task: Implement Supabase caching layer
      done: false
    - task: Build batch processing with progress tracking
      done: false
    - task: Create REST API endpoints
      done: false
    - task: Write comprehensive test suite
      done: false
    - task: Performance testing and optimization
      done: false
    - task: Documentation and API specs
      done: false
    - task: Code review and refinement
      done: false
- key: T41
  title: Scene Extraction Service
  type: Feature
  milestone: M2 - ML Training & Development
  iteration: I3
  priority: p0
  effort: 5
  area: ml
  dependsOn:
  - T40
  - T21
  agent_notes:
    research_findings: '**Context:**

      The Scene Extraction Service is a critical ML component that automatically identifies
      and segments distinct narrative scenes from novel text. This solves the fundamental
      problem of breaking down long-form prose into visually coherent comic panels.
      Each extracted scene becomes a potential comic panel with its own visual composition,
      characters, and setting. This service provides the foundation for subsequent
      image generation and panel layout by creating structured scene metadata including
      dialogue, character positions, setting descriptions, and emotional tone.


      **Technical Approach:**

      Implement a multi-stage NLP pipeline using OpenAI/Anthropic LLMs for semantic
      scene understanding combined with rule-based text analysis. Use a hybrid approach:
      1) Text preprocessing with paragraph/dialogue boundaries 2) LLM-based scene
      boundary detection using few-shot prompting 3) Scene metadata extraction (characters,
      setting, actions, mood) 4) Confidence scoring and human review flagging. Build
      as a dedicated microservice within the ML pipeline, exposing REST endpoints
      for batch and real-time processing. Store results in Supabase with proper indexing
      for scene relationships.


      **Dependencies:**

      - External: @anthropic-ai/sdk, openai, natural, compromise, tiktoken

      - Internal: ML pipeline orchestrator, authentication service, database models
      for scenes/novels


      **Risks:**

      - LLM inconsistency: implement confidence scoring and fallback rules

      - Scene boundary ambiguity: provide manual override interface

      - Token limits with large novels: implement chunking with context preservation

      - Cost explosion: implement caching and incremental processing


      **Complexity Notes:**

      More complex than initially estimated due to narrative structure variety across
      genres. Literary fiction requires different scene detection than action/romance.
      Need to handle flashbacks, dream sequences, and non-linear narratives. Consider
      this a P0 foundational service that other ML components depend on.


      **Key Files:**

      - apps/ml-service/src/services/scene-extraction.ts: core extraction logic

      - apps/ml-service/src/routes/scenes.ts: API endpoints

      - packages/database/src/types/scene.ts: scene data models

      - apps/ml-service/src/prompts/scene-detection.ts: LLM prompt templates

      '
    design_decisions:
    - decision: Hybrid LLM + rule-based approach rather than pure ML
      rationale: Provides deterministic fallbacks when LLMs fail, enables cost optimization
        through rule pre-filtering, and allows domain-specific customization per genre
      alternatives_considered:
      - Pure LLM approach
      - Traditional NLP only
      - Custom trained transformer model
    - decision: Scene confidence scoring with human review queue
      rationale: Ensures quality control for ambiguous scenes while maintaining automation
        for clear boundaries, critical for downstream image generation accuracy
      alternatives_considered:
      - Fully automated with no review
      - Manual scene marking only
    - decision: Incremental processing with scene relationship tracking
      rationale: Enables efficient re-processing when users edit novels, maintains
        narrative continuity context between scenes
      alternatives_considered:
      - Full reprocessing on changes
      - Stateless scene extraction
    researched_at: '2026-02-07T19:02:55.795495'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:27:15.599790'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a FastAPI-style service within the ML microservice that processes
      novel text in configurable chunks while preserving narrative context. Use LLM
      few-shot prompting to identify scene boundaries and extract structured metadata
      (characters, setting, actions, dialogue, mood). Implement confidence scoring
      based on boundary clarity and metadata completeness. Store extracted scenes
      with relationship mapping in Supabase, enabling efficient querying for comic
      generation pipeline.

      '
    external_dependencies:
    - name: '@anthropic-ai/sdk'
      version: ^0.24.0
      reason: Primary LLM for scene analysis with strong instruction following
    - name: openai
      version: ^4.20.0
      reason: Fallback LLM and embedding generation for scene similarity
    - name: natural
      version: ^6.0.0
      reason: Text preprocessing, tokenization, and linguistic analysis
    - name: compromise
      version: ^14.10.0
      reason: Named entity recognition for character and location extraction
    - name: tiktoken
      version: ^1.0.10
      reason: Accurate token counting for LLM request optimization
    files_to_modify:
    - path: packages/database/src/schema.sql
      changes: Add scenes table, scene_metadata table, and indexes for novel_id and
        scene_order
    - path: apps/ml-service/src/app.ts
      changes: Register scene extraction routes and middleware
    - path: packages/shared/src/types/api.ts
      changes: Add SceneExtractionRequest and SceneExtractionResponse types
    new_files:
    - path: apps/ml-service/src/services/scene-extraction.ts
      purpose: Core scene extraction service with LLM integration and chunking logic
    - path: apps/ml-service/src/routes/scenes.ts
      purpose: REST API endpoints for scene extraction operations
    - path: packages/database/src/types/scene.ts
      purpose: TypeScript types and Zod schemas for scene data models
    - path: apps/ml-service/src/prompts/scene-detection.ts
      purpose: LLM prompt templates and few-shot examples for different genres
    - path: apps/ml-service/src/utils/text-chunking.ts
      purpose: Smart text chunking with context preservation utilities
    - path: apps/ml-service/src/services/confidence-scoring.ts
      purpose: Confidence scoring algorithms for scene boundaries and metadata
    - path: apps/ml-service/src/config/extraction-config.ts
      purpose: Configuration for chunk sizes, token limits, and model parameters
    - path: apps/ml-service/src/__tests__/fixtures/sample-novels.ts
      purpose: Test data with manually labeled scene boundaries for accuracy testing
  acceptance_criteria:
  - criterion: Service successfully processes novels of varying lengths (1K-100K words)
      and identifies scene boundaries with >80% accuracy against manually labeled
      test data
    verification: 'Run automated test suite against labeled dataset: `npm test apps/ml-service/src/__tests__/scene-extraction-accuracy.test.ts`'
  - criterion: API returns structured scene metadata including characters, setting,
      actions, dialogue, and mood with confidence scores for each field
    verification: POST request to `/api/ml/scenes/extract` returns JSON with required
      fields and confidence scores 0-1
  - criterion: Service handles token limits gracefully by chunking large novels while
      preserving narrative context across boundaries
    verification: Process 150K word novel and verify no scenes are lost at chunk boundaries,
      check logs for successful chunking
  - criterion: Scene extraction completes within 2 minutes per 10K words of input
      text
    verification: 'Performance test measuring processing time: `npm run perf-test:scene-extraction`'
  - criterion: All extracted scenes are properly stored in Supabase with correct relationships
      and indexing
    verification: Query database after extraction to verify scene records, relationships,
      and run index performance tests
  testing:
    unit_tests:
    - file: apps/ml-service/src/__tests__/services/scene-extraction.test.ts
      coverage_target: 90%
      scenarios:
      - Single scene extraction with all metadata fields
      - Multi-scene novel processing
      - 'Edge cases: dialogue-heavy scenes, action sequences, flashbacks'
      - 'Error handling: invalid text, API failures, token limits'
      - Confidence scoring accuracy
    - file: apps/ml-service/src/__tests__/prompts/scene-detection.test.ts
      coverage_target: 85%
      scenarios:
      - Prompt template generation with different genres
      - Few-shot example selection logic
      - Context window optimization
    integration_tests:
    - file: apps/ml-service/src/__tests__/integration/scene-extraction-flow.test.ts
      scenarios:
      - 'End-to-end: novel upload → scene extraction → database storage'
      - API authentication and authorization
      - Batch processing workflow
      - Error recovery and retry logic
    manual_testing:
    - step: Upload sample novels from different genres (literary fiction, fantasy,
        romance)
      expected: Scenes extracted with genre-appropriate boundaries and metadata
    - step: Test chunking with a 200K word novel
      expected: No context loss at boundaries, all scenes properly identified
    - step: Verify confidence scoring by comparing low vs high confidence scenes
      expected: Low confidence scenes show ambiguous boundaries, high confidence show
        clear narrative breaks
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Set up database schema and types for scene storage
      done: false
    - task: Implement core text chunking with context preservation
      done: false
    - task: Create LLM prompt templates for scene detection and metadata extraction
      done: false
    - task: Build scene extraction service with confidence scoring
      done: false
    - task: Implement REST API endpoints with proper validation
      done: false
    - task: Add batch processing capabilities for large novels
      done: false
    - task: Create comprehensive test suite with labeled datasets
      done: false
    - task: Performance optimization and caching implementation
      done: false
    - task: Integration testing with ML pipeline orchestrator
      done: false
    - task: Documentation and API specification
      done: false
- key: T42
  title: Character Profiling Service
  type: Feature
  milestone: M2 - ML Training & Development
  iteration: I3
  priority: p0
  effort: 5
  area: ml
  dependsOn:
  - T40
  - T22
  agent_notes:
    research_findings: '**Context:**

      Character Profiling Service is critical for consistent visual and narrative
      representation across comic panels. When transforming novels to comics, characters
      must maintain visual consistency, personality traits, and relationships throughout
      the story. This service extracts and maintains character profiles from source
      text, enabling consistent image generation and dialogue adaptation. Without
      this, characters would appear differently across panels, breaking immersion
      and narrative coherence.


      **Technical Approach:**

      Build a character extraction and profiling service using LLMs for text analysis
      and vector embeddings for character relationship mapping. Use OpenAI''s function
      calling to extract structured character data (physical descriptions, personality
      traits, relationships). Store character profiles in Supabase with JSONB columns
      for flexible schema. Implement character embedding similarity search for consistent
      reference retrieval. Create a character validation service that checks generated
      images against stored profiles using CLIP embeddings.


      **Dependencies:**

      - External: @openai/openai, @supabase/supabase-js, @langchain/core, sentence-transformers
      (via Python bridge), sharp, canvas

      - Internal: Novel parsing service, Image generation pipeline, Database schemas,
      Authentication service


      **Risks:**

      - Character extraction accuracy: Use few-shot prompting with validated examples
      and human-in-the-loop validation

      - Inconsistent visual representation: Implement CLIP-based similarity scoring
      between generated images and reference descriptions

      - Performance bottlenecks: Cache character profiles with Redis, batch process
      character extractions

      - Storage costs: Compress character embeddings, use efficient vector storage
      strategies


      **Complexity Notes:**

      More complex than initially estimated. Requires sophisticated NLP for character
      relationship mapping, computer vision integration for visual consistency validation,
      and careful prompt engineering for accurate extraction. The interconnected nature
      of characters (relationships, mentions, aliases) adds significant complexity
      to the data modeling and retrieval systems.


      **Key Files:**

      - packages/ml/src/services/character-profiling.service.ts: Core character extraction
      logic

      - packages/ml/src/models/character.model.ts: Character data structures and validation

      - packages/database/migrations/: Character tables and indexes

      - packages/api/src/routes/characters.ts: Character CRUD endpoints

      - packages/dashboard/src/components/character-editor/: Character management
      UI

      '
    design_decisions:
    - decision: Use LLM function calling for structured character extraction
      rationale: Provides consistent JSON output format and reduces parsing errors
        compared to prompt-only approaches
      alternatives_considered:
      - Regex-based extraction
      - Named entity recognition only
      - Custom fine-tuned models
    - decision: Store character profiles as JSONB in PostgreSQL with vector columns
      rationale: Balances flexibility for evolving character schema with performance
        for similarity searches using pgvector
      alternatives_considered:
      - Pure vector database
      - Graph database
      - Document store
    - decision: Implement character visual consistency validation using CLIP embeddings
      rationale: Enables automated quality assurance by comparing generated images
        to text descriptions
      alternatives_considered:
      - Manual validation only
      - Custom vision model
      - Style transfer approaches
    researched_at: '2026-02-07T19:03:17.905645'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:27:40.085455'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a multi-stage character profiling pipeline: (1) Extract character
      mentions and descriptions from novel text using OpenAI function calling, (2)
      Consolidate character references and resolve aliases using embedding similarity,
      (3) Generate comprehensive character profiles with physical descriptions, personality
      traits, and relationships, (4) Store profiles with vector embeddings for efficient
      retrieval, (5) Validate generated comic images against character profiles using
      CLIP similarity scoring.

      '
    external_dependencies:
    - name: '@openai/openai'
      version: ^4.20.0
      reason: LLM function calling for structured character extraction
    - name: '@langchain/core'
      version: ^0.1.0
      reason: Text chunking and embedding management
    - name: '@supabase/vecs'
      version: ^0.4.0
      reason: Vector similarity search for character matching
    - name: openai-clip
      version: ^1.2.0
      reason: Visual-text similarity validation for character consistency
    - name: compromise
      version: ^14.10.0
      reason: Natural language processing for entity extraction preprocessing
    files_to_modify:
    - path: packages/database/prisma/schema.prisma
      changes: Add Character, CharacterRelationship, and CharacterEmbedding models
        with proper indexes
    - path: packages/api/src/app.ts
      changes: Register character routes and middleware
    - path: packages/ml/src/index.ts
      changes: Export character profiling service and related utilities
    new_files:
    - path: packages/ml/src/services/character-profiling.service.ts
      purpose: Core character extraction, consolidation, and profiling logic
    - path: packages/ml/src/services/character-validation.service.ts
      purpose: CLIP-based visual consistency validation for character images
    - path: packages/ml/src/models/character.model.ts
      purpose: Character data structures, validation schemas, and embedding utilities
    - path: packages/ml/src/utils/embedding.utils.ts
      purpose: Vector embedding generation and similarity calculation utilities
    - path: packages/api/src/routes/characters.ts
      purpose: REST API endpoints for character CRUD operations
    - path: packages/api/src/controllers/character.controller.ts
      purpose: Request handling and business logic for character operations
    - path: packages/database/migrations/20241201000000_add_character_tables.sql
      purpose: Database schema for character profiles, relationships, and embeddings
    - path: packages/dashboard/src/components/character-editor/CharacterProfile.tsx
      purpose: Character profile editing and visualization component
    - path: packages/dashboard/src/components/character-editor/CharacterList.tsx
      purpose: Character listing and management interface
    - path: packages/dashboard/src/pages/characters/index.tsx
      purpose: Main character management page
    - path: packages/ml/src/prompts/character-extraction.prompt.ts
      purpose: LLM prompts for character extraction with few-shot examples
  acceptance_criteria:
  - criterion: Character extraction accurately identifies and profiles at least 90%
      of major characters from novel text with physical descriptions, personality
      traits, and relationships
    verification: Run character extraction on test novels and manually validate against
      ground truth dataset using `npm test character-profiling.accuracy.test.ts`
  - criterion: Character profile API endpoints support full CRUD operations with sub-200ms
      response times for profile retrieval
    verification: Execute load tests with `k6 run packages/api/tests/load/character-endpoints.js`
      and verify 95th percentile < 200ms
  - criterion: Visual consistency validation achieves >0.8 CLIP similarity score between
      generated character images and stored profiles
    verification: 'Generate test character images and run validation pipeline: `npm
      run test:character-validation` with minimum 0.8 similarity threshold'
  - criterion: Character relationship mapping correctly identifies and stores character
      connections with 85% accuracy
    verification: Test relationship extraction on annotated novel samples using `npm
      test character-relationships.test.ts`
  - criterion: System handles character alias resolution and consolidation without
      creating duplicate profiles
    verification: Process novels with character aliases and verify single consolidated
      profile per character in database
  testing:
    unit_tests:
    - file: packages/ml/src/services/__tests__/character-profiling.service.test.ts
      coverage_target: 90%
      scenarios:
      - Character extraction from text passages
      - Profile consolidation and alias resolution
      - Embedding generation and similarity matching
      - Error handling for malformed input
      - Cache hit/miss scenarios
    - file: packages/ml/src/models/__tests__/character.model.test.ts
      coverage_target: 95%
      scenarios:
      - Character data validation
      - Profile serialization/deserialization
      - Relationship mapping validation
    integration_tests:
    - file: packages/api/src/__tests__/integration/character-profiling.test.ts
      scenarios:
      - End-to-end character extraction from novel upload to profile storage
      - Character profile retrieval with relationship data
      - Visual validation pipeline integration
      - Batch processing workflow
    manual_testing:
    - step: Upload test novel through dashboard and trigger character extraction
      expected: Character profiles appear in dashboard with extracted descriptions
        and relationships
    - step: Generate character image and validate against profile
      expected: Validation score >0.8 with detailed similarity breakdown
    - step: Edit character profile through dashboard UI
      expected: Changes persist and trigger re-validation of existing images
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Design and implement database schema for characters, relationships, and
        embeddings
      done: false
    - task: Build character extraction service with OpenAI function calling
      done: false
    - task: Implement character consolidation and alias resolution using embeddings
      done: false
    - task: Create CLIP-based visual validation service
      done: false
    - task: Build REST API endpoints for character management
      done: false
    - task: Develop character editor UI components
      done: false
    - task: Implement caching layer with Redis for performance optimization
      done: false
    - task: Create comprehensive test suite including accuracy validation
      done: false
    - task: Write technical documentation and API specifications
      done: false
    - task: Conduct code review and performance optimization
      done: false
- key: T43
  title: Embedding Generation Service
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I3
  priority: p0
  effort: 3
  area: ml
  dependsOn:
  - T41
  - T42
  agent_notes:
    research_findings: '**Context:**

      The embedding generation service is crucial for Morpheus''s ability to understand
      and process novel content semantically. This service will convert text chunks
      (chapters, scenes, character descriptions) and generated comic panels into high-dimensional
      vector representations that enable:

      - Semantic search across novel content for comic panel generation

      - Character consistency tracking across scenes

      - Style transfer matching between text descriptions and visual outputs

      - Content recommendation and similarity matching in the storefront

      - Training data preparation for fine-tuning custom models


      **Technical Approach:**

      Implement a dedicated embedding microservice using Fastify that handles both
      text and image embeddings:

      - Text embeddings: OpenAI text-embedding-3-large or Anthropic''s embedding models
      for novel content

      - Image embeddings: CLIP models via RunPod for generated comic panels

      - Vector storage in Supabase using pgvector extension

      - Batch processing queue for large novels using BullMQ

      - Caching layer with Redis for frequently accessed embeddings

      - RESTful API with streaming support for real-time embedding generation


      **Dependencies:**

      - External: @openai/openai, @anthropic-ai/anthropic-sdk, @supabase/supabase-js,
      bullmq, ioredis, @tensorflow/tfjs (for CLIP), sharp (image processing)

      - Internal: Depends on content parsing service (T41), needs integration with
      novel processing pipeline, comic generation service will consume embeddings


      **Risks:**

      - Rate limiting from OpenAI/Anthropic APIs: implement exponential backoff and
      request queuing

      - Vector storage costs scaling with content: implement embedding lifecycle management
      and archival

      - CLIP model memory usage on RunPod: batch processing and memory monitoring

      - Embedding drift affecting consistency: version embeddings and maintain model
      consistency


      **Complexity Notes:**

      Higher complexity than initially estimated due to multi-modal requirements (text
      + images) and the need for real-time + batch processing modes. The vector similarity
      search optimization and embedding versioning add significant architectural complexity.


      **Key Files:**

      - apps/api/src/services/embedding/: new service directory

      - apps/api/src/routes/embeddings/: API routes for embedding CRUD

      - packages/database/src/types/: embedding schema types

      - packages/shared/src/ml/: shared ML utilities and types

      '
    design_decisions:
    - decision: Use hybrid embedding approach with OpenAI for text and CLIP for images
      rationale: OpenAI embeddings excel at semantic text understanding while CLIP
        provides superior text-image alignment for comic generation consistency
      alternatives_considered:
      - Single model approach with multimodal transformers
      - Custom trained embeddings
      - Sentence transformers only
    - decision: Store embeddings in Supabase with pgvector for similarity search
      rationale: Keeps vector data co-located with relational data, leverages existing
        Supabase infrastructure, and pgvector provides excellent performance for similarity
        queries
      alternatives_considered:
      - Dedicated vector DB like Pinecone
      - Redis with vector search
      - Elasticsearch with dense vectors
    researched_at: '2026-02-07T19:03:42.237248'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:28:05.205624'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a dedicated embedding service in the Fastify backend that exposes
      REST endpoints for generating, storing, and querying text/image embeddings.
      Implement a job queue system for batch processing large novels while supporting
      real-time embedding generation for interactive features. Use Supabase''s pgvector
      extension for efficient vector similarity search, with Redis caching for frequently
      accessed embeddings. Design the service to handle both OpenAI text embeddings
      and CLIP image embeddings from RunPod instances.

      '
    external_dependencies:
    - name: '@openai/openai'
      version: ^4.20.0
      reason: Text embedding generation for novel content
    - name: '@anthropic-ai/anthropic-sdk'
      version: ^0.17.0
      reason: Alternative text embedding provider for redundancy
    - name: bullmq
      version: ^4.15.0
      reason: Job queue for batch embedding processing
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for embedding cache and job queue
    - name: '@tensorflow/tfjs-node'
      version: ^4.15.0
      reason: CLIP model inference for image embeddings
    - name: pgvector
      version: ^0.1.8
      reason: PostgreSQL extension for vector similarity search
    - name: sharp
      version: ^0.33.0
      reason: Image preprocessing before embedding generation
    files_to_modify:
    - path: apps/api/src/app.ts
      changes: Register embedding routes and initialize BullMQ connection
    - path: packages/database/src/schema/index.ts
      changes: Add embeddings table schema with vector column and indexes
    - path: packages/shared/src/types/index.ts
      changes: Export embedding types and API interfaces
    - path: apps/api/src/config/index.ts
      changes: Add OpenAI, Anthropic, and RunPod API configuration
    new_files:
    - path: apps/api/src/services/embedding/embedding-service.ts
      purpose: Core embedding generation service with OpenAI and CLIP integration
    - path: apps/api/src/services/embedding/vector-store.ts
      purpose: Supabase pgvector operations and similarity search
    - path: apps/api/src/services/embedding/cache-manager.ts
      purpose: Redis caching layer for embeddings and search results
    - path: apps/api/src/services/embedding/embedding-queue.ts
      purpose: BullMQ job processor for batch embedding generation
    - path: apps/api/src/routes/embeddings/index.ts
      purpose: REST API routes for embedding CRUD operations
    - path: apps/api/src/routes/embeddings/text.ts
      purpose: Text embedding generation endpoints
    - path: apps/api/src/routes/embeddings/image.ts
      purpose: Image embedding generation via RunPod
    - path: apps/api/src/routes/embeddings/search.ts
      purpose: Vector similarity search endpoints
    - path: apps/api/src/routes/embeddings/batch.ts
      purpose: Batch processing endpoints for novels
    - path: packages/database/src/types/embedding.ts
      purpose: TypeScript types for embedding data structures
    - path: packages/shared/src/ml/embedding-utils.ts
      purpose: Shared utilities for vector operations and preprocessing
    - path: apps/api/src/middleware/rate-limit.ts
      purpose: Rate limiting middleware for embedding endpoints
    - path: packages/database/migrations/20241201_create_embeddings_table.sql
      purpose: Database migration for embeddings table with pgvector support
  acceptance_criteria:
  - criterion: Text embedding generation produces 3072-dimension vectors for novel
      content with <2s response time for single chapters
    verification: POST /api/embeddings/text with chapter content returns vector array
      of length 3072 within 2000ms
  - criterion: Image embedding generation using CLIP models processes comic panels
      and stores results in pgvector
    verification: POST /api/embeddings/image with comic panel returns embedding ID
      and SELECT from embeddings table shows stored vector
  - criterion: Batch processing queue handles full novel embedding (50+ chapters)
      without memory issues or rate limit failures
    verification: POST /api/embeddings/batch with novel ID completes successfully
      and all chapters have embeddings in database
  - criterion: Vector similarity search returns relevant content with >0.8 cosine
      similarity for semantic queries
    verification: GET /api/embeddings/search with query text returns ranked results
      with similarity scores >0.8
  - criterion: Redis caching reduces embedding lookup time by >50% for frequently
      accessed content
    verification: Performance test shows cached embedding requests <100ms vs >200ms
      for uncached
  testing:
    unit_tests:
    - file: apps/api/src/services/embedding/__tests__/embedding-service.test.ts
      coverage_target: 90%
      scenarios:
      - Text embedding generation success
      - Image embedding processing
      - Cache hit/miss scenarios
      - Rate limit handling with exponential backoff
      - Vector storage and retrieval
      - Batch job creation and processing
    - file: apps/api/src/services/embedding/__tests__/vector-store.test.ts
      coverage_target: 85%
      scenarios:
      - Vector insertion and similarity search
      - Embedding versioning and lifecycle
      - Database connection handling
    integration_tests:
    - file: apps/api/src/__tests__/integration/embedding-flow.test.ts
      scenarios:
      - End-to-end text embedding to vector search
      - Novel batch processing pipeline
      - Cache integration with Redis
      - External API integration (OpenAI/RunPod)
    - file: apps/api/src/__tests__/integration/embedding-queue.test.ts
      scenarios:
      - BullMQ job processing and error handling
      - Concurrent embedding generation
    manual_testing:
    - step: Upload test novel and trigger batch embedding generation
      expected: All chapters processed and stored with embeddings in <10 minutes
    - step: Search for character descriptions using semantic query
      expected: Returns relevant character scenes ranked by similarity
    - step: Generate embeddings for comic panels via RunPod integration
      expected: Image embeddings stored and searchable for style matching
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup database schema and pgvector extension configuration
      done: false
    - task: Implement core embedding service with OpenAI text-embedding-3-large integration
      done: false
    - task: Create CLIP image embedding service with RunPod API integration
      done: false
    - task: Build vector store service with Supabase pgvector operations
      done: false
    - task: Implement Redis caching layer with configurable TTL and eviction policies
      done: false
    - task: Create BullMQ job queue for batch processing with retry logic and monitoring
      done: false
    - task: Build REST API endpoints with input validation and error handling
      done: false
    - task: Implement rate limiting and exponential backoff for external APIs
      done: false
    - task: Add comprehensive logging and monitoring for embedding operations
      done: false
    - task: Create integration with content parsing service for novel processing pipeline
      done: false
- key: T48
  title: Literary Cultural Context Service
  type: Feature
  milestone: M2 - ML Training & Development
  iteration: I3
  priority: p2
  effort: 5
  area: ml
  dependsOn:
  - T40
  agent_notes:
    research_findings: '**Context:**

      Literary Cultural Context Service analyzes novels to extract cultural, historical,
      and literary context that influences visual adaptation decisions. When converting
      novels to comics, understanding the cultural setting, time period, literary
      genre conventions, and thematic elements is crucial for generating appropriate
      visual styles, character designs, and scene compositions. This service feeds
      contextual metadata to the visual generation pipeline, ensuring cultural authenticity
      and genre-appropriate artistic choices.


      **Technical Approach:**

      Implement a multi-stage NLP pipeline using LLMs for cultural analysis:

      1. Text segmentation and context extraction using sliding window approach

      2. Named Entity Recognition for places, time periods, cultural references

      3. Genre classification and literary device identification

      4. Cultural context scoring and metadata generation

      5. Integration with existing ML pipeline through Redis pub/sub

      Built as a microservice within the ML workspaces, exposing REST + WebSocket
      APIs


      **Dependencies:**

      - External: @anthropic-ai/sdk, openai, natural, compromise, date-fns, country-list

      - Internal: shared/types, ml/text-processor, ml/analysis-queue, database/novel-metadata


      **Risks:**

      - Cultural bias in LLM responses: Implement validation against cultural knowledge
      bases

      - High token consumption: Use caching and batch processing strategies

      - Latency impact on novel processing: Implement async processing with progress
      tracking

      - Accuracy of historical context: Cross-reference with structured historical
      data APIs


      **Complexity Notes:**

      More complex than initially estimated due to need for cultural sensitivity validation
      and integration with multiple knowledge sources. Requires careful prompt engineering
      and result validation.


      **Key Files:**

      - apps/ml/src/services/cultural-context.service.ts: Main service implementation

      - apps/ml/src/analyzers/literary-analyzer.ts: Core analysis logic

      - packages/shared/src/types/cultural-context.ts: Type definitions

      - apps/backend/src/routes/novels/context.ts: API endpoint integration

      '
    design_decisions:
    - decision: Use Anthropic Claude for cultural analysis with OpenAI as fallback
      rationale: Claude shows better performance on nuanced cultural and literary
        analysis tasks
      alternatives_considered:
      - OpenAI GPT-4 only
      - Hybrid ensemble approach
      - Fine-tuned local model
    - decision: Implement sliding window text analysis with overlap
      rationale: Preserves context across chunk boundaries while managing token limits
      alternatives_considered:
      - Chapter-based analysis
      - Full text analysis
      - Sentence-level processing
    - decision: Cache cultural context results in Redis with novel content hash
      rationale: Expensive LLM operations should be cached, hash ensures cache invalidation
        on content changes
      alternatives_considered:
      - Database caching
      - Memory-only caching
      - No caching
    researched_at: '2026-02-07T19:04:03.508795'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:28:33.000564'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a staged analysis pipeline that processes novel text in overlapping
      chunks, extracts cultural markers through LLM prompting, validates results against
      knowledge bases, and generates structured metadata. Use async queue processing
      to handle multiple novels concurrently while managing API rate limits. Integrate
      with existing novel processing workflow through event-driven architecture.

      '
    external_dependencies:
    - name: '@anthropic-ai/sdk'
      version: ^0.17.0
      reason: Primary LLM for cultural and literary analysis
    - name: natural
      version: ^6.0.0
      reason: Text processing utilities for tokenization and NLP preprocessing
    - name: compromise
      version: ^14.0.0
      reason: Natural language understanding for entity extraction
    - name: country-list
      version: ^2.3.0
      reason: Standardized country/culture identification
    - name: date-fns
      version: ^3.0.0
      reason: Historical date parsing and normalization
    files_to_modify:
    - path: apps/ml/src/services/ml-pipeline.service.ts
      changes: Add cultural context integration, subscribe to Redis cultural-analysis-complete
        events
    - path: apps/backend/src/routes/novels/index.ts
      changes: Add route handlers for cultural context endpoints, WebSocket upgrade
        handling
    - path: packages/shared/src/types/novel.types.ts
      changes: Extend Novel interface with cultural_context field and metadata structure
    - path: apps/ml/src/config/redis.config.ts
      changes: Add cultural context pub/sub channel configurations
    new_files:
    - path: apps/ml/src/services/cultural-context.service.ts
      purpose: Main orchestration service for cultural analysis pipeline with LLM
        integration
    - path: apps/ml/src/analyzers/literary-analyzer.ts
      purpose: Core NLP analysis logic for genre classification and literary device
        detection
    - path: apps/ml/src/analyzers/cultural-validator.ts
      purpose: Cultural bias detection and validation against knowledge bases
    - path: packages/shared/src/types/cultural-context.ts
      purpose: TypeScript interfaces for cultural metadata, analysis results, and
        API contracts
    - path: apps/backend/src/routes/novels/context.ts
      purpose: REST API endpoints for cultural analysis requests and status checking
    - path: apps/ml/src/processors/text-segmenter.ts
      purpose: Sliding window text segmentation with overlap handling for context
        preservation
    - path: apps/ml/src/utils/cultural-knowledge.ts
      purpose: Integration with external cultural and historical knowledge APIs for
        validation
    - path: apps/ml/src/queues/cultural-analysis.queue.ts
      purpose: Bull queue processor for async cultural analysis with progress tracking
  acceptance_criteria:
  - criterion: Cultural context service successfully analyzes novel text and extracts
      structured cultural metadata including time period, geographical setting, cultural
      references, and literary genre
    verification: POST /api/novels/:id/analyze-context returns structured JSON with
      cultural_period, geographical_context, literary_genre, and cultural_references
      fields populated
  - criterion: Service processes novel text in chunks with 90%+ accuracy for major
      cultural markers (time period, location, genre)
    verification: Run test suite with sample novels from different eras/cultures,
      verify extracted metadata against ground truth with accuracy >= 90%
  - criterion: Integration with ML pipeline completes end-to-end with cultural metadata
      flowing to visual generation
    verification: Redis pub/sub message 'cultural-analysis-complete' triggers visual
      pipeline with cultural context payload, verified in ml-pipeline logs
  - criterion: Service handles concurrent novel processing with rate limiting and
      progress tracking
    verification: Submit 5 novels simultaneously, verify all complete within 10 minutes
      with progress updates via WebSocket, no rate limit errors
  - criterion: Cultural bias validation rejects or flags potentially biased/inaccurate
      cultural interpretations
    verification: Test with novels containing sensitive cultural content, verify validation
      layer flags or corrects misinterpretations in cultural_validation_notes field
  testing:
    unit_tests:
    - file: apps/ml/src/__tests__/cultural-context.service.test.ts
      coverage_target: 90%
      scenarios:
      - Text segmentation with sliding window
      - NER extraction for places and dates
      - Genre classification accuracy
      - LLM prompt engineering edge cases
      - Cultural bias detection
      - Error handling for API failures
    - file: apps/ml/src/__tests__/literary-analyzer.test.ts
      coverage_target: 85%
      scenarios:
      - Literary device identification
      - Historical context scoring
      - Cultural reference validation
      - Batch processing logic
    integration_tests:
    - file: apps/ml/src/__tests__/integration/cultural-pipeline.test.ts
      scenarios:
      - End-to-end novel text to cultural metadata pipeline
      - Redis pub/sub integration with ML queue
      - Database persistence of cultural context
      - WebSocket progress updates
    - file: apps/backend/src/__tests__/integration/context-api.test.ts
      scenarios:
      - REST API endpoints for cultural analysis
      - Authentication and authorization
      - Rate limiting behavior
    manual_testing:
    - step: Upload Pride and Prejudice text via API
      expected: Returns Regency England cultural context, romance genre, historical
        references to social class
    - step: Process 1984 and monitor WebSocket progress
      expected: Real-time progress updates, dystopian genre detection, 20th century
        British context
    - step: Submit non-English novel with cultural sensitivity
      expected: Appropriate cultural context without stereotypes, validation flags
        if needed
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Set up project structure and install dependencies (@anthropic-ai/sdk,
        openai, natural, compromise)
      done: false
    - task: Implement text segmentation service with sliding window approach and context
        overlap
      done: false
    - task: Build literary analyzer with NER, genre classification, and cultural reference
        extraction
      done: false
    - task: Create cultural validation layer with bias detection and knowledge base
        cross-referencing
      done: false
    - task: Implement main cultural context service with LLM orchestration and prompt
        engineering
      done: false
    - task: Set up Redis pub/sub integration and Bull queue for async processing with
        progress tracking
      done: false
    - task: Build REST API endpoints with authentication, rate limiting, and WebSocket
        support
      done: false
    - task: Integrate with existing ML pipeline and database schema updates
      done: false
    - task: Implement comprehensive test suite including cultural sensitivity test
        cases
      done: false
    - task: Performance optimization, caching strategy, and API rate limit management
      done: false
- key: T49
  title: Book Timeline Extractor
  type: Feature
  milestone: M2 - ML Training & Development
  iteration: I3
  priority: p2
  effort: 3
  area: ml
  dependsOn:
  - T41
  agent_notes:
    research_findings: "**Context:**\nThe Book Timeline Extractor is essential for\
      \ converting novels into comics by identifying and sequencing key narrative\
      \ events. This component analyzes full-text novels to extract a chronological\
      \ timeline of major plot points, character interactions, and scene changes.\
      \ This structured timeline becomes the foundation for panel generation, ensuring\
      \ the comic adaptation maintains narrative coherence and proper pacing. Without\
      \ this, the ML pipeline would struggle to create coherent visual sequences from\
      \ unstructured text.\n\n**Technical Approach:**\nImplement a hybrid approach\
      \ combining LLM-based content analysis with rule-based text processing. Use\
      \ OpenAI/Anthropic models for semantic understanding of plot progression, while\
      \ leveraging spaCy for linguistic analysis (named entity recognition, temporal\
      \ expressions, dialogue detection). Design as a streaming pipeline to handle\
      \ large novels efficiently, with chunked processing and intermediate result\
      \ storage in PostgreSQL. Integrate with existing ML workflow infrastructure\
      \ and provide REST endpoints for dashboard monitoring.\n\n**Dependencies:**\n\
      - External: spacy (NLP), tiktoken (token counting), date-fns (temporal parsing),\
      \ zod (validation)\n- Internal: ML service infrastructure, database models,\
      \ OpenAI/Anthropic integration layer, job queue system\n\n**Risks:**\n- Memory\
      \ exhaustion with large novels: implement streaming with configurable chunk\
      \ sizes and Redis caching\n- LLM inconsistency across chunks: use few-shot prompting\
      \ with consistent examples and validation schemas\n- Processing time/cost: implement\
      \ smart caching, result reuse, and progress tracking with user notifications\n\
      - Timeline coherence issues: add validation layer that cross-references extracted\
      \ events for logical consistency\n\n**Complexity Notes:**\nHigher complexity\
      \ than initially apparent due to need for maintaining narrative coherence across\
      \ large text volumes. Requires sophisticated prompt engineering, temporal reasoning,\
      \ and robust error handling. The streaming architecture and result validation\
      \ add significant engineering overhead beyond basic text processing.\n\n**Key\
      \ Files:**\n- apps/api/src/services/ml/timeline-extractor.ts: core extraction\
      \ service\n- apps/api/src/routes/books/timeline.ts: API endpoints  \n- packages/database/src/schema/timelines.sql:\
      \ timeline data models\n- apps/api/src/jobs/timeline-extraction.ts: background\
      \ job processing\n"
    design_decisions:
    - decision: Hybrid LLM + NLP approach with streaming architecture
      rationale: LLMs provide semantic understanding while spaCy handles structured
        linguistic analysis. Streaming prevents memory issues with large novels and
        enables progress tracking.
      alternatives_considered:
      - Pure LLM approach
      - Rule-based only
      - Vector embedding similarity
    - decision: PostgreSQL storage with JSON timeline format
      rationale: Leverages existing Supabase infrastructure while providing flexible
        schema for timeline events. JSON columns allow complex event metadata without
        rigid structure.
      alternatives_considered:
      - Separate timeline database
      - Redis-only caching
      - File-based storage
    researched_at: '2026-02-07T19:04:25.701638'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:28:55.270234'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a streaming timeline extraction service that processes novels
      in configurable chunks (2000-4000 tokens). Each chunk gets analyzed by LLM for
      key events, then correlated with spaCy''s temporal and entity extraction. Events
      are stored with timestamps, character references, and confidence scores in PostgreSQL.
      A validation layer ensures timeline coherence by cross-referencing extracted
      events. The service integrates with the existing job queue for background processing
      and provides real-time progress updates via WebSocket connections to the dashboard.

      '
    external_dependencies:
    - name: spacy
      version: ^3.7.0
      reason: Advanced NLP for named entity recognition, temporal expressions, and
        linguistic analysis
    - name: tiktoken
      version: ^1.0.0
      reason: Accurate token counting for LLM API calls and chunk size management
    - name: date-fns
      version: ^3.0.0
      reason: Robust temporal expression parsing and timeline sequencing
    - name: zod
      version: ^3.22.0
      reason: Runtime validation for timeline event schemas and LLM response parsing
    files_to_modify:
    - path: packages/database/src/schema/index.ts
      changes: Export timeline table schemas and types
    - path: apps/api/src/config/openai.ts
      changes: Add timeline extraction prompts and model configurations
    - path: apps/api/src/middleware/validation.ts
      changes: Add timeline request validation schemas
    - path: apps/api/src/routes/books/index.ts
      changes: Import and mount timeline routes
    - path: apps/api/src/jobs/index.ts
      changes: Register timeline extraction job handlers
    new_files:
    - path: packages/database/src/schema/timelines.sql
      purpose: Database schema for timeline events, metadata, and processing status
    - path: apps/api/src/services/ml/timeline-extractor.ts
      purpose: Core timeline extraction service with streaming and LLM integration
    - path: apps/api/src/services/ml/timeline-validator.ts
      purpose: Timeline coherence validation and event correlation logic
    - path: apps/api/src/services/ml/text-chunker.ts
      purpose: Intelligent text chunking with context preservation
    - path: apps/api/src/routes/books/timeline.ts
      purpose: REST API endpoints for timeline extraction and retrieval
    - path: apps/api/src/jobs/timeline-extraction.ts
      purpose: Background job processing for timeline extraction
    - path: apps/api/src/types/timeline.ts
      purpose: TypeScript types and Zod schemas for timeline data
    - path: apps/api/src/utils/temporal-parser.ts
      purpose: Utility functions for parsing temporal expressions with date-fns
  acceptance_criteria:
  - criterion: Timeline extractor processes novels in streaming chunks and extracts
      chronological events with 90%+ accuracy on test corpus
    verification: Run test suite with sample novels, verify extracted events match
      manually annotated timelines
  - criterion: System handles novels up to 500MB with <16GB memory usage and provides
      progress updates every 30 seconds
    verification: Load test with large novels, monitor memory usage with htop, verify
      WebSocket progress events
  - criterion: Background job processing completes within 2x estimated time and recovers
      from failures
    verification: Submit timeline extraction jobs via API, verify completion and retry
      behavior in job queue dashboard
  - criterion: API endpoints return proper responses with timeline data and validation
      errors
    verification: Test POST /books/:id/timeline and GET /books/:id/timeline endpoints
      with curl/Postman
  - criterion: Timeline coherence validation catches 95% of inconsistent temporal
      references
    verification: Test with novels containing timeline inconsistencies, verify validation
      layer flags them
  testing:
    unit_tests:
    - file: apps/api/src/services/ml/__tests__/timeline-extractor.test.ts
      coverage_target: 90%
      scenarios:
      - Chunk processing with valid novel text
      - Event extraction and temporal parsing
      - Error handling for malformed text
      - Memory management and streaming
      - LLM response parsing and validation
    - file: apps/api/src/services/ml/__tests__/timeline-validator.test.ts
      coverage_target: 85%
      scenarios:
      - Timeline coherence validation
      - Event correlation and deduplication
      - Confidence score calculation
    integration_tests:
    - file: apps/api/src/__tests__/integration/timeline-extraction.test.ts
      scenarios:
      - End-to-end timeline extraction flow
      - Database persistence and retrieval
      - Job queue integration
      - WebSocket progress updates
    - file: apps/api/src/__tests__/integration/timeline-api.test.ts
      scenarios:
      - API endpoint responses
      - Authentication and authorization
      - Error handling and validation
    manual_testing:
    - step: Upload a short novel (50 pages) through dashboard
      expected: Timeline extraction completes in <5 minutes with progress updates
    - step: Test with novel containing complex flashbacks
      expected: Timeline correctly sequences events chronologically
    - step: Submit extraction for same novel twice
      expected: Second request uses cached results, completes quickly
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Create database schema and migrations for timeline storage
      done: false
    - task: Implement text chunking service with context preservation
      done: false
    - task: Build core timeline extractor with LLM integration
      done: false
    - task: Develop timeline validation and coherence checking
      done: false
    - task: Create REST API endpoints with proper validation
      done: false
    - task: Implement background job processing with progress tracking
      done: false
    - task: Add WebSocket integration for real-time updates
      done: false
    - task: Write comprehensive test suite
      done: false
    - task: Performance testing and optimization
      done: false
    - task: Documentation and code review
      done: false
- key: T58
  title: SDXL LoRA Training
  type: Task
  milestone: M2 - ML Training & Development
  iteration: I4
  priority: p1
  effort: 8
  area: ml
  dependsOn:
  - T52
  agent_notes:
    research_findings: '**Context:**

      SDXL LoRA (Low-Rank Adaptation) training enables fine-tuning Stable Diffusion
      XL models with custom artistic styles, character consistency, and visual themes
      specific to comic book aesthetics. For Morpheus, this solves the critical problem
      of generating consistent character appearances and maintaining artistic coherence
      across comic panels. Instead of relying on generic SDXL outputs, LoRA training
      allows us to create specialized models that understand specific art styles,
      character designs, and visual narratives that align with the source novel''s
      themes.


      **Technical Approach:**

      Implement LoRA training pipeline using RunPod''s GPU infrastructure with Kohya-ss/sd-scripts
      framework. Create a training orchestration service in the backend that manages
      dataset preparation, training job submission, and model artifact storage. Use
      Supabase for storing training metadata, progress tracking, and model versioning.
      Integrate with existing ML pipeline through a dedicated LoRA training API that
      accepts training datasets, hyperparameters, and returns trained model weights.


      **Dependencies:**

      - External: kohya-ss/sd-scripts, diffusers>=0.21.0, transformers, torch>=2.0.0,
      accelerate, xformers

      - Internal: ml-service (for RunPod integration), database schemas for training
      jobs, file storage service for datasets and model weights


      **Risks:**

      - GPU cost overruns: Implement training time limits and cost monitoring with
      automatic job termination

      - Training instability: Use proven hyperparameter sets and gradient checkpointing
      to prevent divergence

      - Storage bloat: Implement model pruning and automatic cleanup of failed training
      artifacts

      - Quality inconsistency: Establish validation metrics and automated quality
      assessment before model deployment


      **Complexity Notes:**

      Higher complexity than initially estimated due to distributed training orchestration,
      dataset preprocessing pipelines, and integration with existing ML infrastructure.
      The need for training progress monitoring, automatic failure recovery, and model
      quality validation adds significant engineering overhead beyond basic LoRA implementation.


      **Key Files:**

      - packages/ml-service/src/training/lora-trainer.ts: Core training orchestration
      logic

      - packages/ml-service/src/services/runpod-training.ts: RunPod GPU cluster integration

      - packages/database/migrations/: Training jobs and model metadata schemas

      - apps/dashboard/src/pages/training/: Training management UI components

      '
    design_decisions:
    - decision: Use Kohya-ss framework over custom LoRA implementation
      rationale: Proven stability, extensive hyperparameter optimization, and active
        community support reduce development risk
      alternatives_considered:
      - Custom PyTorch LoRA implementation
      - HuggingFace PEFT library
      - Diffusers native LoRA training
    - decision: Implement asynchronous training job queue with Redis
      rationale: Enables concurrent training jobs, proper resource management, and
        graceful failure handling
      alternatives_considered:
      - Synchronous training API
      - Database-based job queue
      - Celery task queue
    - decision: Store training datasets in Supabase Storage with metadata in PostgreSQL
      rationale: Leverages existing infrastructure while providing efficient blob
        storage and relational metadata queries
      alternatives_considered:
      - AWS S3 with RDS
      - Local file system storage
      - Dedicated training data service
    researched_at: '2026-02-07T19:04:48.243981'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:29:18.261699'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a comprehensive LoRA training pipeline that integrates with RunPod''s
      GPU infrastructure through a dedicated training service. Implement dataset preprocessing,
      training job orchestration, and model artifact management as separate microservices.
      Create a dashboard interface for training configuration, progress monitoring,
      and model deployment. Use Redis for job queuing and WebSocket connections for
      real-time training progress updates.

      '
    external_dependencies:
    - name: '@runpod/sdk'
      version: ^1.2.0
      reason: RunPod API integration for GPU cluster management
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for training job queue management
    - name: '@aws-sdk/client-s3'
      version: ^3.450.0
      reason: S3-compatible storage client for model artifacts
    - name: multer
      version: ^1.4.5
      reason: File upload handling for training datasets
    - name: sharp
      version: ^0.33.0
      reason: Image preprocessing and validation for training data
    files_to_modify:
    - path: packages/database/src/schema.sql
      changes: Add training_jobs, model_versions, and training_datasets tables
    - path: packages/ml-service/src/services/inference.ts
      changes: Add LoRA model loading and weight merging capabilities
    - path: apps/dashboard/src/components/ModelLibrary.tsx
      changes: Display trained LoRA models with metadata and download options
    new_files:
    - path: packages/ml-service/src/training/lora-trainer.ts
      purpose: Core LoRA training orchestration and job management
    - path: packages/ml-service/src/training/dataset-processor.ts
      purpose: Dataset validation, preprocessing, and augmentation
    - path: packages/ml-service/src/services/runpod-training.ts
      purpose: RunPod GPU cluster integration and job submission
    - path: packages/ml-service/src/training/training-monitor.ts
      purpose: Real-time progress tracking and WebSocket updates
    - path: apps/dashboard/src/pages/training/LoRATraining.tsx
      purpose: Training configuration UI with hyperparameter controls
    - path: apps/dashboard/src/pages/training/TrainingProgress.tsx
      purpose: Real-time training monitoring dashboard
    - path: packages/ml-service/src/training/cost-monitor.ts
      purpose: GPU cost tracking and automatic job termination
    - path: packages/database/migrations/20241201_training_tables.sql
      purpose: Database schema for training jobs and model metadata
    - path: packages/ml-service/src/training/model-validator.ts
      purpose: Automated quality assessment of trained models
    - path: packages/shared/src/types/training.ts
      purpose: TypeScript interfaces for training job data structures
  acceptance_criteria:
  - criterion: LoRA training pipeline successfully processes datasets and generates
      model weights
    verification: Upload test dataset via dashboard, trigger training job, verify
      .safetensors model file generated in Supabase storage
  - criterion: Real-time training progress monitoring displays loss curves and ETA
      updates
    verification: Start training job and confirm WebSocket updates show loss metrics,
      current epoch, and estimated completion time in dashboard
  - criterion: Training job cost monitoring prevents GPU overruns with automatic termination
    verification: Set $50 cost limit, start expensive training config, verify job
      auto-terminates when limit reached
  - criterion: Generated LoRA models integrate with existing SDXL inference pipeline
    verification: Complete training job, load model weights in inference service,
      generate test images with LoRA applied
  - criterion: Training metadata and model versioning tracked in database
    verification: Query training_jobs and model_versions tables, verify hyperparameters,
      timestamps, and artifact URLs stored correctly
  testing:
    unit_tests:
    - file: packages/ml-service/src/__tests__/lora-trainer.test.ts
      coverage_target: 90%
      scenarios:
      - Training job creation with valid parameters
      - Hyperparameter validation and sanitization
      - Training progress calculation
      - Error handling for invalid datasets
      - Cost calculation and monitoring
    - file: packages/ml-service/src/__tests__/runpod-training.test.ts
      coverage_target: 85%
      scenarios:
      - RunPod API integration
      - Job submission and status polling
      - GPU instance management
      - Network error handling
    integration_tests:
    - file: packages/ml-service/src/__tests__/integration/lora-pipeline.test.ts
      scenarios:
      - End-to-end training pipeline with mock RunPod
      - Dataset upload to model deployment flow
      - Training job failure recovery
      - WebSocket progress updates
    manual_testing:
    - step: Upload 50-image character dataset through dashboard
      expected: Dataset validated, preprocessed, and stored with metadata
    - step: Configure LoRA training with custom hyperparameters
      expected: Training job queued and GPU instance provisioned
    - step: Monitor training progress in real-time
      expected: Loss curves update every 10 steps, ETA displayed
    - step: Generate images using trained LoRA model
      expected: Character consistency visible in generated outputs
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Design database schema for training jobs and model versioning
      done: false
    - task: Implement RunPod API integration for GPU provisioning
      done: false
    - task: Create dataset preprocessing pipeline with validation
      done: false
    - task: Build core LoRA training orchestration service
      done: false
    - task: Implement real-time progress monitoring with WebSockets
      done: false
    - task: Create training dashboard UI components
      done: false
    - task: Add cost monitoring and automatic job termination
      done: false
    - task: Integrate trained models with existing inference pipeline
      done: false
    - task: Implement model quality validation and artifact cleanup
      done: false
    - task: Write comprehensive tests and documentation
      done: false
- key: T50
  title: Prompt Engineering Service
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 5
  area: image-gen
  dependsOn:
  - T41
  - T42
  agent_notes:
    research_findings: '**Context:**

      The Prompt Engineering Service is critical for the comic generation pipeline,
      responsible for transforming novel text into optimized prompts for Stable Diffusion
      image generation. This service needs to analyze narrative context, extract visual
      elements, maintain character consistency across panels, and generate prompts
      that produce coherent comic artwork. Without sophisticated prompt engineering,
      the generated images will lack narrative coherence, visual consistency, and
      comic-appropriate styling.


      **Technical Approach:**

      - Create a dedicated `PromptEngineeringService` class with template-based prompt
      construction

      - Implement prompt templates for different comic elements (characters, scenes,
      panels, styles)

      - Use LLM integration (OpenAI/Anthropic) for intelligent prompt enhancement
      and context analysis

      - Build character consistency tracking with visual descriptor persistence

      - Create prompt validation and optimization pipeline with A/B testing capabilities

      - Implement caching layer for prompt templates and character descriptors using
      Redis

      - Use Zod schemas for prompt structure validation and type safety


      **Dependencies:**

      - External: openai ^4.0.0, @anthropic-ai/sdk, zod ^3.22.0, ioredis ^5.3.0, handlebars
      ^4.7.8

      - Internal: Database service (character persistence), Novel parsing service,
      Image generation queue, Analytics service


      **Risks:**

      - Prompt drift: Character descriptions changing between panels - mitigate with
      strict descriptor versioning

      - Context window limits: Large novels exceeding LLM context - implement chunking
      and summarization

      - Prompt injection: User input contaminating prompts - strict sanitization and
      validation

      - Cost escalation: Excessive LLM API calls - implement aggressive caching and
      batch processing

      - Style inconsistency: Prompts generating different art styles - maintain strict
      style guide templates


      **Complexity Notes:**

      This is significantly more complex than initially apparent. The service must
      handle narrative analysis, visual consistency across potentially hundreds of
      panels, character relationship tracking, and dynamic style adaptation. The context-aware
      prompt generation requires sophisticated NLP and may need custom fine-tuning
      of smaller models for cost efficiency.


      **Key Files:**

      - apps/api/src/services/prompt-engineering/: Main service directory

      - apps/api/src/services/prompt-engineering/templates/: Handlebars prompt templates

      - packages/shared/src/types/prompts.ts: TypeScript interfaces for prompt structures

      - apps/api/src/routes/prompts/: REST endpoints for prompt generation

      - packages/database/src/schema/prompts.sql: Prompt cache and character descriptor
      tables

      '
    design_decisions:
    - decision: Use template-based prompt construction with Handlebars
      rationale: Provides structured, maintainable prompt generation with variable
        interpolation while allowing non-technical team members to modify templates
      alternatives_considered:
      - String concatenation
      - Custom DSL
      - Pure LLM generation
    - decision: Implement character descriptor versioning with database persistence
      rationale: Ensures visual consistency across comic panels by maintaining canonical
        character descriptions that evolve predictably
      alternatives_considered:
      - In-memory storage
      - File-based descriptors
      - LLM-only consistency
    - decision: Multi-tier caching strategy (Redis + in-memory)
      rationale: Reduces LLM API costs and improves response times for frequently
        used prompts while handling high-frequency requests
      alternatives_considered:
      - Database-only caching
      - No caching
      - File system cache
    researched_at: '2026-02-07T19:05:13.710852'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:29:49.390694'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a service-oriented architecture with PromptEngineeringService
      as the core orchestrator. The service will use Handlebars templates for structured
      prompt generation, integrate with OpenAI/Anthropic for context analysis and
      enhancement, and maintain character consistency through a versioned descriptor
      system. A multi-tier caching strategy will optimize performance and costs. The
      service exposes REST endpoints for the image generation pipeline and includes
      comprehensive validation, testing, and monitoring capabilities.

      '
    external_dependencies:
    - name: openai
      version: ^4.20.0
      reason: GPT-4 integration for intelligent prompt enhancement and narrative analysis
    - name: '@anthropic-ai/sdk'
      version: ^0.9.0
      reason: Claude integration as fallback LLM and for specific prompt optimization
        tasks
    - name: handlebars
      version: ^4.7.8
      reason: Template engine for structured, maintainable prompt generation with
        variable interpolation
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for high-performance caching of prompts and character descriptors
    - name: natural
      version: ^6.0.0
      reason: NLP utilities for text analysis, sentiment detection, and entity extraction
        from novel content
    files_to_modify:
    - path: packages/database/src/schema/index.ts
      changes: Add character_descriptors and prompt_cache table imports
    - path: apps/api/src/routes/index.ts
      changes: Register /api/prompts route handler
    - path: apps/api/src/services/index.ts
      changes: Export PromptEngineeringService for dependency injection
    - path: apps/api/src/middleware/validation.ts
      changes: Add prompt request validation middleware
    new_files:
    - path: apps/api/src/services/prompt-engineering/PromptEngineeringService.ts
      purpose: Core service orchestrator for prompt generation and character consistency
    - path: apps/api/src/services/prompt-engineering/PromptTemplateEngine.ts
      purpose: Handlebars template management and rendering
    - path: apps/api/src/services/prompt-engineering/CharacterConsistencyTracker.ts
      purpose: Character descriptor persistence and versioning
    - path: apps/api/src/services/prompt-engineering/ContextAnalyzer.ts
      purpose: Novel text analysis and chunking for LLM processing
    - path: apps/api/src/services/prompt-engineering/PromptOptimizer.ts
      purpose: LLM integration for prompt enhancement and validation
    - path: apps/api/src/services/prompt-engineering/PromptCache.ts
      purpose: Redis-based caching layer for templates and generated prompts
    - path: apps/api/src/services/prompt-engineering/templates/character.hbs
      purpose: Character-focused prompt template
    - path: apps/api/src/services/prompt-engineering/templates/scene.hbs
      purpose: Scene/background prompt template
    - path: apps/api/src/services/prompt-engineering/templates/panel.hbs
      purpose: Comic panel layout prompt template
    - path: apps/api/src/services/prompt-engineering/templates/style.hbs
      purpose: Art style and quality modifier template
    - path: packages/shared/src/types/prompts.ts
      purpose: TypeScript interfaces and Zod schemas for prompt structures
    - path: packages/shared/src/types/characters.ts
      purpose: Character descriptor and consistency tracking types
    - path: apps/api/src/routes/prompts/index.ts
      purpose: REST API routes for prompt generation endpoints
    - path: apps/api/src/routes/prompts/generate.ts
      purpose: POST /api/prompts/generate endpoint handler
    - path: apps/api/src/routes/prompts/characters.ts
      purpose: Character descriptor CRUD endpoints
    - path: packages/database/src/schema/prompts.sql
      purpose: Database tables for prompt cache and character descriptors
    - path: apps/api/src/config/prompt-engineering.ts
      purpose: Service configuration including LLM API settings and cache TTL
    - path: apps/api/src/services/prompt-engineering/utils/sanitization.ts
      purpose: Input sanitization and prompt injection prevention
    - path: apps/api/src/services/prompt-engineering/utils/tokenizer.ts
      purpose: Token counting and context window management
  acceptance_criteria:
  - criterion: Service generates consistent character prompts across multiple panels
      with same character descriptors
    verification: POST /api/prompts/generate with same character_id returns prompts
      with identical character descriptions
  - criterion: Prompt templates correctly interpolate novel context and character
      data into Stable Diffusion format
    verification: Generated prompts follow 'subject, style, background, lighting,
      quality modifiers' structure with < 77 tokens
  - criterion: Character consistency tracking persists visual descriptors with versioning
    verification: Database query shows character_descriptors table maintains version
      history and retrieves latest by character_id
  - criterion: Service handles context window limits through intelligent chunking
      and summarization
    verification: Novel chapters > 8000 tokens are automatically chunked and processed
      without LLM API errors
  - criterion: Caching reduces LLM API calls by >80% for repeated character/scene
      combinations
    verification: Redis cache hit rate > 80% measured via /api/health/prompts endpoint
      after processing same novel twice
  testing:
    unit_tests:
    - file: apps/api/src/services/prompt-engineering/__tests__/PromptEngineeringService.test.ts
      coverage_target: 90%
      scenarios:
      - Template interpolation with valid data
      - Character consistency tracking
      - Prompt validation and sanitization
      - Cache hit/miss scenarios
      - LLM API error handling
    - file: apps/api/src/services/prompt-engineering/__tests__/PromptTemplateEngine.test.ts
      coverage_target: 85%
      scenarios:
      - Handlebars template rendering
      - Template not found errors
      - Invalid template data handling
    - file: packages/shared/src/types/__tests__/prompts.test.ts
      coverage_target: 95%
      scenarios:
      - Zod schema validation success/failure
      - Type guard functions
    integration_tests:
    - file: apps/api/src/services/prompt-engineering/__tests__/integration/prompt-pipeline.test.ts
      scenarios:
      - End-to-end novel text to optimized prompt generation
      - Character persistence across multiple API calls
      - Redis cache integration
      - LLM API integration with rate limiting
    - file: apps/api/src/routes/__tests__/integration/prompts.test.ts
      scenarios:
      - REST API endpoints with authentication
      - Error handling and validation
    manual_testing:
    - step: Generate prompts for 3-chapter novel with 5 characters
      expected: All character descriptions remain consistent across 15+ generated
        prompts
    - step: Process same novel section twice within cache TTL
      expected: Second processing completes 5x faster with cache hits logged
    - step: Submit novel with 50,000+ word count
      expected: Service chunks content and processes without timeout errors
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup database schema and Redis configuration for caching
      done: false
    - task: Implement core PromptEngineeringService with dependency injection
      done: false
    - task: Create Handlebars template system with character/scene/style templates
      done: false
    - task: Build CharacterConsistencyTracker with versioned descriptor persistence
      done: false
    - task: Integrate LLM APIs (OpenAI/Anthropic) with error handling and rate limiting
      done: false
    - task: Implement multi-tier caching strategy with Redis
      done: false
    - task: Create REST API endpoints with validation middleware
      done: false
    - task: Add comprehensive unit and integration test suites
      done: false
    - task: Implement monitoring, logging, and health check endpoints
      done: false
    - task: Create API documentation and service integration guides
      done: false
- key: T51
  title: Character Description Service
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 3
  area: image-gen
  dependsOn:
  - T42
  agent_notes:
    research_findings: '**Context:**

      Character Description Service is a critical component for the image generation
      pipeline that extracts, standardizes, and maintains consistent character descriptions
      from novel text. This service ensures visual consistency across comic panels
      by providing detailed, structured character profiles that can be fed to Stable
      Diffusion models. Without this, generated characters would vary wildly between
      panels, breaking narrative immersion.


      **Technical Approach:**

      Implement as a dedicated service within the content generation pipeline using:

      - LLM-based character extraction from novel chapters using structured prompts

      - Character profile normalization and deduplication using vector embeddings

      - PostgreSQL storage with JSONB fields for flexible character attributes

      - Caching layer (Redis/Upstash) for frequently accessed character data

      - Event-driven updates when new chapters are processed

      - RESTful API with TypeScript schemas for type safety


      **Dependencies:**

      - External: openai@^4.28.0, @anthropic-ai/sdk@^0.17.0, @supabase/supabase-js@^2.39.0,
      zod@^3.22.0, ioredis@^5.3.0

      - Internal: LLM service abstraction, novel processing pipeline, image generation
      service, database schemas


      **Risks:**

      - Character inconsistency: Implement semantic similarity checks and manual override
      capabilities

      - LLM hallucination: Use structured prompts with examples and validation schemas

      - Performance bottleneck: Cache character profiles and batch process multiple
      characters

      - Storage bloat: Implement character profile versioning with cleanup policies


      **Complexity Notes:**

      More complex than initially estimated due to character deduplication challenges
      (same character described differently across chapters) and the need for semantic
      understanding of character relationships and variations.


      **Key Files:**

      - packages/api/src/services/character-description.service.ts: Core service implementation

      - packages/database/migrations/: Character tables and indexes

      - packages/shared/src/types/character.types.ts: TypeScript definitions

      - apps/dashboard/src/components/character-editor/: Admin UI for character management

      '
    design_decisions:
    - decision: Use LLM-based extraction with structured prompts rather than NLP libraries
      rationale: LLMs provide better context understanding for nuanced character descriptions
        and handle varied writing styles more effectively
      alternatives_considered:
      - spaCy/NLTK entity extraction
      - Rule-based pattern matching
      - Hybrid LLM + NLP approach
    - decision: Store character data as JSONB in PostgreSQL with semantic search via
        embeddings
      rationale: Flexible schema for varied character attributes while maintaining
        ACID properties and enabling similarity searches for deduplication
      alternatives_considered:
      - Separate NoSQL database
      - Rigid relational schema
      - Vector database only
    researched_at: '2026-02-07T19:05:33.987829'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:30:11.742067'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a multi-stage pipeline: (1) Extract character mentions from novel
      text using LLM prompts, (2) Generate structured character profiles with physical
      descriptions, personality traits, and relationships, (3) Use vector embeddings
      to detect and merge duplicate characters across chapters, (4) Store normalized
      profiles in PostgreSQL with caching layer, (5) Provide REST API for image generation
      service to retrieve consistent character descriptions.

      '
    external_dependencies:
    - name: openai
      version: ^4.28.0
      reason: GPT-4 for character extraction and description standardization
    - name: '@anthropic-ai/sdk'
      version: ^0.17.0
      reason: Claude as fallback LLM for character analysis
    - name: zod
      version: ^3.22.0
      reason: Runtime validation for character profile schemas
    - name: '@supabase/vecs'
      version: ^0.2.0
      reason: Vector similarity search for character deduplication
    - name: ioredis
      version: ^5.3.0
      reason: Caching frequently accessed character profiles
    files_to_modify:
    - path: packages/database/supabase/migrations/20241201000000_create_characters_tables.sql
      changes: Add characters, character_profiles, character_embeddings tables with
        indexes
    - path: packages/shared/src/types/index.ts
      changes: Export character types for cross-package usage
    - path: apps/dashboard/src/pages/characters/index.tsx
      changes: Add character management page with list, edit, merge functionality
    new_files:
    - path: packages/api/src/services/character-description.service.ts
      purpose: Core service for character extraction, normalization, and deduplication
    - path: packages/api/src/controllers/character.controller.ts
      purpose: REST API endpoints for character CRUD operations
    - path: packages/api/src/lib/character-embeddings.ts
      purpose: Vector embedding utilities for character similarity detection
    - path: packages/shared/src/types/character.types.ts
      purpose: TypeScript schemas and Zod validation for character data structures
    - path: packages/api/src/events/character-events.ts
      purpose: Event handlers for character updates from novel processing pipeline
    - path: packages/api/src/routes/characters.ts
      purpose: Express routes for character API endpoints
    - path: apps/dashboard/src/components/character-editor/CharacterEditor.tsx
      purpose: React component for editing individual character profiles
    - path: apps/dashboard/src/components/character-editor/CharacterMerge.tsx
      purpose: React component for merging duplicate character profiles
    - path: packages/api/src/services/llm-prompts/character-extraction.ts
      purpose: Structured prompts for LLM-based character extraction
  acceptance_criteria:
  - criterion: Service extracts character descriptions from novel text with 90%+ accuracy
    verification: Run test suite with sample novel chapters containing 20+ characters
      and verify extraction completeness
  - criterion: Character deduplication identifies same characters across chapters
      with <5% false positives
    verification: Test with Harry Potter sample data - 'Harry', 'Harry Potter', 'the
      boy wizard' should merge to single profile
  - criterion: API responds to character lookup requests within 200ms for cached data
    verification: Load test GET /api/characters/{id} endpoint with 100 concurrent
      requests
  - criterion: 'Generated character profiles include required fields: physical_description,
      personality_traits, relationships'
    verification: Validate API responses against Zod schema in packages/shared/src/types/character.types.ts
  - criterion: Character profiles maintain consistency across comic panel generation
    verification: Generate 5 panels with same character and verify visual consistency
      using image similarity metrics
  testing:
    unit_tests:
    - file: packages/api/src/services/__tests__/character-description.service.test.ts
      coverage_target: 90%
      scenarios:
      - Character extraction from novel text
      - Profile normalization and validation
      - Vector embedding generation
      - Cache hit/miss scenarios
      - Error handling for malformed input
      - LLM service failures
    - file: packages/api/src/controllers/__tests__/character.controller.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations via REST API
      - Query parameter validation
      - Authentication/authorization
    integration_tests:
    - file: packages/api/src/__tests__/integration/character-pipeline.test.ts
      scenarios:
      - End-to-end novel processing to character extraction
      - Database persistence and retrieval
      - Cache layer integration
      - Event-driven character updates
    manual_testing:
    - step: Upload sample novel chapter via dashboard
      expected: Characters automatically extracted and displayed in character management
        UI
    - step: Merge duplicate character profiles via admin interface
      expected: Character relationships updated, vector embeddings recalculated
    - step: Generate comic panel using character description
      expected: Stable Diffusion receives structured prompt with consistent character
        details
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Create database schema and migrations for character storage
      done: false
    - task: Implement TypeScript types and Zod schemas for character data
      done: false
    - task: Build core CharacterDescriptionService with LLM integration
      done: false
    - task: Implement vector embedding pipeline for character deduplication
      done: false
    - task: Create REST API controllers and routes
      done: false
    - task: Add Redis caching layer for character profiles
      done: false
    - task: Build character management UI components
      done: false
    - task: Integrate with novel processing pipeline via events
      done: false
    - task: Connect character service to image generation pipeline
      done: false
    - task: Write comprehensive test suites and documentation
      done: false
- key: T52
  title: Batch Image Generation
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 5
  area: image-gen
  dependsOn:
  - T50
  agent_notes:
    research_findings: '**Context:**

      Batch image generation is critical for the comic creation pipeline where multiple
      panels need to be generated simultaneously for a chapter or scene. Instead of
      generating images one-by-one (which is slow and expensive), batch processing
      allows for parallel generation, better resource utilization, and cost optimization
      through RunPod''s batch inference capabilities. This is essential for production
      comics which may have 20-50 panels per chapter.


      **Technical Approach:**

      - Queue-based architecture using BullMQ for job management and Redis for state

      - Implement batch grouping strategy (group by style, model, or user preference)

      - Use RunPod''s batch inference API with async/await patterns for parallel processing

      - Implement progress tracking with WebSocket updates to frontend

      - Add retry mechanisms with exponential backoff for failed generations

      - Use streaming responses to show partial results as they complete

      - Implement resource pooling to manage RunPod endpoint scaling


      **Dependencies:**

      - External: bullmq, ioredis, ws (WebSocket), p-limit, async-retry

      - Internal: existing image-gen service, notification system, database models
      for batch jobs


      **Risks:**

      - Rate limiting: RunPod API limits could cause bottlenecks (implement queue
      throttling)

      - Memory issues: Large batch processing could exhaust memory (implement chunking)

      - Partial failures: Some images succeed, others fail (implement granular status
      tracking)

      - Cost explosion: Batch jobs could rack up unexpected costs (implement budget
      controls)

      - Timeout handling: Long-running batches may exceed request timeouts (use background
      processing)


      **Complexity Notes:**

      More complex than initially expected due to state management across multiple
      async operations, error recovery strategies, and the need for real-time progress
      updates. The queue management and WebSocket integration add significant architectural
      complexity.


      **Key Files:**

      - packages/backend/src/services/image-gen/batch-processor.ts: Core batch processing
      logic

      - packages/backend/src/queues/image-batch-queue.ts: BullMQ job definitions

      - packages/backend/src/routes/image-gen/batch.ts: API endpoints

      - packages/frontend/src/components/batch-progress.tsx: Progress tracking UI

      - packages/shared/types/batch-job.ts: Shared type definitions

      '
    design_decisions:
    - decision: Use BullMQ with Redis for job queue management
      rationale: Provides robust job scheduling, retry logic, and progress tracking
        with minimal setup. Integrates well with existing Redis infrastructure.
      alternatives_considered:
      - AWS SQS
      - Custom queue implementation
      - PostgreSQL-based queue
    - decision: Implement chunked batch processing (max 10 images per chunk)
      rationale: Prevents memory exhaustion and allows for better error isolation.
        RunPod performs better with smaller batches.
      alternatives_considered:
      - Single large batch
      - Dynamic chunk sizing
      - User-configurable chunks
    - decision: WebSocket-based progress updates
      rationale: Real-time feedback is essential for user experience with long-running
        batch jobs. WebSockets provide low-latency updates.
      alternatives_considered:
      - Server-sent events
      - Polling-based updates
      - Email notifications only
    researched_at: '2026-02-07T19:05:56.605829'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:30:34.879563'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a multi-layered batch processing system with BullMQ handling
      job orchestration, chunked processing to manage resource usage, and WebSocket
      connections for real-time progress updates. The system will accept batch requests
      via REST API, queue them for processing, and use RunPod''s batch inference capabilities
      to generate images in parallel while maintaining granular status tracking for
      each individual image.

      '
    external_dependencies:
    - name: bullmq
      version: ^5.0.0
      reason: Redis-based queue for batch job management with built-in retry and progress
        tracking
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for queue state management and caching batch results
    - name: ws
      version: ^8.16.0
      reason: WebSocket server for real-time progress updates to frontend clients
    - name: p-limit
      version: ^5.0.0
      reason: Control concurrency of RunPod API calls to prevent rate limiting
    - name: async-retry
      version: ^1.3.3
      reason: Implement exponential backoff retry logic for failed image generations
    files_to_modify:
    - path: packages/backend/src/services/image-gen/image-service.ts
      changes: Add batch processing hooks and integration with batch processor
    - path: packages/backend/src/database/models/image-job.ts
      changes: Add batch_job_id foreign key and batch-related status fields
    - path: packages/backend/src/middleware/websocket.ts
      changes: Add batch progress event handlers and room management
    - path: packages/shared/types/api.ts
      changes: Add batch request/response types and WebSocket event types
    new_files:
    - path: packages/backend/src/services/image-gen/batch-processor.ts
      purpose: Core batch processing logic, chunking, and RunPod integration
    - path: packages/backend/src/queues/image-batch-queue.ts
      purpose: BullMQ job definitions and queue management for batch operations
    - path: packages/backend/src/routes/image-gen/batch.ts
      purpose: REST API endpoints for batch job CRUD operations
    - path: packages/backend/src/services/cost-estimator.ts
      purpose: Calculate costs and enforce budget limits for batch operations
    - path: packages/backend/src/database/models/batch-job.ts
      purpose: Database model for batch job tracking and status
    - path: packages/frontend/src/components/batch-progress.tsx
      purpose: React component for real-time batch progress visualization
    - path: packages/frontend/src/hooks/use-batch-progress.ts
      purpose: Custom hook for WebSocket-based progress tracking
    - path: packages/shared/types/batch-job.ts
      purpose: Shared TypeScript types for batch job data structures
  acceptance_criteria:
  - criterion: System can process batch image generation requests with 10-50 images,
      grouping them efficiently and tracking individual status
    verification: POST /api/image-gen/batch with 20 images returns batch job ID, WebSocket
      shows progress updates, all images complete within expected time
  - criterion: Real-time progress tracking shows completion status for individual
      images in a batch job
    verification: WebSocket connection receives progress events with completed/failed/pending
      counts and individual image status updates
  - criterion: Failed individual images are retried with exponential backoff while
      successful ones remain completed
    verification: Simulate RunPod API failures for subset of batch, verify retry logic
      triggers and only failed images are reprocessed
  - criterion: Batch processing handles memory constraints by chunking large requests
    verification: Submit batch of 100 images, verify system processes in chunks of
      10-20 and memory usage stays below threshold
  - criterion: System provides cost estimation and budget controls for batch operations
    verification: API returns cost estimate before processing, batch jobs respect
      user budget limits and fail gracefully when exceeded
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/batch-processor.test.ts
      coverage_target: 90%
      scenarios:
      - Batch job creation and validation
      - Chunking strategy for different batch sizes
      - Retry logic with exponential backoff
      - Progress calculation and status updates
      - Error handling for RunPod API failures
    - file: packages/backend/src/__tests__/queues/image-batch-queue.test.ts
      coverage_target: 85%
      scenarios:
      - Job enqueueing and dequeuing
      - Priority handling and batch grouping
      - Queue failure recovery
    integration_tests:
    - file: packages/backend/src/__tests__/integration/batch-image-generation.test.ts
      scenarios:
      - End-to-end batch processing with mocked RunPod
      - WebSocket progress updates during batch execution
      - Database state consistency across batch lifecycle
      - Cost calculation and budget enforcement
    manual_testing:
    - step: Create batch request with 25 comic panel prompts via frontend UI
      expected: Batch job starts, progress bar shows 0/25, WebSocket updates incrementally
    - step: Monitor batch processing in different browser tabs
      expected: All connected clients receive same progress updates simultaneously
    - step: Cancel batch job mid-processing
      expected: In-progress images complete, queued images are cancelled, resources
        cleaned up
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Set up BullMQ infrastructure and Redis configuration for batch queues
      done: false
    - task: Implement batch job database models and migrations
      done: false
    - task: Create batch processor service with chunking and RunPod integration
      done: false
    - task: Build REST API endpoints for batch job management
      done: false
    - task: Implement WebSocket handlers for real-time progress updates
      done: false
    - task: Create cost estimation service with budget controls
      done: false
    - task: Build frontend batch progress UI components
      done: false
    - task: Add retry logic with exponential backoff for failed generations
      done: false
    - task: Implement comprehensive error handling and cleanup mechanisms
      done: false
    - task: Write unit and integration tests
      done: false
- key: T53
  title: Location & Environment Service
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 3
  area: image-gen
  dependsOn:
  - T41
  agent_notes:
    research_findings: "**Context:**\nThe Location & Environment Service is critical\
      \ for generating consistent, contextually appropriate comic panel backgrounds.\
      \ When transforming novels to comics, the system needs to intelligently extract\
      \ and manage location information from text, then generate appropriate visual\
      \ environments that maintain consistency across panels. This solves the problem\
      \ of having characters appear in random or inconsistent backgrounds, which breaks\
      \ narrative flow. The service provides structured location data to the image\
      \ generation pipeline, ensuring visual continuity and proper scene setting.\n\
      \n**Technical Approach:**\nBuild a TypeScript service that combines NLP for\
      \ location extraction with a structured environment database. Use OpenAI/Anthropic\
      \ LLMs for semantic location understanding and classification. Implement a caching\
      \ layer with Redis for frequently used environments. Create a RESTful API with\
      \ Fastify that serves location metadata to the image generation pipeline. Use\
      \ Supabase for persistent storage of location templates, environment presets,\
      \ and scene continuity tracking. Integrate with the existing prompt engineering\
      \ system to inject location-specific details into Stable Diffusion prompts.\n\
      \n**Dependencies:**\n- External: @supabase/supabase-js, openai, @anthropic-ai/sdk,\
      \ natural, compromise, ioredis, zod\n- Internal: prompt-engineering service,\
      \ image-gen pipeline, novel-analysis service, database schemas\n\n**Risks:**\n\
      - Location extraction accuracy: Implement confidence scoring and fallback to\
      \ manual tagging\n- Environment consistency drift: Use embedding-based similarity\
      \ matching for related scenes  \n- Performance bottlenecks: Cache location analyses\
      \ and implement batching for bulk processing\n- Prompt injection vulnerabilities:\
      \ Sanitize all location descriptions before passing to LLMs\n\n**Complexity\
      \ Notes:**\nThis is more complex than initially apparent due to the need for\
      \ semantic understanding of implicit locations (e.g., \"the old oak tree where\
      \ they first met\" requires context memory). The service needs sophisticated\
      \ NLP capabilities and state management for scene transitions. However, the\
      \ modular architecture makes it manageable by separating extraction, classification,\
      \ and generation concerns.\n\n**Key Files:**\n- packages/api/src/services/location-environment.service.ts:\
      \ Core service implementation\n- packages/api/src/routes/locations/: RESTful\
      \ API endpoints\n- packages/database/supabase/migrations/: Location and environment\
      \ tables\n- packages/shared/src/types/location.types.ts: TypeScript interfaces\n\
      - packages/image-gen/src/prompt-builders/environment.builder.ts: Location-aware\
      \ prompt generation\n"
    design_decisions:
    - decision: Use hybrid NLP approach with LLM + rule-based extraction
      rationale: LLMs provide semantic understanding while rule-based systems catch
        explicit location mentions reliably
      alternatives_considered:
      - Pure LLM approach
      - Pure NLP library approach
      - Manual annotation only
    - decision: Implement vector embeddings for location similarity matching
      rationale: Enables intelligent environment reuse and consistency checking across
        similar scenes
      alternatives_considered:
      - Keyword matching
      - Manual categorization
      - No similarity matching
    - decision: Cache environment data in Redis with Supabase as source of truth
      rationale: Balances performance with data persistence for frequently accessed
        location metadata
      alternatives_considered:
      - Pure database approach
      - In-memory only
      - File-based caching
    researched_at: '2026-02-07T19:06:19.205713'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:30:58.888203'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a three-layer architecture: extraction layer using LLMs to
      parse locations from novel text, classification layer that categorizes and matches
      environments using vector embeddings, and generation layer that produces Stable
      Diffusion prompts with location-specific details. Use Supabase for persistent
      storage of location templates and scene continuity, Redis for caching frequently
      used environments, and integrate with the existing image generation pipeline
      through standardized prompt injection patterns.

      '
    external_dependencies:
    - name: compromise
      version: ^14.10.0
      reason: Natural language processing for location entity extraction
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for caching location and environment data
    - name: sentence-transformers
      version: ^1.0.0
      reason: Vector embeddings for location similarity matching
    - name: '@xenova/transformers'
      version: ^2.17.1
      reason: Client-side transformer models for location classification
    files_to_modify:
    - path: packages/image-gen/src/prompt-builders/base-prompt.builder.ts
      changes: Add location context injection methods and environment template integration
    - path: packages/api/src/config/database.config.ts
      changes: Add Redis configuration for location caching
    - path: packages/shared/src/types/index.ts
      changes: Export location-related type definitions
    new_files:
    - path: packages/api/src/services/location-environment.service.ts
      purpose: Core service for location extraction, classification, and environment
        management
    - path: packages/api/src/routes/locations/index.ts
      purpose: Route handler registration and middleware setup
    - path: packages/api/src/routes/locations/locations.routes.ts
      purpose: RESTful endpoints for location CRUD operations
    - path: packages/api/src/routes/locations/environments.routes.ts
      purpose: Environment template and preset management endpoints
    - path: packages/shared/src/types/location.types.ts
      purpose: TypeScript interfaces for locations, environments, and scene continuity
    - path: packages/database/supabase/migrations/20241201_create_locations_tables.sql
      purpose: Database schema for locations, environments, and scene tracking
    - path: packages/api/src/utils/location-extractor.util.ts
      purpose: NLP utilities for parsing locations from text using LLMs
    - path: packages/api/src/utils/environment-matcher.util.ts
      purpose: Vector embedding utilities for environment similarity matching
    - path: packages/image-gen/src/prompt-builders/environment.builder.ts
      purpose: Location-aware prompt generation for Stable Diffusion
    - path: packages/api/src/cache/location.cache.ts
      purpose: Redis caching layer for location analyses and environment data
    - path: packages/api/src/validators/location.validators.ts
      purpose: Zod schemas for input validation and sanitization
  acceptance_criteria:
  - criterion: Location extraction from novel text achieves >85% accuracy with confidence
      scoring
    verification: 'Run test suite with annotated novel excerpts: npm run test:location-extraction
      -- --coverage'
  - criterion: Environment consistency maintained across sequential panels with <0.3
      embedding distance
    verification: 'Integration test verifies similar scenes use consistent environments:
      npm run test:integration -- --grep ''environment consistency'''
  - criterion: API responds within 200ms for cached locations and 2s for new extractions
    verification: 'Load test endpoints: k6 run tests/performance/location-api.js'
  - criterion: Generated prompts include location-specific details and pass safety
      validation
    verification: Manual review of 20 generated prompts + automated safety checks
      in test suite
  - criterion: Service integrates seamlessly with image generation pipeline without
      breaking existing flows
    verification: 'End-to-end test generates comic panel with location context: npm
      run test:e2e -- --spec comic-generation'
  testing:
    unit_tests:
    - file: packages/api/src/services/__tests__/location-environment.service.test.ts
      coverage_target: 90%
      scenarios:
      - Location extraction from various text formats
      - Environment classification and matching
      - Confidence scoring accuracy
      - Cache hit/miss scenarios
      - Error handling for malformed input
      - LLM API failure graceful degradation
    - file: packages/api/src/routes/locations/__tests__/locations.routes.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations for locations
      - Bulk processing endpoints
      - Authentication and authorization
      - Input validation and sanitization
    integration_tests:
    - file: packages/api/src/__tests__/integration/location-pipeline.test.ts
      scenarios:
      - Novel text to environment prompt generation flow
      - Redis caching integration
      - Supabase persistence and retrieval
      - Integration with prompt-engineering service
    - file: packages/image-gen/src/__tests__/integration/environment-prompts.test.ts
      scenarios:
      - Location-aware prompt injection
      - Environment consistency across panel sequence
    manual_testing:
    - step: Upload novel chapter with diverse location mentions
      expected: Locations extracted and classified correctly in dashboard
    - step: Generate comic panels for same scene across multiple frames
      expected: Background environments remain visually consistent
    - step: Test location extraction with ambiguous/implicit references
      expected: System provides confidence scores and fallback options
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Setup database schema and migrations for locations/environments
      done: false
    - task: Implement core LocationEnvironmentService with LLM integration
      done: false
    - task: Create Redis caching layer for performance optimization
      done: false
    - task: Build RESTful API endpoints with proper validation
      done: false
    - task: Implement environment prompt builder for image generation
      done: false
    - task: Integrate with existing prompt-engineering service
      done: false
    - task: Add vector embedding system for environment matching
      done: false
    - task: Write comprehensive test suite (unit + integration)
      done: false
    - task: Performance testing and optimization
      done: false
    - task: Documentation and API reference
      done: false
- key: T54
  title: Image Quality Assessment
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 3
  area: image-gen
  dependsOn:
  - T52
  agent_notes:
    research_findings: "**Context:**\nImage Quality Assessment (IQA) is critical for\
      \ Morpheus's content generation pipeline to ensure generated comic panels meet\
      \ publication standards. This addresses two key problems: 1) Stable Diffusion\
      \ models can produce inconsistent quality outputs requiring filtering, and 2)\
      \ Users need confidence that generated images are suitable for their comic projects.\
      \ This prevents poor-quality images from reaching the final comic, reducing\
      \ manual review time and improving user satisfaction.\n\n**Technical Approach:**\n\
      Implement a multi-tier assessment system combining:\n- Technical metrics (resolution,\
      \ sharpness, noise levels) using image processing libraries\n- AI-powered quality\
      \ scoring via OpenAI Vision API for semantic quality assessment\n- Domain-specific\
      \ comic art validation (character consistency, panel composition)\n- Configurable\
      \ quality thresholds per generation request\n- Real-time scoring during image\
      \ generation with automatic retry logic\n- Quality metadata storage for analytics\
      \ and model improvement\n\n**Dependencies:**\n- External: sharp (image processing),\
      \ @tensorflow/tfjs-node (optional ML models), openai (GPT-4 Vision), canvas\
      \ (image analysis)\n- Internal: image-generation service, database schemas for\
      \ quality scores, job queue system, notification service\n\n**Risks:**\n- Performance\
      \ bottleneck: IQA could significantly slow generation pipeline - mitigate with\
      \ async processing and caching\n- Subjective quality metrics: Comic art quality\
      \ is subjective - mitigate with configurable scoring weights and user feedback\
      \ loops  \n- API costs: Vision API calls for every image - mitigate with local\
      \ models for basic checks, Vision API for final validation\n- False positives/negatives:\
      \ Over-filtering good images or passing poor ones - mitigate with A/B testing\
      \ and manual override options\n\n**Complexity Notes:**\nMore complex than initially\
      \ estimated due to the subjective nature of comic art quality. Requires balancing\
      \ multiple quality dimensions (technical, artistic, narrative consistency) and\
      \ building feedback mechanisms for continuous improvement. The integration with\
      \ existing generation pipeline adds complexity around error handling and retry\
      \ logic.\n\n**Key Files:**\n- apps/backend/src/services/image-quality-service.ts:\
      \ Core IQA implementation\n- apps/backend/src/lib/image-processing.ts: Technical\
      \ quality metrics\n- packages/shared/src/types/image-quality.ts: Quality score\
      \ interfaces\n- apps/backend/src/routes/images/generate.ts: Integration with\
      \ generation endpoint\n- packages/database/migrations/: Quality scores table\
      \ schema\n"
    design_decisions:
    - decision: Hybrid local + cloud quality assessment
      rationale: Balance between speed/cost (local technical metrics) and accuracy
        (cloud AI for semantic quality)
      alternatives_considered:
      - Purely local assessment
      - Purely cloud-based assessment
      - Third-party IQA services
    - decision: Configurable quality thresholds per user/project
      rationale: Different comic styles and user requirements need different quality
        standards
      alternatives_considered:
      - Global quality thresholds
      - Model-based adaptive thresholds
    - decision: Async quality assessment with webhook notifications
      rationale: Prevents blocking the generation pipeline while allowing real-time
        user updates
      alternatives_considered:
      - Synchronous assessment
      - Polling-based status checks
    researched_at: '2026-02-07T19:06:43.964441'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:31:24.818640'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a quality assessment service that processes generated images
      through multiple evaluation stages: technical metrics (sharpness, resolution,
      artifacts) using Sharp.js, followed by AI-powered semantic assessment via OpenAI
      Vision API. Quality scores are stored with configurable thresholds triggering
      automatic regeneration or user notification. The service integrates asynchronously
      with the existing RunPod Stable Diffusion pipeline, using Redis for job queuing
      and WebSocket connections for real-time quality updates to the frontend dashboard.

      '
    external_dependencies:
    - name: sharp
      version: ^0.32.0
      reason: High-performance image processing for technical quality metrics (sharpness,
        noise, resolution analysis)
    - name: canvas
      version: ^2.11.0
      reason: Advanced image analysis capabilities for composition and visual balance
        assessment
    - name: openai
      version: ^4.0.0
      reason: GPT-4 Vision API integration for semantic quality assessment of comic
        art style and composition
    - name: image-ssim
      version: ^0.2.0
      reason: Structural similarity metrics for comparing generated images against
        reference quality standards
    files_to_modify:
    - path: apps/backend/src/routes/images/generate.ts
      changes: Add async quality assessment job creation, WebSocket integration for
        score delivery
    - path: apps/backend/src/services/image-generation-service.ts
      changes: Integrate quality assessment callback, implement retry logic for failed
        quality checks
    - path: apps/backend/src/lib/redis-client.ts
      changes: Add quality assessment job queue configuration and processors
    - path: apps/frontend/src/components/ImagePanel.tsx
      changes: Display quality scores, manual override controls, loading states for
        assessment
    - path: packages/database/src/schema.prisma
      changes: Add ImageQualityScore model with technical/semantic/comic score fields
    new_files:
    - path: apps/backend/src/services/image-quality-service.ts
      purpose: Core quality assessment orchestration, score aggregation, threshold
        evaluation
    - path: apps/backend/src/lib/image-processing.ts
      purpose: Technical quality metrics using Sharp.js - sharpness, noise, resolution
        analysis
    - path: apps/backend/src/lib/openai-vision-client.ts
      purpose: OpenAI Vision API wrapper for semantic and comic-specific quality assessment
    - path: packages/shared/src/types/image-quality.ts
      purpose: TypeScript interfaces for quality scores, assessment requests, and
        responses
    - path: apps/backend/src/routes/images/quality.ts
      purpose: Quality override endpoints, manual assessment triggers, quality analytics
    - path: apps/backend/src/jobs/quality-assessment.ts
      purpose: Background job processor for async quality assessment with Redis Bull
    - path: packages/database/migrations/20241201_add_image_quality_scores.sql
      purpose: Database schema for quality scores, user overrides, and assessment
        metadata
    - path: apps/frontend/src/components/QualityDashboard.tsx
      purpose: Admin dashboard for quality trends, threshold configuration, override
        management
  acceptance_criteria:
  - criterion: Generated images receive quality scores (0-100) across technical, semantic,
      and comic-specific dimensions within 5 seconds
    verification: POST /api/images/generate returns quality_score object with technical_score,
      semantic_score, comic_score, and overall_score fields
  - criterion: Images below configurable quality threshold (default 70) trigger automatic
      regeneration up to 3 attempts
    verification: Monitor generation logs showing retry attempts and verify final
      image meets threshold or returns error after max retries
  - criterion: Quality assessment pipeline processes images asynchronously without
      blocking generation response
    verification: Generation endpoint returns immediately with job_id, quality scores
      delivered via WebSocket within 5s
  - criterion: System stores quality metadata for analytics and provides quality trend
      dashboard
    verification: Database contains image_quality_scores table with all metrics, admin
      dashboard shows quality trends over time
  - criterion: Manual quality override allows users to accept/reject images regardless
      of automatic scoring
    verification: PUT /api/images/{id}/quality-override endpoint allows quality_approved
      boolean override with user feedback
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/image-quality-service.test.ts
      coverage_target: 90%
      scenarios:
      - Technical quality metrics calculation
      - OpenAI Vision API integration with mocked responses
      - Quality threshold evaluation logic
      - Score aggregation and weighting
      - Error handling for API failures
    - file: apps/backend/src/__tests__/lib/image-processing.test.ts
      coverage_target: 85%
      scenarios:
      - Sharpness detection algorithms
      - Noise level analysis
      - Resolution validation
      - Artifact detection
    integration_tests:
    - file: apps/backend/src/__tests__/integration/image-quality-pipeline.test.ts
      scenarios:
      - End-to-end quality assessment with real image files
      - Integration with Redis job queue
      - WebSocket quality score delivery
      - Database quality score persistence
      - Automatic regeneration trigger flow
    manual_testing:
    - step: Generate comic panel with various Stable Diffusion prompts
      expected: Quality scores returned within 5s, poor quality images trigger regeneration
    - step: Test manual override functionality on borderline quality images
      expected: Override accepted, user feedback stored, analytics updated
    - step: Load test with 10 concurrent image generations
      expected: No performance degradation, all quality assessments complete
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Setup database migration for image quality scores table
      done: false
    - task: Implement technical quality metrics with Sharp.js (sharpness, noise, resolution)
      done: false
    - task: Create OpenAI Vision API client for semantic quality assessment
      done: false
    - task: Build core ImageQualityService with score aggregation and threshold logic
      done: false
    - task: Integrate async quality assessment job processing with Redis Bull
      done: false
    - task: Modify image generation pipeline to trigger quality assessment
      done: false
    - task: Add WebSocket delivery for real-time quality score updates
      done: false
    - task: Implement manual quality override API endpoints
      done: false
    - task: Create frontend components for quality display and override controls
      done: false
    - task: Build admin quality dashboard with analytics and threshold configuration
      done: false
    - task: Add comprehensive error handling and retry logic for API failures
      done: false
    - task: Performance optimization and caching for repeated assessments
      done: false
- key: T55
  title: Mood & Atmosphere Service
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p1
  effort: 3
  area: image-gen
  dependsOn:
  - T41
  agent_notes:
    research_findings: '**Context:**

      The Mood & Atmosphere Service is critical for generating cohesive visual narratives
      in comics. It analyzes novel text to extract emotional tone, setting atmosphere,
      and visual mood descriptors that guide Stable Diffusion image generation. This
      ensures panels maintain consistent visual themes (dark/gritty, whimsical, romantic,
      etc.) and that character emotions are visually represented accurately. Without
      this service, generated images would lack narrative coherence and emotional
      depth.


      **Technical Approach:**

      - Implement a microservice architecture with mood analysis pipeline using LLM
      prompt engineering

      - Create mood taxonomy system with standardized descriptors for Stable Diffusion

      - Build context-aware analysis that considers chapter/scene continuity

      - Use caching layer for mood profiles to optimize repeated generations

      - Integrate with existing image-gen pipeline as preprocessing step

      - Implement mood interpolation for smooth transitions between scenes


      **Dependencies:**

      - External: openai@^4.20.0, anthropic@^0.7.0, redis@^4.6.0, zod@^3.22.0

      - Internal: LLM service wrapper, image generation pipeline, content analysis
      service, database models for mood persistence


      **Risks:**

      - Subjective interpretation: LLMs may interpret mood inconsistently - mitigate
      with structured prompts and validation

      - Performance bottleneck: Real-time mood analysis could slow generation - mitigate
      with background processing and caching

      - Mood drift: Long novels may have inconsistent atmosphere detection - mitigate
      with sliding window context analysis

      - Cost escalation: Additional LLM calls increase operational costs - mitigate
      with intelligent caching and batching


      **Complexity Notes:**

      More complex than initially estimated due to need for contextual continuity
      tracking and mood taxonomy standardization. Requires sophisticated prompt engineering
      and validation systems.


      **Key Files:**

      - packages/services/src/mood/mood-analyzer.ts: Core mood analysis logic

      - packages/services/src/mood/mood-taxonomy.ts: Standardized mood descriptors

      - packages/db/src/schema/mood-profiles.sql: Mood persistence layer

      - packages/image-gen/src/pipeline/mood-integration.ts: Integration with SD pipeline

      - packages/api/src/routes/mood.ts: REST endpoints for mood operations

      '
    design_decisions:
    - decision: Use structured LLM prompts with Zod validation for mood extraction
      rationale: Ensures consistent, type-safe mood data that integrates reliably
        with Stable Diffusion prompts
      alternatives_considered:
      - Pre-trained sentiment models
      - Rule-based mood detection
      - Hybrid ML approach
    - decision: Implement Redis-based mood profile caching with TTL
      rationale: Reduces LLM API calls for repeated generations while allowing mood
        evolution updates
      alternatives_considered:
      - Database-only caching
      - In-memory caching
      - No caching
    - decision: Create hierarchical mood taxonomy (primary/secondary/tertiary)
      rationale: Allows granular control over visual generation while maintaining
        simplicity for basic use cases
      alternatives_considered:
      - Flat mood tags
      - Emotion wheel mapping
      - Continuous mood vectors
    researched_at: '2026-02-07T19:07:05.736992'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:31:48.364491'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a mood analysis pipeline that processes novel chapters through
      LLM APIs to extract structured mood data using Zod schemas. Implement a hierarchical
      mood taxonomy that maps to Stable Diffusion prompt modifiers. Cache mood profiles
      in Redis with intelligent invalidation. Integrate as preprocessing step in existing
      image generation pipeline with fallback mechanisms for service failures.

      '
    external_dependencies:
    - name: openai
      version: ^4.20.0
      reason: Primary LLM for mood analysis with structured output support
    - name: anthropic
      version: ^0.7.0
      reason: Fallback LLM for mood analysis redundancy
    - name: redis
      version: ^4.6.0
      reason: High-performance caching for mood profiles and analysis results
    - name: zod
      version: ^3.22.0
      reason: Runtime validation of LLM-generated mood data structures
    - name: natural
      version: ^6.7.0
      reason: Text preprocessing and sentiment analysis utilities
    files_to_modify:
    - path: packages/image-gen/src/pipeline/image-generator.ts
      changes: Add mood preprocessing step, integrate mood descriptors into SD prompts
    - path: packages/services/src/llm/llm-service.ts
      changes: Add mood analysis prompt templates and response parsing
    - path: packages/db/src/schema/index.ts
      changes: Export mood profile schema definitions
    - path: packages/api/src/routes/index.ts
      changes: Register mood service routes
    new_files:
    - path: packages/services/src/mood/mood-analyzer.ts
      purpose: Core mood analysis service with LLM integration
    - path: packages/services/src/mood/mood-taxonomy.ts
      purpose: Standardized mood descriptor system and SD prompt mapping
    - path: packages/services/src/mood/mood-cache.ts
      purpose: Redis-based caching layer for mood profiles
    - path: packages/services/src/mood/mood-interpolation.ts
      purpose: Smooth mood transitions between scenes/chapters
    - path: packages/services/src/mood/types.ts
      purpose: TypeScript interfaces and Zod schemas for mood data
    - path: packages/db/src/schema/mood-profiles.sql
      purpose: Database schema for mood profile persistence
    - path: packages/image-gen/src/pipeline/mood-integration.ts
      purpose: Mood data integration with Stable Diffusion pipeline
    - path: packages/api/src/routes/mood.ts
      purpose: REST endpoints for mood analysis operations
    - path: packages/api/src/middleware/mood-validation.ts
      purpose: Request validation for mood service endpoints
  acceptance_criteria:
  - criterion: Mood analysis extracts structured emotional tone, atmosphere, and visual
      descriptors from novel text with 90% consistency across identical inputs
    verification: Run integration test with sample novel chapters, verify mood taxonomy
      fields populated correctly
  - criterion: Generated mood profiles integrate seamlessly with Stable Diffusion
      pipeline, producing visually consistent images that match extracted mood descriptors
    verification: Generate 10 images from same mood profile, manually verify visual
      consistency and mood alignment
  - criterion: System maintains mood continuity across chapter boundaries with smooth
      transitions (mood drift < 20% between adjacent scenes)
    verification: Process multi-chapter novel, measure mood similarity scores between
      consecutive chapters
  - criterion: Mood analysis performance stays under 2 seconds per chapter with Redis
      caching achieving 80% hit rate after initial analysis
    verification: Load test with performance monitoring, check Redis metrics dashboard
  - criterion: Service handles failures gracefully with fallback to previous mood
      profiles and comprehensive error logging
    verification: Simulate LLM API failures, verify image generation continues with
      cached mood data
  testing:
    unit_tests:
    - file: packages/services/src/__tests__/mood/mood-analyzer.test.ts
      coverage_target: 90%
      scenarios:
      - Text analysis with various mood types
      - Mood taxonomy mapping validation
      - Caching layer interactions
      - Error handling for malformed input
      - Mood interpolation calculations
    - file: packages/services/src/__tests__/mood/mood-taxonomy.test.ts
      coverage_target: 95%
      scenarios:
      - Taxonomy validation and structure
      - Stable Diffusion prompt generation
      - Mood descriptor normalization
    integration_tests:
    - file: packages/services/src/__tests__/integration/mood-pipeline.test.ts
      scenarios:
      - End-to-end mood analysis pipeline
      - Redis caching and retrieval
      - Database persistence operations
      - LLM service integration
      - Image generation pipeline integration
    - file: packages/api/src/__tests__/integration/mood-endpoints.test.ts
      scenarios:
      - REST API mood analysis requests
      - Batch mood processing
      - Authentication and rate limiting
    manual_testing:
    - step: Upload sample novel chapter via API
      expected: Returns structured mood analysis with taxonomy fields populated
    - step: Generate images using extracted mood profile
      expected: Images visually reflect mood descriptors (dark/bright, emotional tone)
    - step: Process long novel with multiple chapters
      expected: Mood transitions are smooth, no abrupt visual style changes
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Design mood taxonomy structure and validate with sample content
      done: false
    - task: Implement core mood analyzer with LLM prompt engineering
      done: false
    - task: Build Redis caching layer with intelligent invalidation
      done: false
    - task: Create database schema and persistence layer
      done: false
    - task: Integrate mood preprocessing into image generation pipeline
      done: false
    - task: Implement mood interpolation for scene transitions
      done: false
    - task: Build REST API endpoints with validation middleware
      done: false
    - task: Write comprehensive test suites (unit + integration)
      done: false
    - task: Performance testing and optimization
      done: false
    - task: Documentation and API specs
      done: false
- key: T56
  title: Prompt Caching & Optimization
  type: Task
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p1
  effort: 2
  area: image-gen
  dependsOn:
  - T50
  agent_notes:
    research_findings: '**Context:**

      In Morpheus''s image generation pipeline, users frequently request similar visual
      content (character descriptions, scene compositions, art styles). Without caching,
      every generation hits RunPod''s Stable Diffusion API, leading to unnecessary
      costs, slower response times, and potential rate limiting. Prompt caching optimizes
      this by storing generated images and reusing semantically similar prompts, while
      prompt optimization ensures we get the best possible outputs with minimal API
      calls.


      **Technical Approach:**

      Implement a multi-layered caching strategy:

      1. **Exact Match Cache**: Hash-based storage for identical prompts using Redis

      2. **Semantic Similarity Cache**: Vector embeddings (OpenAI text-embedding-ada-002)
      to find similar prompts within threshold

      3. **Prompt Optimization**: Pre-processing pipeline to standardize, enhance,
      and validate prompts before generation

      4. **Cache Invalidation**: TTL-based expiry with manual purging capabilities

      5. **Supabase Integration**: Store cache metadata, usage stats, and image references
      in PostgreSQL


      **Dependencies:**

      - External: redis, ioredis, @supabase/supabase-js, openai, crypto, sharp

      - Internal: image-gen service, database schemas, storage service, analytics
      pipeline


      **Risks:**

      - **Storage Costs**: Large image cache could inflate storage costs - mitigate
      with aggressive TTL and size limits

      - **Cache Poisoning**: Bad generations cached permanently - implement quality
      scoring and manual review flags

      - **Memory Leaks**: Vector embeddings consuming excessive RAM - use lazy loading
      and periodic cleanup

      - **Consistency Issues**: Cache/DB sync problems - implement eventual consistency
      with reconciliation jobs


      **Complexity Notes:**

      This is more complex than initially estimated. Semantic similarity requires
      ML pipeline integration, vector storage, and similarity threshold tuning. The
      caching layer adds significant architectural complexity requiring careful error
      handling, fallback mechanisms, and monitoring.


      **Key Files:**

      - apps/backend/src/services/image-gen/prompt-cache.service.ts: Core caching
      logic

      - apps/backend/src/services/image-gen/prompt-optimizer.service.ts: Prompt preprocessing

      - apps/backend/src/lib/redis.ts: Redis connection and utilities

      - packages/database/src/schema/cache.sql: Cache metadata tables

      - apps/backend/src/routes/image-gen.ts: Integration with generation endpoints

      '
    design_decisions:
    - decision: Use Redis for hot cache + Supabase for metadata persistence
      rationale: Redis provides sub-millisecond lookup for frequent requests, while
        Supabase handles durable storage, analytics, and complex queries
      alternatives_considered:
      - Pure PostgreSQL with materialized views
      - In-memory LRU cache only
      - External cache service (Elasticache)
    - decision: Implement semantic similarity using OpenAI embeddings with cosine
        similarity
      rationale: Leverages existing OpenAI integration, provides good semantic understanding,
        and cosine similarity is computationally efficient
      alternatives_considered:
      - Local sentence transformers
      - Fuzzy string matching
      - Custom trained embeddings
    - decision: Cache final generated images, not intermediate states
      rationale: Reduces storage complexity while maximizing reuse value for end users
      alternatives_considered:
      - Cache all pipeline steps
      - Cache only prompt embeddings
      - Cache generation parameters only
    researched_at: '2026-02-07T19:07:30.676541'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:32:12.217385'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a PromptCacheService that intercepts image generation requests,
      computes embeddings for semantic matching, and maintains a Redis-backed cache
      with Supabase persistence. The service will implement a three-tier lookup: exact
      hash match, semantic similarity within 0.85 threshold, then fallback to actual
      generation. Include a PromptOptimizer that standardizes language, adds quality
      enhancers, and validates against known problematic patterns before caching.

      '
    external_dependencies:
    - name: ioredis
      version: ^5.3.2
      reason: High-performance Redis client for Node.js with TypeScript support
    - name: openai
      version: ^4.24.1
      reason: Generate text embeddings for semantic prompt similarity matching
    - name: ml-distance
      version: ^4.0.1
      reason: Fast cosine similarity calculations for vector comparisons
    - name: hash-sum
      version: ^2.0.0
      reason: Consistent hashing for exact prompt match detection
    - name: sharp
      version: ^0.33.1
      reason: Image processing for cache thumbnail generation and optimization
    files_to_modify:
    - path: apps/backend/src/routes/image-gen.ts
      changes: Integrate PromptCacheService before RunPod API calls, add cache status
        to response metadata
    - path: apps/backend/src/lib/redis.ts
      changes: Add cache-specific Redis methods for hash storage, TTL management,
        and vector operations
    - path: packages/database/src/migrations/001_initial.sql
      changes: Add cache metadata tables via new migration file
    - path: apps/backend/src/config/index.ts
      changes: Add cache configuration (TTL, similarity threshold, size limits)
    new_files:
    - path: apps/backend/src/services/image-gen/prompt-cache.service.ts
      purpose: Core caching logic with Redis/Supabase integration and semantic similarity
        matching
    - path: apps/backend/src/services/image-gen/prompt-optimizer.service.ts
      purpose: Prompt preprocessing, standardization, and quality enhancement
    - path: packages/database/src/schema/cache.sql
      purpose: Cache metadata tables (cache_entries, cache_stats, cache_embeddings)
    - path: apps/backend/src/lib/embeddings.ts
      purpose: OpenAI embedding generation and cosine similarity calculations
    - path: apps/backend/src/types/cache.types.ts
      purpose: TypeScript interfaces for cache entries, metadata, and configuration
    - path: packages/database/src/migrations/012_cache_tables.sql
      purpose: Database migration for cache-related tables and indexes
  acceptance_criteria:
  - criterion: Cache hit rate of >70% for similar prompts (cosine similarity ≥0.85)
    verification: Check analytics dashboard metrics after 100 test generations with
      30% duplicate/similar prompts
  - criterion: Image generation response time reduced by >60% for cached results
    verification: Performance tests showing <500ms response for cache hits vs >2s
      for API calls
  - criterion: Prompt optimization improves generation quality scores by >15%
    verification: A/B test comparing raw vs optimized prompts using quality scoring
      endpoint
  - criterion: Cache storage remains under 10GB with TTL-based cleanup
    verification: Monitor Redis memory usage and Supabase storage metrics over 7 days
  - criterion: System gracefully degrades when cache unavailable
    verification: Manual Redis shutdown test - API continues working with direct generation
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/prompt-cache.service.test.ts
      coverage_target: 90%
      scenarios:
      - Exact hash match retrieval
      - Semantic similarity matching within threshold
      - Cache miss fallback to generation
      - TTL expiration handling
      - Redis connection failure
      - Invalid embedding responses
    - file: apps/backend/src/__tests__/services/prompt-optimizer.service.test.ts
      coverage_target: 85%
      scenarios:
      - Prompt standardization and enhancement
      - Problematic content filtering
      - Quality enhancer injection
      - Character limit handling
    integration_tests:
    - file: apps/backend/src/__tests__/integration/image-gen-cache.test.ts
      scenarios:
      - Full generation pipeline with caching
      - Cache invalidation workflows
      - Supabase metadata persistence
      - OpenAI embedding integration
    manual_testing:
    - step: Generate same image prompt 3 times consecutively
      expected: First call slow (>2s), subsequent calls fast (<500ms)
    - step: Generate semantically similar prompts ('red car' vs 'crimson automobile')
      expected: Second generation returns cached result with similarity score logged
    - step: Fill cache to size limit, generate new content
      expected: Oldest entries evicted, new content cached successfully
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup Redis configuration and connection utilities
      done: false
    - task: Create database schema and migration for cache tables
      done: false
    - task: Implement OpenAI embeddings service with similarity calculations
      done: false
    - task: Build PromptOptimizer service with standardization rules
      done: false
    - task: Develop PromptCacheService with three-tier lookup logic
      done: false
    - task: Integrate caching into existing image-gen route handlers
      done: false
    - task: Add cache analytics and monitoring endpoints
      done: false
    - task: Implement cache invalidation and cleanup jobs
      done: false
    - task: Create comprehensive test suite with mocked dependencies
      done: false
    - task: Performance testing and threshold tuning
      done: false
- key: T57
  title: Generation Progress Tracking
  type: Task
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 2
  area: image-gen
  dependsOn:
  - T52
  agent_notes:
    research_findings: '**Context:**

      Generation Progress Tracking is critical for the comic generation pipeline where
      users need real-time feedback on long-running AI operations (text-to-image generation
      via RunPod Stable Diffusion, panel layout processing, story analysis). Without
      proper progress tracking, users face a black box experience during 30-60 second
      generation cycles, leading to abandonment and poor UX. This directly impacts
      user retention and platform credibility.


      **Technical Approach:**

      Implement a multi-layered progress system using WebSockets for real-time updates,
      Redis for progress state persistence, and a structured event system. Use Server-Sent
      Events (SSE) as WebSocket fallback. Create a progress management service that
      tracks generation jobs through distinct phases (queuing, processing, post-processing,
      completion) with percentage completion and estimated time remaining. Integrate
      with RunPod webhooks for external ML pipeline updates.


      **Dependencies:**

      - External: socket.io@4.7.5, ioredis@5.3.2, @fastify/websocket@8.3.0, uuid@9.0.1

      - Internal: Supabase job tracking tables, authentication middleware, existing
      image generation service, notification system


      **Risks:**

      - WebSocket connection drops: Implement automatic reconnection with exponential
      backoff and SSE fallback

      - Progress state inconsistency: Use Redis TTL with database backup for persistence
      across service restarts

      - RunPod webhook reliability: Implement polling fallback mechanism for critical
      progress updates

      - Memory leaks from uncleaned progress listeners: Proper cleanup on job completion/cancellation


      **Complexity Notes:**

      More complex than initially estimated due to need for fault-tolerant real-time
      communication, integration with external RunPod webhooks, and maintaining progress
      state across potential service restarts. The multi-phase nature of comic generation
      (story analysis → panel planning → image generation → post-processing) requires
      sophisticated progress calculation algorithms.


      **Key Files:**

      - packages/backend/src/services/progress-tracker.service.ts: Core progress management

      - packages/backend/src/routes/generation/progress.routes.ts: WebSocket endpoints

      - packages/backend/src/types/generation.types.ts: Progress event type definitions

      - packages/frontend/src/hooks/useGenerationProgress.ts: React hook for progress
      subscription

      - packages/frontend/src/components/ProgressTracker.tsx: UI component

      - packages/backend/src/integrations/runpod-webhook.handler.ts: External progress
      updates

      '
    design_decisions:
    - decision: Use Redis for progress state with database backup
      rationale: Provides fast real-time updates while ensuring persistence across
        service restarts
      alternatives_considered:
      - Memory-only storage
      - Database-only storage
      - Event sourcing pattern
    - decision: WebSockets with SSE fallback for client communication
      rationale: WebSockets offer lowest latency for real-time updates, SSE ensures
        compatibility with restrictive networks
      alternatives_considered:
      - Polling-based updates
      - WebSockets only
      - SSE only
    - decision: Structured phase-based progress tracking
      rationale: Comic generation has distinct phases with different time characteristics,
        allowing better user communication
      alternatives_considered:
      - Simple percentage completion
      - Time-based estimates only
      - Binary status updates
    researched_at: '2026-02-07T19:07:54.211395'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:32:38.009045'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a centralized ProgressTracker service that manages job states
      in Redis with unique job IDs. Implement WebSocket endpoints for real-time client
      subscriptions and integrate RunPod webhook handlers for external progress updates.
      Use a structured event system with phases (QUEUED, ANALYZING, GENERATING, POST_PROCESSING,
      COMPLETE) and percentage completion within each phase. Build React hooks and
      components for seamless frontend integration with automatic reconnection handling.

      '
    external_dependencies:
    - name: socket.io
      version: ^4.7.5
      reason: WebSocket server implementation with built-in fallbacks and room management
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for fast progress state storage and pub/sub capabilities
    - name: '@fastify/websocket'
      version: ^8.3.0
      reason: Native Fastify WebSocket support for lightweight real-time communication
    - name: uuid
      version: ^9.0.1
      reason: Generate unique job IDs for progress tracking correlation
    files_to_modify:
    - path: packages/backend/src/services/image-generation.service.ts
      changes: Integrate ProgressTracker calls at generation milestones
    - path: packages/backend/src/routes/generation/index.ts
      changes: Add WebSocket and SSE progress endpoints
    - path: packages/backend/src/config/redis.ts
      changes: Add progress-specific Redis configuration and TTL settings
    - path: packages/frontend/src/pages/GenerationPage.tsx
      changes: Integrate ProgressTracker component and useGenerationProgress hook
    new_files:
    - path: packages/backend/src/services/progress-tracker.service.ts
      purpose: Core progress management, Redis operations, client subscription handling
    - path: packages/backend/src/routes/generation/progress.routes.ts
      purpose: WebSocket and SSE endpoint definitions for real-time progress updates
    - path: packages/backend/src/types/generation.types.ts
      purpose: TypeScript definitions for progress events, job states, and phase enums
    - path: packages/backend/src/integrations/runpod-webhook.handler.ts
      purpose: Handle RunPod webhook callbacks and translate to internal progress
        events
    - path: packages/backend/src/middleware/progress-auth.middleware.ts
      purpose: Authentication middleware for WebSocket connections using JWT tokens
    - path: packages/frontend/src/hooks/useGenerationProgress.ts
      purpose: React hook for WebSocket subscription, reconnection logic, and state
        management
    - path: packages/frontend/src/components/ProgressTracker.tsx
      purpose: UI component with progress bar, phase indicators, and time estimates
    - path: packages/frontend/src/utils/websocket-client.ts
      purpose: WebSocket client with automatic reconnection and SSE fallback logic
    - path: packages/backend/src/utils/progress-calculator.ts
      purpose: Algorithms for calculating overall progress across multiple generation
        phases
  acceptance_criteria:
  - criterion: Real-time progress updates are delivered to clients within 500ms of
      state changes during comic generation
    verification: WebSocket message timestamps show <500ms latency between progress
      events in browser dev tools
  - criterion: Progress tracking survives service restarts and maintains state consistency
    verification: Restart backend service during generation, verify client reconnects
      and receives correct progress state from Redis
  - criterion: Fallback to SSE works when WebSocket connections fail
    verification: Block WebSocket connections in browser, verify SSE endpoint delivers
      same progress data
  - criterion: All generation phases (QUEUED, ANALYZING, GENERATING, POST_PROCESSING,
      COMPLETE) report progress percentages
    verification: Generate comic and verify progress events contain phase and percentage
      fields for each stage
  - criterion: RunPod webhook integration updates progress for external ML operations
    verification: Mock RunPod webhook calls update Redis state and propagate to connected
      clients
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/progress-tracker.service.test.ts
      coverage_target: 90%
      scenarios:
      - Job creation and state initialization
      - Progress updates and phase transitions
      - Client subscription management
      - Redis state persistence and retrieval
      - Job completion and cleanup
    - file: packages/backend/src/__tests__/integrations/runpod-webhook.handler.test.ts
      coverage_target: 85%
      scenarios:
      - Valid webhook payload processing
      - Invalid webhook handling
      - Progress calculation from RunPod status
    - file: packages/frontend/src/__tests__/hooks/useGenerationProgress.test.ts
      coverage_target: 85%
      scenarios:
      - WebSocket connection and subscription
      - Automatic reconnection on disconnect
      - SSE fallback activation
      - Progress state updates
    integration_tests:
    - file: packages/backend/src/__tests__/integration/progress-tracking.test.ts
      scenarios:
      - End-to-end WebSocket progress flow
      - Redis persistence across service restart
      - RunPod webhook to client propagation
      - Multiple client subscription handling
    e2e_tests:
    - file: packages/e2e/src/generation-progress.spec.ts
      scenarios:
      - Full comic generation with progress tracking
      - Network interruption recovery
      - Multiple concurrent generations
    manual_testing:
    - step: Start comic generation and monitor progress in UI
      expected: Progress bar updates smoothly through all phases with percentage and
        phase labels
    - step: Disconnect internet during generation, reconnect after 10 seconds
      expected: Progress automatically resumes with correct state
    - step: Open multiple browser tabs during generation
      expected: All tabs receive synchronized progress updates
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup Redis progress key structure and TTL configuration
      done: false
    - task: Implement ProgressTracker service with job lifecycle management
      done: false
    - task: Create WebSocket endpoints with authentication middleware
      done: false
    - task: Build SSE fallback endpoint with same progress data
      done: false
    - task: Implement RunPod webhook handler with progress translation
      done: false
    - task: Create React hook with reconnection and fallback logic
      done: false
    - task: Build ProgressTracker UI component with phase visualization
      done: false
    - task: Integrate progress tracking into existing image generation flow
      done: false
    - task: Implement comprehensive test suite (unit, integration, e2e)
      done: false
    - task: Add monitoring and error logging for production debugging
      done: false
- key: T59
  title: SDXL Prompt Validation
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 2
  area: image-gen
  dependsOn:
  - T50
  agent_notes:
    research_findings: '**Context:**

      SDXL (Stable Diffusion XL) prompt validation is critical for the content generation
      pipeline to ensure high-quality comic panel generation. Invalid prompts can
      cause RunPod API failures, wasted compute resources, poor image quality, or
      NSFW content generation. This task ensures prompts are properly formatted, within
      token limits, contain appropriate descriptors for comic art style, and are filtered
      for content policy compliance before expensive GPU inference.


      **Technical Approach:**

      Implement a multi-layered validation system using Zod schemas for structure
      validation, custom validators for SDXL-specific constraints, and OpenAI moderation
      API for content filtering. Create a validation service that integrates with
      the existing image generation pipeline, providing both sync validation (for
      immediate feedback) and async batch validation (for processing queues). Use
      a caching layer (Redis) to avoid re-validating identical prompts.


      **Dependencies:**

      - External: zod (validation schemas), openai (moderation API), ioredis (caching),
      validator.js (string validation)

      - Internal: Image generation service, content moderation service, prompt engineering
      utilities, error handling middleware


      **Risks:**

      - Over-aggressive filtering: Could block legitimate comic art prompts; mitigation:
      implement allowlist for comic-specific terms

      - Performance bottleneck: Validation could slow down generation pipeline; mitigation:
      implement async validation with queuing

      - False positives in moderation: AI moderation may flag comic violence/fantasy;
      mitigation: custom moderation rules for comic context

      - Token counting accuracy: Different tokenizers between validation and SDXL;
      mitigation: use CLIP tokenizer for accurate counting


      **Complexity Notes:**

      Initially seems straightforward but complexity increases due to SDXL''s specific
      requirements (negative prompts, style modifiers, aspect ratios), comic genre
      considerations (fantasy violence, supernatural elements), and the need for context-aware
      validation that understands comic art terminology vs. potentially harmful content.


      **Key Files:**

      - packages/image-gen/src/validation/: New validation service directory

      - packages/image-gen/src/services/sdxl-service.ts: Integrate validation before
      API calls

      - packages/shared/src/schemas/prompt-schemas.ts: Zod schemas for prompt validation

      - apps/backend/src/routes/image-gen.ts: Add validation endpoint for frontend

      '
    design_decisions:
    - decision: Use Zod for schema validation with custom refinements
      rationale: Type-safe validation that integrates well with TypeScript codebase,
        allows custom validation logic, and provides detailed error messages for debugging
      alternatives_considered:
      - Joi validation
      - Manual validation functions
      - JSON Schema with ajv
    - decision: Implement tiered validation (fast local checks, then AI moderation)
      rationale: Optimize for performance by catching obvious issues locally before
        expensive API calls, while still ensuring content safety
      alternatives_considered:
      - AI-only moderation
      - Rule-based only
      - User reporting system
    - decision: Cache validation results by prompt hash
      rationale: Avoid redundant validation of identical prompts, especially important
        for batch processing and user iterations
      alternatives_considered:
      - No caching
      - Database-based caching
      - In-memory only caching
    researched_at: '2026-02-07T19:08:18.158557'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:33:07.613373'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a PromptValidationService that accepts SDXL prompt objects and
      runs them through progressive validation layers: structural validation (required
      fields, types), constraint validation (token limits, aspect ratios), content
      validation (moderation API), and comic-context validation (genre-appropriate
      terms). The service returns detailed validation results with specific error
      messages and suggestions. Integrate this service into the existing image generation
      pipeline as a pre-processing step, with caching to optimize repeated validations.

      '
    external_dependencies:
    - name: zod
      version: ^3.22.4
      reason: Type-safe schema validation with custom refinements for SDXL constraints
    - name: openai
      version: ^4.28.0
      reason: Content moderation API to filter inappropriate prompts
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for caching validation results by prompt hash
    - name: gpt-3-encoder
      version: ^1.1.4
      reason: Accurate token counting for prompt length validation
    - name: validator
      version: ^13.11.0
      reason: Additional string validation utilities for prompt sanitization
    files_to_modify:
    - path: packages/image-gen/src/services/sdxl-service.ts
      changes: Add PromptValidationService integration before RunPod API calls; modify
        generateImage method to validate prompts first
    - path: apps/backend/src/routes/image-gen.ts
      changes: Add /validate-prompt and /validate-batch endpoints; integrate validation
        middleware for existing generation endpoints
    - path: packages/image-gen/src/types/sdxl.ts
      changes: Add ValidationResult, ValidationError, and PromptValidationOptions
        type definitions
    - path: apps/backend/src/middleware/error-handler.ts
      changes: Add specific error handling for PromptValidationError with user-friendly
        messages
    new_files:
    - path: packages/image-gen/src/validation/prompt-validation-service.ts
      purpose: Main validation service orchestrating all validation layers
    - path: packages/image-gen/src/validation/content-moderator.ts
      purpose: OpenAI moderation API integration with comic-context awareness
    - path: packages/image-gen/src/validation/constraint-validator.ts
      purpose: SDXL-specific validation (tokens, aspect ratios, model parameters)
    - path: packages/image-gen/src/validation/comic-context-validator.ts
      purpose: Comic genre allowlist and context-aware filtering
    - path: packages/shared/src/schemas/prompt-validation-schemas.ts
      purpose: Zod schemas for SDXL prompt structure validation
    - path: packages/image-gen/src/validation/token-counter.ts
      purpose: CLIP tokenizer integration for accurate token counting
    - path: packages/image-gen/src/validation/validation-cache.ts
      purpose: Redis caching layer for validation results
    - path: packages/image-gen/src/validation/index.ts
      purpose: Barrel exports for validation services
  acceptance_criteria:
  - criterion: SDXL prompts are validated against token limits (77 tokens for positive,
      77 for negative prompts) and structural requirements
    verification: Unit test with prompts exceeding limits returns specific error messages;
      API endpoint rejects invalid prompts with 400 status
  - criterion: Content moderation prevents NSFW/harmful prompts while allowing comic-appropriate
      fantasy/violence terms
    verification: Integration test suite with 50+ test prompts (legitimate comic terms
      vs. actual NSFW content) achieves <5% false positive rate
  - criterion: Validation performance does not exceed 200ms for cached prompts and
      2s for new prompts including moderation API calls
    verification: Load test with 100 concurrent validation requests; Redis cache hit
      rate >80% in production metrics
  - criterion: Image generation pipeline integrates validation seamlessly with detailed
      error responses for invalid prompts
    verification: 'E2E test: Submit invalid prompt through frontend, receive specific
      validation error; valid prompt proceeds to RunPod API'
  - criterion: Validation service provides sync endpoint for real-time feedback and
      async processing for batch operations
    verification: API documentation shows both /validate-prompt (sync) and /validate-batch
      (async) endpoints; manual testing confirms <500ms response for sync validation
  testing:
    unit_tests:
    - file: packages/image-gen/src/validation/__tests__/prompt-validation.test.ts
      coverage_target: 90%
      scenarios:
      - Token limit validation for positive/negative prompts
      - Required field validation (prompt, model_version)
      - Aspect ratio validation against SDXL supported ratios
      - Comic-specific term allowlist functionality
      - Cache hit/miss scenarios
    - file: packages/image-gen/src/validation/__tests__/content-moderation.test.ts
      coverage_target: 85%
      scenarios:
      - OpenAI moderation API integration
      - Comic context filtering (violence/fantasy terms)
      - Moderation API failure handling
      - Custom moderation rules for comic art
    integration_tests:
    - file: packages/image-gen/src/__tests__/integration/validation-pipeline.test.ts
      scenarios:
      - Full validation pipeline with Redis caching
      - Integration with existing image generation service
      - Batch validation processing
      - Error propagation through pipeline
    e2e_tests:
    - file: apps/backend/src/__tests__/e2e/image-gen-validation.test.ts
      scenarios:
      - Frontend prompt submission through validation to RunPod
      - Invalid prompt rejection with user-friendly error messages
      - Valid prompt processing end-to-end
    manual_testing:
    - step: Submit comic prompt with fantasy violence terms (e.g., 'dragon battle,
        sword fighting')
      expected: Prompt passes validation and proceeds to image generation
    - step: Submit prompt exceeding 77 tokens
      expected: Returns specific error with token count and suggestions to shorten
    - step: Submit inappropriate content prompt
      expected: Blocked by content moderation with policy violation message
    - step: Test Redis cache performance with identical prompts
      expected: Second validation completes in <200ms
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup validation service structure and dependencies (zod, openai, ioredis)
      done: false
    - task: Implement Zod schemas for SDXL prompt structure validation
      done: false
    - task: Create token counting service using CLIP tokenizer
      done: false
    - task: Implement constraint validator for SDXL limits (tokens, aspect ratios)
      done: false
    - task: Build content moderator with OpenAI API integration and comic allowlist
      done: false
    - task: Create Redis caching layer for validation results
      done: false
    - task: Implement main PromptValidationService orchestrator
      done: false
    - task: Integrate validation into existing SDXL service and API routes
      done: false
    - task: Add comprehensive test suite (unit, integration, e2e)
      done: false
    - task: Update API documentation and error handling
      done: false
    - task: Performance testing and cache optimization
      done: false
    - task: Code review and deployment preparation
      done: false
- key: T60
  title: M3 Integration Testing
  type: Task
  milestone: M3 - Content Generation Pipeline
  iteration: I4
  priority: p0
  effort: 5
  area: image-gen
  dependsOn:
  - T54
  - T57
  - T59
  agent_notes:
    research_findings: '**Context:**

      M3 Integration Testing focuses on validating the entire Content Generation Pipeline
      end-to-end, particularly the image generation components. This is critical because
      the pipeline involves complex integrations between LLMs (for scene description),
      RunPod Stable Diffusion (for image generation), and Supabase (for asset storage).
      Without comprehensive integration tests, we risk deploying a system where individual
      components work but fail when orchestrated together, leading to poor user experience
      and potential data corruption.


      **Technical Approach:**

      - Use Playwright for full E2E testing of the novel-to-comic transformation workflow

      - Implement Vitest integration tests for service-to-service communication (API
      → ML services → Database)

      - Create mock RunPod environments using Docker containers for consistent testing

      - Use Supabase local development setup with test data fixtures

      - Implement visual regression testing for generated comic panels using pixelmatch

      - Create test harnesses that can validate image generation quality and consistency


      **Dependencies:**

      - External: @playwright/test, @supabase/supabase-js, pixelmatch, sharp, docker,
      msw (Mock Service Worker)

      - Internal: image-generation service, content-pipeline orchestrator, novel-parser
      service, supabase client wrapper


      **Risks:**

      - Flaky tests due to ML model non-determinism: Use seed values and mock responses
      for consistent results

      - RunPod API rate limits during testing: Implement request queuing and circuit
      breaker patterns

      - Large test asset storage costs: Use compressed test images and cleanup strategies

      - Long test execution times: Parallel test execution and selective test running
      based on changed files


      **Complexity Notes:**

      This is significantly more complex than typical integration testing due to:

      1. ML service dependencies that are inherently non-deterministic

      2. Large binary assets (images) that need validation beyond simple API responses

      3. Multi-service orchestration with async processing pipelines

      4. Visual quality assessment that goes beyond functional correctness


      **Key Files:**

      - tests/integration/content-pipeline.spec.ts: Main integration test suite

      - tests/fixtures/: Test novels, expected comic outputs, mock API responses

      - packages/image-gen/src/test-utils.ts: Image generation testing utilities

      - docker-compose.test.yml: Test environment orchestration

      - packages/testing/src/visual-regression.ts: Image comparison utilities

      '
    design_decisions:
    - decision: Use Docker-based RunPod mocking instead of live API calls
      rationale: Ensures test determinism, reduces costs, and eliminates external
        service dependencies that could cause CI failures
      alternatives_considered:
      - Live API with test account
      - Simple HTTP mocking
      - RunPod sandbox environment
    - decision: Implement visual regression testing for generated images
      rationale: Functional tests alone cannot validate that generated comic panels
        meet quality standards or detect visual regressions in ML models
      alternatives_considered:
      - Hash-based image comparison
      - Manual visual inspection
      - Metadata-only validation
    - decision: Create separate integration test database with Supabase branching
      rationale: Allows testing real database interactions without polluting development
        data or requiring complex cleanup logic
      alternatives_considered:
      - In-memory database
      - Shared test database
      - Transaction rollback strategy
    researched_at: '2026-02-07T19:08:43.778166'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:33:34.538880'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a comprehensive integration testing suite that validates the
      entire novel-to-comic pipeline using Docker-orchestrated services. Create test
      fixtures with known novel inputs and expected comic outputs, then use Playwright
      to drive the full user workflow while Vitest validates service-level integrations.
      Implement visual regression testing to ensure generated images meet quality
      standards, and use Supabase branching for isolated test environments.

      '
    external_dependencies:
    - name: '@playwright/test'
      version: ^1.40.0
      reason: End-to-end testing of the complete user workflow through the web interface
    - name: pixelmatch
      version: ^5.3.0
      reason: Pixel-level image comparison for visual regression testing of generated
        comic panels
    - name: sharp
      version: ^0.32.0
      reason: Image processing and normalization for consistent test comparisons
    - name: msw
      version: ^2.0.0
      reason: Mock external API calls to OpenAI/Anthropic during integration tests
    - name: docker
      version: ^24.0.0
      reason: Orchestrate test environment with mock RunPod services and isolated
        databases
    - name: testcontainers
      version: ^10.2.0
      reason: Programmatically manage Docker containers for Stable Diffusion API mocking
    files_to_modify:
    - path: packages/image-gen/src/services/image-generation.ts
      changes: Add test mode flags, deterministic seed support for consistent testing
    - path: packages/supabase/src/client.ts
      changes: Add test database configuration, cleanup utilities for test data
    - path: apps/backend/src/services/content-pipeline.ts
      changes: Add integration test hooks, mock service injection points
    new_files:
    - path: tests/integration/content-pipeline.spec.ts
      purpose: Main integration test suite for end-to-end pipeline validation
    - path: tests/fixtures/novels/test-novel-short.txt
      purpose: Minimal novel for fast integration testing
    - path: tests/fixtures/novels/test-novel-complex.txt
      purpose: Complex multi-chapter novel for comprehensive testing
    - path: tests/fixtures/expected-outputs/
      purpose: Baseline comic panels and metadata for regression testing
    - path: packages/testing/src/visual-regression.ts
      purpose: Image comparison utilities using pixelmatch and sharp
    - path: packages/testing/src/test-utils.ts
      purpose: Shared testing utilities, mock factories, test data generators
    - path: docker-compose.test.yml
      purpose: Orchestrate test environment with mock RunPod, local Supabase
    - path: tests/setup/test-environment.ts
      purpose: Test environment initialization, database seeding, cleanup
    - path: tests/mocks/runpod-mock-server.ts
      purpose: MSW-based RunPod API mocking with realistic response delays
    - path: tests/e2e/comic-generation-workflow.spec.ts
      purpose: Playwright-driven end-to-end user workflow testing
    - path: .github/workflows/integration-tests.yml
      purpose: CI/CD pipeline configuration for integration testing
  acceptance_criteria:
  - criterion: End-to-end novel-to-comic pipeline completes successfully with all
      services integrated
    verification: Run `npm run test:e2e` - all Playwright tests pass, comic panels
      are generated and stored in Supabase
  - criterion: Generated comic images match visual quality standards with <5% pixel
      difference from baseline
    verification: Visual regression tests pass with `npm run test:visual-regression`
      showing pixelmatch scores
  - criterion: Pipeline handles failures gracefully with proper error recovery and
      logging
    verification: Integration tests simulate RunPod failures, database timeouts -
      system recovers without data corruption
  - criterion: Test suite runs in <10 minutes with reliable, non-flaky results
    verification: CI pipeline completes integration tests within time limit with <1%
      failure rate over 10 runs
  - criterion: All service-to-service communications are properly mocked and tested
    verification: Vitest integration tests cover LLM→RunPod→Supabase flow with MSW
      mocks, 90%+ coverage
  testing:
    unit_tests:
    - file: packages/image-gen/src/__tests__/image-validator.test.ts
      coverage_target: 90%
      scenarios:
      - Valid comic panel validation
      - Corrupted image handling
      - Unsupported format rejection
    - file: packages/testing/src/__tests__/visual-regression.test.ts
      coverage_target: 85%
      scenarios:
      - Pixel-perfect image comparison
      - Acceptable difference thresholds
      - Baseline image management
    integration_tests:
    - file: tests/integration/content-pipeline.spec.ts
      scenarios:
      - Complete novel-to-comic transformation
      - Multi-panel comic generation
      - Asset storage and retrieval
      - Error recovery and retry logic
    - file: tests/integration/service-communication.spec.ts
      scenarios:
      - LLM scene description generation
      - RunPod Stable Diffusion API calls
      - Supabase asset upload/download
      - Pipeline orchestration timing
    e2e_tests:
    - file: tests/e2e/comic-generation-workflow.spec.ts
      scenarios:
      - User uploads novel, receives completed comic
      - Progress tracking throughout pipeline
      - Error handling in UI
    manual_testing:
    - step: Upload test novel 'sample-adventure.txt' through UI
      expected: Comic generation starts, progress indicator shows pipeline stages
    - step: Monitor generated panels for visual quality and story coherence
      expected: Each panel matches scene description, consistent art style maintained
    - step: Verify final comic download and sharing functionality
      expected: PDF/image export works, social sharing generates correct previews
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Set up Docker test environment with mock services
      done: false
    - task: Create test fixtures (sample novels, expected outputs)
      done: false
    - task: Implement visual regression testing utilities with pixelmatch
      done: false
    - task: Build MSW mocks for RunPod and external API dependencies
      done: false
    - task: Write Vitest integration tests for service-to-service communication
      done: false
    - task: Implement Playwright E2E tests for user workflows
      done: false
    - task: Add test mode configurations to existing services
      done: false
    - task: Create CI/CD pipeline for automated integration testing
      done: false
    - task: Optimize test execution time and reliability
      done: false
    - task: Document testing procedures and maintenance guidelines
      done: false
- key: T61
  title: ComfyUI Integration
  type: Task
  milestone: M3 - Content Generation Pipeline
  iteration: I5
  priority: p0
  effort: 5
  area: comic
  dependsOn:
  - T52
  agent_notes:
    research_findings: '**Context:**

      ComfyUI integration replaces the current RunPod Stable Diffusion setup with
      ComfyUI, a node-based UI for Stable Diffusion that offers more flexible workflow
      composition and better control over the image generation pipeline. This is crucial
      for comic generation as it allows for consistent character generation, style
      transfer, panel layout control, and advanced prompt engineering through visual
      workflows. ComfyUI provides better reproducibility and fine-grained control
      over the generation process compared to basic Stable Diffusion APIs.


      **Technical Approach:**

      - Create a ComfyUI workflow service that communicates via WebSocket API

      - Design workflow templates for different comic generation tasks (character
      sheets, panels, backgrounds)

      - Implement a queue system for batch processing comic pages

      - Create workflow builders for dynamic prompt injection and parameter adjustment

      - Use ComfyUI''s API mode for headless operation in production

      - Implement workflow versioning and template management


      **Dependencies:**

      - External: [@comfyui/api-client, ws, axios, form-data, sharp]

      - Internal: existing image processing service, job queue system, comic generation
      pipeline


      **Risks:**

      - ComfyUI API stability: Monitor API changes and maintain fallback workflows

      - Workflow complexity: Start with simple templates and gradually add complexity

      - Resource management: Implement proper GPU memory cleanup and queue throttling

      - Workflow persistence: Store workflow definitions in database for consistency


      **Complexity Notes:**

      More complex than initial estimate due to workflow management requirements and
      the need to create reusable templates for different comic generation scenarios.
      The visual nature of ComfyUI workflows requires careful abstraction.


      **Key Files:**

      - apps/backend/src/services/comfyui/: New service directory

      - apps/backend/src/services/comic-generation/: Integration with existing pipeline

      - packages/shared/src/types/comfyui.ts: Type definitions

      - apps/backend/src/config/comfyui.ts: Configuration management

      '
    design_decisions:
    - decision: Use ComfyUI API mode with WebSocket communication
      rationale: Provides real-time progress updates and better resource management
        than HTTP polling
      alternatives_considered:
      - HTTP-only API
      - Direct Python integration
      - Docker container per job
    - decision: Implement workflow template system with parameter injection
      rationale: Allows reusable workflows while maintaining flexibility for different
        comic styles and requirements
      alternatives_considered:
      - Hardcoded workflows
      - Dynamic workflow generation
      - User-defined workflows
    - decision: Queue-based processing with job prioritization
      rationale: Handles multiple comic generation requests efficiently and allows
        for different priority levels
      alternatives_considered:
      - Direct API calls
      - Batch processing only
      - Real-time generation
    researched_at: '2026-02-07T19:09:02.941748'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:34:01.850954'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a ComfyUI service that manages workflow templates for comic
      generation tasks. Create a WebSocket-based communication layer for real-time
      progress tracking. Build workflow builders that inject dynamic parameters (prompts,
      styles, characters) into predefined templates. Integrate with the existing comic
      generation pipeline through a queue system that processes panels, characters,
      and backgrounds using appropriate ComfyUI workflows.

      '
    external_dependencies:
    - name: ws
      version: ^8.14.2
      reason: WebSocket client for ComfyUI API communication
    - name: form-data
      version: ^4.0.0
      reason: Multipart form data for image uploads to ComfyUI
    - name: sharp
      version: ^0.32.6
      reason: Image processing and format conversion
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for job queue and workflow caching
    files_to_modify:
    - path: apps/backend/src/services/comic-generation/comic-generator.ts
      changes: Add ComfyUI integration option, modify generatePanel method to use
        ComfyUI workflows
    - path: apps/backend/src/services/image-processing/image-service.ts
      changes: Add ComfyUI result processing, integrate with existing post-processing
        pipeline
    - path: apps/backend/src/routes/comic.ts
      changes: Add ComfyUI-specific endpoints for workflow management and progress
        tracking
    - path: apps/backend/src/config/index.ts
      changes: Add ComfyUI configuration section with server URL, API keys, timeouts
    - path: packages/shared/src/types/comic.ts
      changes: Add ComfyUI workflow types and generation request interfaces
    new_files:
    - path: apps/backend/src/services/comfyui/client.ts
      purpose: Core ComfyUI API client with WebSocket and HTTP communication
    - path: apps/backend/src/services/comfyui/workflow-builder.ts
      purpose: Dynamic workflow template builder with parameter injection
    - path: apps/backend/src/services/comfyui/template-manager.ts
      purpose: Workflow template storage, versioning, and retrieval system
    - path: apps/backend/src/services/comfyui/queue-manager.ts
      purpose: Queue management for batch processing and resource optimization
    - path: apps/backend/src/services/comfyui/index.ts
      purpose: Main ComfyUI service orchestrator and public interface
    - path: packages/shared/src/types/comfyui.ts
      purpose: TypeScript definitions for ComfyUI workflows, nodes, and API responses
    - path: apps/backend/src/config/comfyui.ts
      purpose: ComfyUI-specific configuration management and validation
    - path: apps/backend/src/services/comfyui/templates/character-sheet.json
      purpose: Character generation workflow template
    - path: apps/backend/src/services/comfyui/templates/panel-generation.json
      purpose: Comic panel generation workflow template
    - path: apps/backend/src/services/comfyui/templates/background.json
      purpose: Background generation workflow template
    - path: apps/backend/src/middleware/comfyui-auth.ts
      purpose: ComfyUI API authentication and rate limiting middleware
  acceptance_criteria:
  - criterion: ComfyUI service successfully generates comic panels using predefined
      workflow templates
    verification: POST /api/comic/generate-panel with character and scene prompts
      returns generated image URL within 30 seconds
  - criterion: WebSocket connection provides real-time progress updates during image
      generation
    verification: Connect to WS endpoint and verify progress events (queued, processing,
      completed) are received during generation
  - criterion: Workflow templates support dynamic parameter injection for characters,
      styles, and prompts
    verification: Generate same character with different poses using character template
      - verify consistency and pose variation
  - criterion: Queue system processes multiple comic generation requests without memory
      leaks
    verification: Submit 10 concurrent panel generation requests, monitor GPU memory
      usage remains stable
  - criterion: Fallback mechanism activates when ComfyUI service is unavailable
    verification: Stop ComfyUI server, attempt generation - verify graceful fallback
      to existing SD API with appropriate error logging
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/comfyui/workflow-builder.test.ts
      coverage_target: 90%
      scenarios:
      - Template parameter injection
      - Workflow validation
      - Invalid parameter handling
    - file: apps/backend/src/__tests__/services/comfyui/client.test.ts
      coverage_target: 85%
      scenarios:
      - WebSocket connection management
      - API request/response handling
      - Error recovery mechanisms
    - file: apps/backend/src/__tests__/services/comfyui/queue-manager.test.ts
      coverage_target: 90%
      scenarios:
      - Queue prioritization
      - Batch processing
      - Resource cleanup
    integration_tests:
    - file: apps/backend/src/__tests__/integration/comfyui-comic-pipeline.test.ts
      scenarios:
      - End-to-end panel generation workflow
      - Character consistency across panels
      - Queue integration with comic generation service
    - file: apps/backend/src/__tests__/integration/comfyui-websocket.test.ts
      scenarios:
      - Real-time progress tracking
      - Connection recovery on failures
    manual_testing:
    - step: Generate character sheet using ComfyUI character template
      expected: Consistent character across 4 different poses with same facial features
        and style
    - step: Create comic page with 3 panels using different workflow templates
      expected: All panels maintain visual consistency and complete within 2 minutes
    - step: Monitor ComfyUI dashboard during batch processing
      expected: GPU memory usage stable, no workflow errors, proper cleanup between
        jobs
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Set up ComfyUI development environment and API exploration
      done: false
    - task: Create core ComfyUI client with WebSocket and HTTP API integration
      done: false
    - task: Implement workflow template system with parameter injection
      done: false
    - task: Build queue manager for batch processing and resource management
      done: false
    - task: Create initial workflow templates for characters, panels, and backgrounds
      done: false
    - task: Integrate ComfyUI service with existing comic generation pipeline
      done: false
    - task: Implement WebSocket progress tracking and error handling
      done: false
    - task: Add fallback mechanism to existing Stable Diffusion API
      done: false
    - task: Create comprehensive test suite with mocked ComfyUI responses
      done: false
    - task: Write API documentation and workflow template guides
      done: false
    - task: Performance testing and GPU memory optimization
      done: false
    - task: Code review and security audit
      done: false
- key: T62
  title: Image Enhancement Pipeline
  type: Feature
  milestone: M3 - Content Generation Pipeline
  iteration: I5
  priority: p0
  effort: 3
  area: comic
  dependsOn:
  - T61
  agent_notes:
    research_findings: '**Context:**

      Image enhancement is critical for comic generation quality. Raw AI-generated
      images often have inconsistencies in lighting, contrast, artifacts, and style
      that make them unsuitable for professional comic panels. This pipeline ensures
      all generated images meet quality standards through automated enhancement (upscaling,
      denoising, color correction) and provides fallback manual enhancement options.
      Essential for maintaining visual consistency across comic pages and meeting
      user expectations for polished output.


      **Technical Approach:**

      Implement a multi-stage enhancement pipeline using RunPod for GPU-intensive
      operations:

      1. **Pre-enhancement Analysis**: Image quality assessment using computer vision
      metrics

      2. **Automated Enhancement Chain**: Real-ESRGAN for upscaling, GFPGAN for face
      enhancement, color/contrast correction

      3. **Quality Validation**: Automated scoring to determine if additional enhancement
      needed

      4. **Fallback Processing**: Alternative enhancement models for failed cases

      5. **Caching Layer**: Redis for enhanced image variants to avoid reprocessing


      Use a queue-based architecture with Bull/BullMQ for processing jobs, integrate
      with existing RunPod infrastructure, and provide real-time progress updates
      via WebSockets.


      **Dependencies:**

      - External: @bull-board/api, @bull-board/fastify, ioredis, sharp, opencv4nodejs-prebuilt,
      image-js, runpod-python (via API)

      - Internal: Existing RunPod service, image storage service, WebSocket notification
      system, comic generation pipeline


      **Risks:**

      - **GPU Resource Contention**: Multiple enhancement jobs competing with Stable
      Diffusion - implement priority queuing and resource monitoring

      - **Processing Time**: Enhancement can take 30-60s per image - add progress
      tracking and batch processing capabilities

      - **Quality Inconsistency**: Different enhancement models may produce varying
      results - implement A/B testing framework for model selection

      - **Cost Scaling**: GPU usage costs increase significantly - add usage monitoring
      and optimization algorithms

      - **Memory Leaks**: Image processing can consume large amounts of RAM - implement
      proper cleanup and memory monitoring


      **Complexity Notes:**

      More complex than initially estimated. Requires sophisticated orchestration
      of multiple AI models, real-time progress tracking, quality assessment algorithms,
      and integration with existing comic generation workflow. The need for fallback
      strategies and quality validation adds significant complexity beyond basic image
      enhancement.


      **Key Files:**

      - apps/api/src/services/enhancement/: New service directory for enhancement
      logic

      - apps/api/src/queues/enhancement-queue.ts: Job queue management

      - apps/api/src/routes/enhancement/: API endpoints for enhancement operations

      - packages/shared/types/enhancement.ts: TypeScript types for enhancement pipeline

      - apps/dashboard/src/components/comic/ImageEnhancementPanel.tsx: UI for manual
      enhancement controls

      '
    design_decisions:
    - decision: Use RunPod + queue-based architecture instead of local processing
      rationale: GPU-intensive operations require specialized hardware. Queue system
        provides better resource management and scalability than synchronous processing
      alternatives_considered:
      - Local GPU processing
      - AWS SageMaker endpoints
      - Replicate API
    - decision: Implement multi-stage enhancement pipeline with quality gates
      rationale: Different images require different enhancement strategies. Quality
        gates prevent over-processing and ensure consistent output
      alternatives_considered:
      - Single-stage enhancement
      - User-controlled manual enhancement only
    - decision: Redis caching for enhanced image variants
      rationale: Enhancement is expensive and time-consuming. Caching prevents redundant
        processing of similar images
      alternatives_considered:
      - Database blob storage
      - File-system caching
      - No caching
    researched_at: '2026-02-07T19:09:29.671405'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:34:31.746712'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a microservice-style enhancement pipeline integrated with existing
      RunPod infrastructure. Jobs enter through Fastify API endpoints, get queued
      with BullMQ, processed on RunPod GPUs using Real-ESRGAN/GFPGAN models, validated
      for quality, and cached in Redis. WebSocket connections provide real-time progress
      updates to the dashboard. The pipeline integrates with the comic generation
      workflow through event-driven architecture, automatically enhancing images post-generation
      while allowing manual re-enhancement requests.

      '
    external_dependencies:
    - name: bullmq
      version: ^5.0.0
      reason: Robust job queue system for managing enhancement tasks with priority
        and retry capabilities
    - name: '@bull-board/api'
      version: ^5.0.0
      reason: Web UI for monitoring enhancement job queues and debugging processing
        issues
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for job queue backend and enhanced image caching
    - name: sharp
      version: ^0.33.0
      reason: High-performance image processing for pre/post-processing and format
        conversions
    - name: image-js
      version: ^0.35.0
      reason: Computer vision utilities for image quality assessment and analysis
    - name: axios
      version: ^1.6.0
      reason: HTTP client for RunPod API communication and webhook handling
    files_to_modify:
    - path: apps/api/src/services/runpod/runpod-service.ts
      changes: Add enhancement job types and model configurations for Real-ESRGAN/GFPGAN
    - path: apps/api/src/config/redis.ts
      changes: Add enhancement cache configuration and TTL settings
    - path: apps/api/src/routes/index.ts
      changes: Register enhancement route handlers
    - path: apps/api/src/services/websocket/websocket-service.ts
      changes: Add enhancement progress event types and handlers
    - path: packages/shared/types/index.ts
      changes: Export enhancement types
    new_files:
    - path: apps/api/src/services/enhancement/enhancement-service.ts
      purpose: Core enhancement orchestration and model coordination
    - path: apps/api/src/services/enhancement/quality-validator.ts
      purpose: Image quality assessment and enhancement decision logic
    - path: apps/api/src/services/enhancement/cache-manager.ts
      purpose: Redis-based caching for enhanced image variants
    - path: apps/api/src/services/enhancement/models/real-esrgan.ts
      purpose: Real-ESRGAN upscaling model integration
    - path: apps/api/src/services/enhancement/models/gfpgan.ts
      purpose: GFPGAN face enhancement model integration
    - path: apps/api/src/queues/enhancement-queue.ts
      purpose: BullMQ job queue management for enhancement processing
    - path: apps/api/src/routes/enhancement/index.ts
      purpose: Enhancement API endpoint definitions
    - path: apps/api/src/routes/enhancement/process.ts
      purpose: Image processing endpoint handlers
    - path: apps/api/src/routes/enhancement/status.ts
      purpose: Job status and progress tracking endpoints
    - path: packages/shared/types/enhancement.ts
      purpose: TypeScript interfaces for enhancement pipeline
    - path: apps/dashboard/src/components/comic/ImageEnhancementPanel.tsx
      purpose: UI component for manual enhancement controls
    - path: apps/dashboard/src/hooks/useImageEnhancement.ts
      purpose: React hook for enhancement API integration
    - path: apps/api/src/middlewares/enhancement-validation.ts
      purpose: Request validation for enhancement endpoints
    - path: scripts/runpod/enhancement-models/setup-enhancement-env.py
      purpose: RunPod environment setup for enhancement models
  acceptance_criteria:
  - criterion: Image enhancement pipeline processes images through automated enhancement
      chain with upscaling, denoising, and color correction
    verification: POST /api/enhancement/process with test image returns enhanced image
      with min 2x resolution increase and quality score >0.8
  - criterion: Queue-based processing handles concurrent enhancement jobs with progress
      tracking
    verification: Submit 5 concurrent enhancement jobs, verify all complete successfully
      with real-time progress updates via WebSocket
  - criterion: Quality validation automatically determines if additional enhancement
      is needed and triggers fallback processing
    verification: Process deliberately low-quality test image, verify quality score
      triggers fallback enhancement pipeline
  - criterion: Enhanced images are cached in Redis to avoid reprocessing identical
      requests
    verification: Process same image twice, second request returns cached result in
      <500ms vs >30s for fresh processing
  - criterion: Pipeline integrates with comic generation workflow and provides manual
      re-enhancement capabilities
    verification: Generate comic panel, verify auto-enhancement triggers, then manually
      request re-enhancement with different settings
  testing:
    unit_tests:
    - file: apps/api/src/services/enhancement/__tests__/enhancement-service.test.ts
      coverage_target: 90%
      scenarios:
      - Image quality assessment scoring
      - Enhancement parameter calculation
      - Cache key generation and retrieval
      - Error handling for invalid images
      - Model selection logic
    - file: apps/api/src/services/enhancement/__tests__/quality-validator.test.ts
      coverage_target: 85%
      scenarios:
      - Quality score calculation
      - Threshold-based enhancement decisions
      - Fallback trigger conditions
    integration_tests:
    - file: apps/api/src/__tests__/integration/enhancement-pipeline.test.ts
      scenarios:
      - End-to-end enhancement workflow
      - RunPod integration and job processing
      - Redis caching behavior
      - WebSocket progress notifications
      - Queue management under load
    - file: apps/api/src/__tests__/integration/comic-enhancement-integration.test.ts
      scenarios:
      - Auto-enhancement trigger from comic generation
      - Manual re-enhancement requests
      - Progress tracking across multiple panels
    manual_testing:
    - step: Upload test comic images with various quality issues (blur, low resolution,
        artifacts)
      expected: Each image processes through appropriate enhancement models and shows
        visible quality improvement
    - step: Monitor enhancement queue during high load (10+ concurrent jobs)
      expected: Jobs process in priority order, progress updates remain accurate,
        no memory leaks observed
    - step: Test fallback processing by submitting corrupted/problematic images
      expected: System gracefully handles failures, attempts fallback models, provides
        meaningful error messages
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup enhancement service architecture and dependency configuration
      done: false
    - task: Implement core enhancement service with quality validation logic
      done: false
    - task: Create BullMQ queue management and job processing handlers
      done: false
    - task: Integrate Real-ESRGAN and GFPGAN models with RunPod service
      done: false
    - task: Implement Redis caching layer for enhanced image variants
      done: false
    - task: Build API endpoints for enhancement processing and status tracking
      done: false
    - task: Create WebSocket progress notification system
      done: false
    - task: Integrate enhancement pipeline with comic generation workflow
      done: false
    - task: Build dashboard UI components for manual enhancement controls
      done: false
    - task: Implement comprehensive error handling and fallback processing
      done: false
    - task: Add monitoring, logging, and performance optimization
      done: false
    - task: Complete testing suite and documentation
      done: false
- key: T70
  title: Quality Control Dashboard
  type: Feature
  milestone: M4 - Dashboard & UI
  iteration: I5
  priority: p1
  effort: 3
  area: comic
  dependsOn:
  - T64
  agent_notes:
    research_findings: '**Context:**

      A Quality Control Dashboard is essential for the novel-to-comic transformation
      pipeline to ensure generated comics meet quality standards before publication.
      This addresses the critical need to catch AI-generated content issues (poor
      image quality, inconsistent character designs, narrative flow problems, text
      readability) and provides human reviewers with tools to approve, reject, or
      request revisions. Without this, low-quality comics could reach users, damaging
      the platform''s reputation and user experience.


      **Technical Approach:**

      Build a dedicated QC dashboard within the existing Next.js admin interface using
      React Server Components for performance. Implement a queue-based review system
      with real-time updates via Supabase realtime subscriptions. Use a card-based
      layout for comic review with side-by-side comparison views, annotation tools,
      and batch operations. Integrate with the existing comic generation pipeline
      by adding QC status fields to the comics table and creating approval workflows.


      **Dependencies:**

      - External: @tanstack/react-query for state management, react-hook-form for
      review forms, fabric.js for image annotation, react-virtualized for large lists

      - Internal: existing auth system, comic generation services, notification system,
      user management, Supabase client


      **Risks:**

      - Bottleneck in review process: implement parallel review workflows and priority
      queues

      - Large image loading performance: add progressive loading, thumbnails, and
      CDN optimization

      - Reviewer fatigue from poor UX: design intuitive interfaces with keyboard shortcuts
      and batch operations

      - Inconsistent quality standards: create detailed review guidelines and training
      materials


      **Complexity Notes:**

      More complex than initially expected due to need for real-time collaboration
      features, image annotation capabilities, and integration with existing comic
      generation workflow. The challenge lies in building an efficient review interface
      that handles large volumes of visual content while maintaining good performance.


      **Key Files:**

      - apps/dashboard/src/app/(protected)/quality-control/page.tsx: main QC dashboard

      - apps/dashboard/src/components/qc/ReviewCard.tsx: individual comic review component

      - packages/database/migrations/: add QC status and reviewer fields to comics
      table

      - apps/backend/src/routes/qc/: QC API endpoints for reviews and approvals

      - packages/shared/types/qc.ts: QC-related TypeScript types

      '
    design_decisions:
    - decision: Queue-based review system with real-time updates
      rationale: Enables multiple reviewers to work efficiently without conflicts,
        with immediate visibility into review progress
      alternatives_considered:
      - Simple list-based approach
      - Assignment-based system
      - AI-first with human override
    - decision: In-browser image annotation using Fabric.js
      rationale: Allows reviewers to mark specific issues directly on comic panels
        without external tools
      alternatives_considered:
      - Comment-only system
      - External annotation tools
      - Simple approve/reject workflow
    - decision: Integration with existing Supabase realtime for live updates
      rationale: Leverages existing infrastructure for real-time collaboration and
        status updates
      alternatives_considered:
      - WebSocket implementation
      - Polling-based updates
      - Server-sent events
    researched_at: '2026-02-07T19:09:53.022205'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:34:57.339747'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a dedicated QC section in the dashboard with a kanban-style
      board showing comics in different review states (pending, in-review, approved,
      rejected). Build reusable ReviewCard components that display comic panels with
      overlay annotation tools. Implement real-time status updates using Supabase
      subscriptions so multiple reviewers see live changes. Add batch operations for
      efficient processing and integrate approval workflows that trigger comic publication
      or regeneration requests.

      '
    external_dependencies:
    - name: fabric
      version: ^5.3.0
      reason: Canvas-based image annotation for marking quality issues on comic panels
    - name: '@tanstack/react-query'
      version: ^5.0.0
      reason: Efficient data fetching and caching for comic review queue management
    - name: react-hook-form
      version: ^7.48.0
      reason: Form handling for review comments, ratings, and approval workflows
    - name: react-window
      version: ^1.8.8
      reason: Virtualization for large lists of comics to maintain performance
    files_to_modify:
    - path: packages/database/schema.sql
      changes: Add qc_status, reviewer_id, review_comments, annotations columns to
        comics table
    - path: apps/dashboard/src/app/(protected)/layout.tsx
      changes: Add Quality Control navigation item to admin sidebar
    - path: packages/shared/types/index.ts
      changes: Export QC types and extend Comic interface with review fields
    new_files:
    - path: apps/dashboard/src/app/(protected)/quality-control/page.tsx
      purpose: Main QC dashboard with kanban board layout
    - path: apps/dashboard/src/components/qc/KanbanBoard.tsx
      purpose: Kanban board component with drag-and-drop functionality
    - path: apps/dashboard/src/components/qc/ReviewCard.tsx
      purpose: Individual comic review card with thumbnail and quick actions
    - path: apps/dashboard/src/components/qc/ReviewModal.tsx
      purpose: Full-screen comic review with annotation tools
    - path: apps/dashboard/src/components/qc/AnnotationCanvas.tsx
      purpose: Fabric.js-based annotation overlay for comic panels
    - path: apps/dashboard/src/components/qc/BatchActions.tsx
      purpose: Batch selection and operations component
    - path: apps/backend/src/routes/qc/index.ts
      purpose: QC API routes for reviews, status updates, batch operations
    - path: packages/database/migrations/20240315_add_qc_fields.sql
      purpose: Database migration for QC-related columns
    - path: packages/shared/types/qc.ts
      purpose: TypeScript interfaces for QC status, reviews, annotations
    - path: apps/dashboard/src/hooks/useQCSubscription.ts
      purpose: Custom hook for real-time QC status updates via Supabase
    - path: apps/backend/src/services/qc-service.ts
      purpose: Business logic for QC operations and workflow management
  acceptance_criteria:
  - criterion: 'QC dashboard displays comics in kanban-style board with states: pending,
      in-review, approved, rejected'
    verification: Navigate to /quality-control, verify board shows 4 columns with
      comics sorted by status
  - criterion: Reviewers can annotate comic panels and submit approval/rejection with
      comments
    verification: Click on comic card, add annotations using fabric.js tools, submit
      review form with status change
  - criterion: Real-time updates show status changes to all connected reviewers within
      2 seconds
    verification: Open dashboard in two browsers, change comic status in one, verify
      other updates automatically
  - criterion: Batch operations allow selecting and approving/rejecting multiple comics
      simultaneously
    verification: Select 5+ comics using checkboxes, use batch action buttons, verify
      all selected items update status
  - criterion: Performance handles 100+ comics with lazy loading and maintains <3s
      initial load time
    verification: Seed database with 100+ comics, measure initial page load with browser
      dev tools
  testing:
    unit_tests:
    - file: apps/dashboard/src/components/qc/__tests__/ReviewCard.test.tsx
      coverage_target: 90%
      scenarios:
      - Renders comic panels correctly
      - Annotation tools functionality
      - Status change handling
      - Error states display
    - file: apps/backend/src/routes/qc/__tests__/qc-routes.test.ts
      coverage_target: 85%
      scenarios:
      - GET /qc/comics returns paginated results
      - POST /qc/review creates review with validation
      - PATCH /qc/batch-update handles multiple comics
      - Authorization checks for reviewer role
    integration_tests:
    - file: apps/backend/src/__tests__/integration/qc-workflow.test.ts
      scenarios:
      - Comic submission to approval pipeline
      - Real-time subscription updates
      - Batch operations with database consistency
    manual_testing:
    - step: Login as reviewer, navigate to QC dashboard
      expected: Dashboard loads with comics in appropriate columns
    - step: Drag comic between columns
      expected: Status updates in database and other browsers
    - step: Test annotation tools on comic panels
      expected: Annotations persist and display to other reviewers
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 8
  progress:
    status: not-started
    checklist:
    - task: Create database migration for QC fields and run migration
      done: false
    - task: Implement TypeScript types and interfaces for QC system
      done: false
    - task: Build QC backend API routes with authentication and validation
      done: false
    - task: Create KanbanBoard component with drag-and-drop using react-beautiful-dnd
      done: false
    - task: Implement ReviewCard component with comic thumbnail and quick actions
      done: false
    - task: Build ReviewModal with fabric.js annotation canvas integration
      done: false
    - task: Add real-time subscription hooks using Supabase realtime
      done: false
    - task: Implement batch operations with optimistic updates
      done: false
    - task: 'Add performance optimizations: virtualization and lazy loading'
      done: false
    - task: Write comprehensive tests and documentation
      done: false
- key: T72
  title: Comic Preview
  type: Feature
  milestone: M4 - Dashboard & UI
  iteration: I5
  priority: p1
  effort: 3
  area: comic
  dependsOn:
  - T64
  agent_notes:
    research_findings: '**Context:**

      Comic Preview allows users to see how their novel chapters will look as comic
      pages before finalizing the transformation. This is crucial for user confidence
      and iteration - users need to validate panel layouts, character consistency,
      dialogue placement, and overall visual flow before committing to expensive generation
      operations. It reduces wasted generations and improves user satisfaction by
      making the process more transparent and controllable.


      **Technical Approach:**

      Build a real-time preview system using React components with canvas/SVG rendering
      for comic layouts. Implement a lightweight preview generation pipeline that
      creates low-res mockups using cached character designs and simplified panel
      structures. Use WebSockets for real-time updates as users modify settings. Leverage
      Supabase real-time subscriptions for collaborative editing scenarios. Create
      a responsive preview component that scales from mobile to desktop with zoom/pan
      capabilities.


      **Dependencies:**

      - External: fabric.js for canvas manipulation, react-zoom-pan-pinch for viewport
      control, html2canvas for export functionality, framer-motion for smooth transitions

      - Internal: Comic generation service APIs, Character design cache service, Panel
      layout algorithms, Scene parsing utilities, User preferences service


      **Risks:**

      - Performance degradation: Large novels could create memory issues with too
      many preview panels - implement virtualization and lazy loading

      - Character consistency: Preview might not match final output due to different
      generation parameters - maintain strict parity between preview and production
      pipelines

      - Real-time sync complexity: Multiple users editing same comic could cause race
      conditions - implement operational transforms or conflict resolution

      - Mobile performance: Canvas operations are CPU intensive on mobile devices
      - provide simplified mobile preview mode


      **Complexity Notes:**

      More complex than initially expected due to need for real-time rendering pipeline
      that mirrors production quality. The preview system essentially requires building
      a lightweight version of the entire comic generation stack. Canvas performance
      optimization and responsive design across devices adds significant complexity.


      **Key Files:**

      - apps/dashboard/src/components/comic/ComicPreview.tsx: Main preview component

      - apps/dashboard/src/hooks/useComicPreview.ts: Preview state management

      - packages/comic-engine/src/preview/: Lightweight preview generation pipeline

      - apps/api/src/routes/comic/preview.ts: Preview API endpoints

      - packages/ui/src/components/Canvas/: Shared canvas utilities

      '
    design_decisions:
    - decision: Canvas-based rendering with fabric.js for interactive preview
      rationale: Provides pixel-perfect control over comic layouts, supports drag-drop
        editing, and can export high-quality previews. Better performance than DOM-based
        approaches for complex layouts.
      alternatives_considered:
      - SVG-based rendering
      - HTML/CSS grid layouts
      - WebGL with three.js
    - decision: Separate preview pipeline from production comic generation
      rationale: Allows optimizations for speed over quality in previews, prevents
        preview operations from affecting production generation queue, enables different
        caching strategies.
      alternatives_considered:
      - Reuse production pipeline with quality flags
      - Client-side only preview generation
    - decision: WebSocket-based real-time updates with Supabase realtime
      rationale: Provides instant feedback as users adjust parameters, supports collaborative
        editing scenarios, integrates well with existing Supabase infrastructure.
      alternatives_considered:
      - HTTP polling
      - Server-sent events
      - Custom WebSocket implementation
    researched_at: '2026-02-07T19:10:18.036986'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:35:28.726201'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a dual-layer preview system: a fast client-side renderer
      for immediate feedback using cached assets, and a server-side preview generator
      for higher-fidelity mockups. The client component uses fabric.js canvas for
      interactive panel editing with drag-drop support. Real-time parameter changes
      trigger debounced API calls to generate updated preview data. Implement viewport
      virtualization to handle long comics efficiently, loading panels on-demand as
      users scroll.

      '
    external_dependencies:
    - name: fabric
      version: ^5.3.0
      reason: Canvas manipulation and interactive editing capabilities
    - name: react-zoom-pan-pinch
      version: ^3.1.0
      reason: Smooth zoom and pan controls for comic viewport
    - name: html2canvas
      version: ^1.4.1
      reason: Export preview as image for sharing/saving
    - name: react-window
      version: ^1.8.8
      reason: Virtualization for performance with long comics
    - name: use-debounce
      version: ^9.0.4
      reason: Debounce preview updates to prevent excessive API calls
    files_to_modify:
    - path: apps/dashboard/src/pages/comic/[id]/preview.tsx
      changes: Add preview page route with layout and navigation
    - path: apps/dashboard/src/components/comic/ComicEditor.tsx
      changes: Integrate preview component and parameter controls
    - path: apps/api/src/routes/comic/index.ts
      changes: Add preview endpoints to existing comic routes
    - path: packages/database/src/schema/comic.sql
      changes: Add preview_settings and preview_cache tables
    new_files:
    - path: apps/dashboard/src/components/comic/ComicPreview.tsx
      purpose: Main preview component with canvas rendering and controls
    - path: apps/dashboard/src/components/comic/PreviewCanvas.tsx
      purpose: Canvas-specific rendering logic using fabric.js
    - path: apps/dashboard/src/components/comic/PreviewControls.tsx
      purpose: Parameter controls for real-time preview updates
    - path: apps/dashboard/src/hooks/useComicPreview.ts
      purpose: Preview state management and API integration
    - path: apps/dashboard/src/hooks/useCanvasInteraction.ts
      purpose: Canvas interaction logic for zoom/pan/selection
    - path: packages/comic-engine/src/preview/PreviewGenerator.ts
      purpose: Lightweight preview generation pipeline
    - path: packages/comic-engine/src/preview/PanelLayoutEngine.ts
      purpose: Panel layout calculation for preview
    - path: packages/comic-engine/src/preview/AssetCache.ts
      purpose: Caching system for character designs and backgrounds
    - path: apps/api/src/routes/comic/preview.ts
      purpose: Preview generation and caching API endpoints
    - path: apps/api/src/services/PreviewService.ts
      purpose: Server-side preview generation business logic
    - path: packages/ui/src/components/Canvas/VirtualizedCanvas.tsx
      purpose: Virtualized canvas component for performance
    - path: packages/ui/src/components/Canvas/CanvasControls.tsx
      purpose: Reusable zoom/pan/export controls
    - path: apps/dashboard/src/utils/previewCache.ts
      purpose: Client-side preview caching utilities
  acceptance_criteria:
  - criterion: Comic preview renders with accurate panel layouts, character placements,
      and dialogue positioning matching user's novel chapter content
    verification: Load test chapter, verify preview shows correct number of panels
      with character consistency and dialogue placement via visual regression tests
  - criterion: Real-time preview updates within 500ms when users modify generation
      parameters (style, layout, character designs)
    verification: Performance test measuring time from parameter change to preview
      update using browser dev tools timeline
  - criterion: Preview system handles long novels (50+ chapters) without memory issues
      through virtualization
    verification: Load test with 50 chapter novel, monitor memory usage stays under
      500MB and scrolling remains smooth at 60fps
  - criterion: Mobile preview mode provides simplified but accurate representation
      with touch controls for zoom/pan
    verification: Manual testing on mobile devices (iOS Safari, Android Chrome) confirming
      touch gestures work and performance is acceptable
  - criterion: Preview-to-final generation parity maintains 95% visual consistency
    verification: A/B comparison test between preview and final generated comic pages
      using image similarity metrics
  testing:
    unit_tests:
    - file: apps/dashboard/src/components/comic/__tests__/ComicPreview.test.tsx
      coverage_target: 90%
      scenarios:
      - Renders empty state correctly
      - Loads preview data from API
      - Handles parameter updates
      - Canvas interaction events
      - Error states and loading states
    - file: apps/dashboard/src/hooks/__tests__/useComicPreview.test.ts
      coverage_target: 85%
      scenarios:
      - State management for preview data
      - Debounced API calls
      - WebSocket connection handling
      - Cache invalidation logic
    - file: packages/comic-engine/src/preview/__tests__/PreviewGenerator.test.ts
      coverage_target: 85%
      scenarios:
      - Panel layout generation
      - Character positioning
      - Dialogue bubble placement
      - Asset caching and retrieval
    integration_tests:
    - file: apps/dashboard/src/__tests__/integration/comic-preview-flow.test.tsx
      scenarios:
      - Complete preview generation flow from chapter upload to rendered preview
      - Real-time parameter updates through WebSocket
      - Preview export functionality
    - file: apps/api/src/__tests__/integration/preview-api.test.ts
      scenarios:
      - Preview generation API endpoints
      - Asset caching and retrieval
      - Concurrent preview requests
    e2e_tests:
    - file: apps/e2e/tests/comic-preview.spec.ts
      scenarios:
      - User uploads chapter and sees preview
      - User modifies parameters and sees real-time updates
      - User navigates through multi-chapter preview
      - Mobile responsive preview interaction
    manual_testing:
    - step: Upload a 10-chapter novel and generate preview
      expected: Preview loads within 3 seconds, all panels render correctly
    - step: Modify character design parameters
      expected: Preview updates in real-time, character consistency maintained
    - step: Test on mobile device with touch gestures
      expected: Zoom/pan works smoothly, simplified UI elements visible
    - step: Load preview with slow network connection
      expected: Progressive loading with appropriate loading states
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Set up database schema for preview caching and settings
      done: false
    - task: Implement server-side preview generation pipeline
      done: false
    - task: Create fabric.js-based canvas preview component
      done: false
    - task: Build real-time parameter update system with WebSockets
      done: false
    - task: Implement viewport virtualization for long comics
      done: false
    - task: Add mobile-optimized preview mode with touch controls
      done: false
    - task: Integrate preview with existing comic editor interface
      done: false
    - task: Implement caching system for preview assets
      done: false
    - task: Add export functionality (PNG/PDF) from preview
      done: false
    - task: Performance optimization and memory management
      done: false
    - task: Comprehensive testing across all components
      done: false
    - task: Documentation and API specification
      done: false
- key: T63
  title: Comic Layout & Composition
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 5
  area: comic
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nThis task handles the automated creation of\
      \ comic panel layouts and visual composition from novel text. It's the bridge\
      \ between narrative content and visual storytelling, determining how scenes\
      \ are arranged into panels, speech bubbles are positioned, and visual elements\
      \ are composed. This is critical for Morpheus as it transforms linear text into\
      \ the spatial, visual medium of comics. Without proper layout composition, generated\
      \ comic pages would lack professional polish and readability.\n\n**Technical\
      \ Approach:**\nImplement a rule-based layout engine with ML enhancement using\
      \ canvas-based rendering. Use Fabric.js or Konva.js for interactive canvas manipulation,\
      \ combined with layout algorithms that consider panel hierarchy, reading flow,\
      \ and visual balance. Integrate with the existing image generation pipeline\
      \ to compose final pages. Store layout templates and rules in PostgreSQL, with\
      \ real-time preview capabilities for the dashboard.\n\n**Dependencies:**\n-\
      \ External: fabric.js/konva.js (canvas manipulation), pdf-lib (PDF generation),\
      \ sharp (image processing), layout algorithms library\n- Internal: Image generation\
      \ service, scene analysis from text processing, character positioning system,\
      \ speech bubble generation\n\n**Risks:**\n- Performance degradation: Complex\
      \ layouts with many panels could slow rendering - mitigate with canvas virtualization\
      \ and progressive loading\n- Layout quality inconsistency: Automated layouts\
      \ may lack artistic coherence - implement template-based systems with manual\
      \ override capabilities  \n- Memory consumption: High-resolution comic pages\
      \ consume significant memory - implement streaming and chunked processing\n\
      - Cross-device compatibility: Canvas rendering differs across browsers - extensive\
      \ testing and fallback strategies needed\n\n**Complexity Notes:**\nThis is significantly\
      \ more complex than initially estimated. Comic layout involves sophisticated\
      \ spatial reasoning, aesthetic considerations, and integration with multiple\
      \ ML services. The combination of automated layout generation with user customization\
      \ options adds substantial complexity to the UI/UX layer.\n\n**Key Files:**\n\
      - apps/api/src/services/layout-engine.ts: Core layout algorithm implementation\n\
      - apps/dashboard/src/components/comic/LayoutEditor.tsx: Interactive layout editing\
      \ interface\n- apps/api/src/models/comic-layout.ts: Database schema for layout\
      \ data\n- packages/shared/src/types/layout.ts: Shared layout type definitions\n"
    design_decisions:
    - decision: Use rule-based layout engine with template system rather than pure
        ML generation
      rationale: Provides predictable, customizable results while maintaining artistic
        quality. ML layout generation is still experimental and hard to control.
      alternatives_considered:
      - Pure ML layout generation
      - Manual layout only
      - Grid-based rigid layouts
    - decision: Implement canvas-based editor with Fabric.js for real-time layout
        manipulation
      rationale: Provides professional-grade editing capabilities with good performance.
        Fabric.js has extensive comic/graphic design community support.
      alternatives_considered:
      - SVG-based editor
      - CSS Grid layouts
      - Custom WebGL solution
    researched_at: '2026-02-07T19:10:39.088291'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:35:57.083498'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a hybrid layout system combining algorithmic panel arrangement
      with interactive editing. The layout engine analyzes scene content to suggest
      optimal panel sizes and arrangements, then renders using Fabric.js canvas for
      real-time manipulation. Implement a template library with common comic layouts
      (splash pages, action sequences, dialogue scenes) that can be automatically
      selected based on content analysis. Store layout metadata in Supabase with versioning
      support for iterative editing.

      '
    external_dependencies:
    - name: fabric
      version: ^5.3.0
      reason: Canvas manipulation and interactive editing capabilities
    - name: pdf-lib
      version: ^1.17.1
      reason: High-quality PDF generation for final comic output
    - name: sharp
      version: ^0.33.0
      reason: Image processing and optimization for panel composition
    - name: '@types/fabric'
      version: ^5.3.0
      reason: TypeScript definitions for Fabric.js
    files_to_modify:
    - path: apps/api/src/routes/comics.ts
      changes: Add POST /:id/layout endpoint for layout generation and PUT /:id/layout
        for updates
    - path: apps/dashboard/src/pages/ComicEditor.tsx
      changes: Integrate LayoutEditor component, add layout tab to editor interface
    - path: packages/shared/src/types/comic.ts
      changes: Add ComicLayout interface and PanelConfiguration types
    - path: apps/api/src/services/comic-generation.ts
      changes: Integrate layout engine into comic generation pipeline
    new_files:
    - path: apps/api/src/services/layout-engine.ts
      purpose: Core layout algorithm implementation with template system
    - path: apps/api/src/services/canvas-renderer.ts
      purpose: Server-side canvas rendering for PDF generation using node-canvas
    - path: apps/api/src/models/comic-layout.ts
      purpose: Supabase schema and queries for layout persistence
    - path: apps/dashboard/src/components/comic/LayoutEditor.tsx
      purpose: Interactive Fabric.js-based layout editing interface
    - path: apps/dashboard/src/components/comic/PanelToolbar.tsx
      purpose: Panel manipulation tools (resize, delete, style)
    - path: apps/dashboard/src/components/comic/LayoutTemplates.tsx
      purpose: Template gallery and selection interface
    - path: apps/api/src/utils/layout-algorithms.ts
      purpose: Panel arrangement algorithms (grid, dynamic, artistic)
    - path: apps/api/src/utils/speech-bubble-positioning.ts
      purpose: Collision detection and optimal positioning for speech bubbles
    - path: packages/shared/src/types/layout.ts
      purpose: Shared TypeScript interfaces for layout data structures
    - path: apps/dashboard/src/hooks/useLayoutEditor.ts
      purpose: Custom hook for managing layout editor state and canvas operations
    - path: apps/api/src/services/pdf-export.ts
      purpose: PDF generation service integrating layouts with pdf-lib
  acceptance_criteria:
  - criterion: System automatically generates comic panel layouts from novel text
      with at least 3 different layout templates (splash, grid, dynamic)
    verification: POST /api/comics/{id}/generate-layout returns layout JSON with panels
      array, coordinates, and template type
  - criterion: Interactive layout editor allows drag-and-drop panel repositioning
      with real-time canvas updates
    verification: 'Manual test: drag panels in LayoutEditor component, verify position
      updates persist in database'
  - criterion: Layout engine handles speech bubble positioning without overlap and
      maintains reading flow (left-to-right, top-to-bottom)
    verification: Unit test verifies bubble collision detection returns false for
      generated layouts, manual test confirms reading order
  - criterion: System renders high-resolution comic pages (300 DPI) within 10 seconds
      for pages with up to 8 panels
    verification: Integration test measures rendering time with performance.now(),
      load test with various panel counts
  - criterion: Generated layouts are responsive and maintain aspect ratios across
      different output formats (web, print, mobile)
    verification: Visual regression tests compare layout outputs at 16:9, 4:3, and
      mobile aspect ratios
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/layout-engine.test.ts
      coverage_target: 90%
      scenarios:
      - Panel arrangement algorithms
      - Speech bubble collision detection
      - Template selection logic
      - Layout validation
      - Error handling for invalid scene data
    - file: apps/dashboard/src/__tests__/components/LayoutEditor.test.tsx
      coverage_target: 85%
      scenarios:
      - Canvas initialization
      - Panel drag and drop
      - Template switching
      - Save/load functionality
    integration_tests:
    - file: apps/api/src/__tests__/integration/comic-layout.test.ts
      scenarios:
      - Full layout generation from text to rendered page
      - Database persistence of layout changes
      - Integration with image generation service
      - PDF export with layouts
    - file: apps/dashboard/src/__tests__/integration/layout-editor.test.tsx
      scenarios:
      - Real-time collaboration on layout editing
      - Undo/redo functionality
      - Template library integration
    manual_testing:
    - step: Upload novel text and generate automatic layout
      expected: System creates 3-6 panels with appropriate sizes based on scene content
    - step: Drag panels to new positions in layout editor
      expected: Panels snap to grid, update coordinates, changes persist on page reload
    - step: Switch between layout templates for same content
      expected: Panel arrangement changes, content reflows appropriately
    - step: Export comic page as PDF at print resolution
      expected: PDF renders at 300 DPI with crisp text and properly positioned elements
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Set up Fabric.js and canvas infrastructure in dashboard
      done: false
    - task: Implement core layout algorithms and template system
      done: false
    - task: Create Supabase schema for layout persistence
      done: false
    - task: Build interactive LayoutEditor component with drag-drop
      done: false
    - task: Implement speech bubble collision detection and positioning
      done: false
    - task: Integrate layout engine with existing image generation pipeline
      done: false
    - task: Add server-side rendering for PDF export
      done: false
    - task: Create layout template library with 5+ templates
      done: false
    - task: Implement real-time collaboration features for layout editing
      done: false
    - task: Performance optimization and memory management
      done: false
    - task: Comprehensive testing and documentation
      done: false
- key: T64
  title: Panel Generation
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 5
  area: comic
  dependsOn:
  - T62
  - T63
  agent_notes:
    research_findings: "**Context:**\nPanel Generation is the core visual output of\
      \ the Morpheus platform - transforming structured narrative content (scenes,\
      \ dialogue, descriptions) into individual comic book panels with appropriate\
      \ layouts, compositions, and visual elements. This is essential for the M5 Product\
      \ Assembly milestone as it creates the final visual comic product from the processed\
      \ novel content. Without proper panel generation, users cannot see their transformed\
      \ comics, making this a critical p0 feature.\n\n**Technical Approach:**\n- Use\
      \ Canvas API or Fabric.js for client-side panel composition and layout\n- Implement\
      \ panel templates system with predefined layouts (single panel, 2-panel horizontal,\
      \ 3-panel grid, etc.)\n- Create panel sizing algorithms based on narrative importance\
      \ and pacing\n- Integrate with Stable Diffusion API (RunPod) for background/character\
      \ image generation\n- Use React-based panel editor component for real-time preview\n\
      - Implement server-side panel data persistence in Supabase\n- Create FastAPI\
      \ endpoints for panel CRUD operations and batch generation\n\n**Dependencies:**\n\
      - External: fabric.js, canvas, sharp (image processing), pdf-lib (export), react-dnd\
      \ (drag/drop)\n- Internal: Scene analysis service, Character tracking, Image\
      \ generation pipeline, Comic layout engine\n\n**Risks:**\n- Canvas performance:\
      \ Large comics with many panels could cause browser memory issues\n  Mitigation:\
      \ Implement virtual scrolling and lazy loading for panel rendering\n- Image\
      \ generation latency: Stable Diffusion calls may timeout for complex panels\
      \  \n  Mitigation: Implement async job queue with progress tracking and fallback\
      \ images\n- Layout complexity: Dynamic panel sizing could create inconsistent\
      \ visual flow\n  Mitigation: Create constraint-based layout system with predefined\
      \ templates\n- Cross-browser compatibility: Canvas rendering differs between\
      \ browsers\n  Mitigation: Use Fabric.js abstraction layer and extensive cross-browser\
      \ testing\n\n**Complexity Notes:**\nThis is significantly more complex than\
      \ initially estimated. Panel generation involves multiple AI services coordination,\
      \ complex visual layout algorithms, real-time rendering performance, and sophisticated\
      \ user interaction patterns. The visual quality requirements and performance\
      \ constraints make this a high-complexity feature requiring careful architecture\
      \ planning.\n\n**Key Files:**\n- apps/dashboard/src/components/PanelEditor.tsx:\
      \ Main panel editing interface\n- apps/dashboard/src/services/panelGeneration.ts:\
      \ Client-side panel logic  \n- apps/backend/src/services/PanelService.ts: Server-side\
      \ panel operations\n- packages/shared/src/types/Panel.ts: Panel data structures\n\
      - apps/backend/src/routes/panels.ts: Panel API endpoints\n- packages/shared/src/services/LayoutEngine.ts:\
      \ Panel layout algorithms\n"
    design_decisions:
    - decision: Use Fabric.js for canvas-based panel composition
      rationale: Provides high-level canvas abstraction with built-in drag/drop, object
        manipulation, and serialization capabilities essential for interactive panel
        editing
      alternatives_considered:
      - Raw Canvas API (too low-level)
      - Konva.js (React-specific but less mature)
      - SVG-based approach (performance limitations)
    - decision: Implement template-based panel layouts with constraint solver
      rationale: Balances creative flexibility with visual consistency, allows non-designers
        to create professional layouts while enabling customization
      alternatives_considered:
      - Fully manual layout (too complex for users)
      - Fixed grid system (too restrictive)
      - AI-generated layouts (too unpredictable)
    - decision: Use job queue for async panel generation with WebSocket progress updates
      rationale: Panel generation involves multiple AI API calls that can take 30+
        seconds, requiring non-blocking UX with real-time progress feedback
      alternatives_considered:
      - Synchronous generation (poor UX)
      - Polling-based progress (inefficient)
      - Client-side generation (limited by browser resources)
    researched_at: '2026-02-07T19:11:06.049505'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:36:25.577269'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a three-tier panel generation system: (1) Layout Engine calculates
      panel dimensions and positions based on narrative pacing and content analysis,
      (2) Asset Pipeline coordinates with RunPod Stable Diffusion to generate panel
      imagery with proper aspect ratios and compositions, (3) Composition Engine uses
      Fabric.js to combine generated images, speech bubbles, and effects into final
      panel canvases. The system supports both automated batch generation for full
      comics and interactive single-panel editing through a React-based visual editor
      with real-time preview capabilities.

      '
    external_dependencies:
    - name: fabric
      version: ^5.3.0
      reason: Canvas manipulation library for interactive panel editing and composition
    - name: sharp
      version: ^0.32.0
      reason: High-performance image processing for panel image optimization and format
        conversion
    - name: pdf-lib
      version: ^1.17.0
      reason: Generate PDF exports of completed comic panels for print/distribution
    - name: react-dnd
      version: ^16.0.1
      reason: Drag and drop functionality for panel reordering and element positioning
    - name: bull
      version: ^4.12.0
      reason: Job queue management for async panel generation workflows
    files_to_modify:
    - path: apps/dashboard/src/components/ComicEditor.tsx
      changes: Add PanelEditor integration and panel navigation
    - path: apps/backend/src/services/ImageGenerationService.ts
      changes: Add panel-specific image generation with aspect ratio constraints
    - path: packages/shared/src/types/Comic.ts
      changes: Add Panel[] property and panel relationship types
    new_files:
    - path: apps/dashboard/src/components/PanelEditor.tsx
      purpose: Main React component for interactive panel editing with Fabric.js canvas
    - path: apps/dashboard/src/services/panelGeneration.ts
      purpose: Client-side panel rendering, canvas management, and API integration
    - path: apps/backend/src/services/PanelService.ts
      purpose: Server-side panel CRUD, batch processing, and job queue management
    - path: packages/shared/src/types/Panel.ts
      purpose: TypeScript interfaces for panel data, layout templates, and canvas
        elements
    - path: apps/backend/src/routes/panels.ts
      purpose: REST endpoints for panel operations (/generate, /batch-generate, CRUD)
    - path: packages/shared/src/services/LayoutEngine.ts
      purpose: Panel layout algorithms, template definitions, and sizing calculations
    - path: apps/dashboard/src/components/PanelTemplateSelector.tsx
      purpose: UI component for selecting and previewing panel layout templates
    - path: apps/backend/src/jobs/PanelGenerationJob.ts
      purpose: Background job handler for batch panel generation with progress tracking
    - path: apps/dashboard/src/hooks/usePanelEditor.ts
      purpose: React hook for panel editor state management and canvas operations
    - path: packages/shared/src/utils/canvasUtils.ts
      purpose: Utility functions for canvas operations, image processing, and export
  acceptance_criteria:
  - criterion: Panel generation creates individual comic panels from scene data with
      proper layout, character placement, and speech bubbles
    verification: POST /api/panels/generate with scene data returns panel with canvas
      data, character positions, and dialogue elements
  - criterion: Interactive panel editor allows real-time editing of panel elements
      with drag-and-drop repositioning
    verification: Load PanelEditor component, drag character/bubble elements, verify
      position updates persist and render correctly
  - criterion: System supports multiple panel layout templates (1-panel, 2-panel horizontal,
      3-panel grid) with automatic sizing
    verification: Generate panels using different layout templates, verify dimensions
      follow template constraints and maintain aspect ratios
  - criterion: Batch panel generation processes full comic scenes with progress tracking
      and error handling
    verification: POST /api/panels/batch-generate with multiple scenes, verify job
      queue processing, progress updates, and fallback handling
  - criterion: Canvas rendering performance handles 20+ panels without memory issues
      using virtualization
    verification: Load comic with 25+ panels, verify smooth scrolling, memory usage
      <500MB, lazy loading of off-screen panels
  testing:
    unit_tests:
    - file: packages/shared/src/services/__tests__/LayoutEngine.test.ts
      coverage_target: 90%
      scenarios:
      - Panel dimension calculations for different templates
      - Constraint validation for panel positioning
      - Aspect ratio maintenance
      - Edge cases with invalid dimensions
    - file: apps/backend/src/services/__tests__/PanelService.test.ts
      coverage_target: 85%
      scenarios:
      - Panel CRUD operations
      - Batch generation job creation
      - Error handling for invalid scene data
      - Image generation API integration
    integration_tests:
    - file: apps/backend/src/__tests__/integration/panel-generation.test.ts
      scenarios:
      - Full panel generation pipeline from scene to canvas
      - RunPod Stable Diffusion integration with fallback
      - Panel data persistence in Supabase
    - file: apps/dashboard/src/__tests__/integration/PanelEditor.test.ts
      scenarios:
      - Panel editor component with fabric.js canvas
      - Real-time updates and persistence
      - Drag-and-drop functionality
    manual_testing:
    - step: Create new comic project, navigate to panel generation
      expected: Panel editor loads with empty canvas and template options
    - step: Select 3-panel grid template, generate panels from sample scene
      expected: Three panels appear with proper layout, generated backgrounds, character
        placement
    - step: Drag speech bubble to new position, verify save
      expected: Bubble moves smoothly, position persists on reload
    - step: Test batch generation with 10 scenes
      expected: Progress bar shows updates, all panels generated within 2 minutes
  estimates:
    development: 8
    code_review: 1.5
    testing: 2.5
    documentation: 1
    total: 13
  progress:
    status: not-started
    checklist:
    - task: Setup Fabric.js dependencies and Canvas API integration
      done: false
    - task: Implement LayoutEngine with panel templates and sizing algorithms
      done: false
    - task: Create Panel data models and TypeScript interfaces
      done: false
    - task: Build PanelService with CRUD operations and RunPod integration
      done: false
    - task: Develop PanelEditor React component with drag-and-drop functionality
      done: false
    - task: Implement batch generation with job queue and progress tracking
      done: false
    - task: Add panel virtualization for performance optimization
      done: false
    - task: Create comprehensive test suite with Canvas testing utilities
      done: false
    - task: Integrate panel editor into main comic creation workflow
      done: false
    - task: Performance testing and cross-browser compatibility validation
      done: false
- key: T65
  title: Comic Metadata
  type: Task
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 2
  area: comic
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Comic metadata is essential for organizing, searching, displaying, and managing
      comics in the Morpheus platform. This includes title, author, genre, creation
      date, page count, reading progress, tags, ratings, and technical details like
      image formats and resolutions. Proper metadata enables the storefront to show
      rich comic information, allows users to track reading progress, enables search/filtering
      functionality, and supports SEO optimization for public comics.


      **Technical Approach:**

      Implement a comprehensive metadata schema in PostgreSQL with proper indexing
      for search performance. Use Supabase''s real-time features for metadata updates.
      Create TypeScript interfaces for type safety across frontend/backend. Implement
      metadata extraction during comic generation pipeline, with manual override capabilities
      in the dashboard. Use structured data (JSON-LD) for SEO and social media sharing.
      Consider EPUB/CBZ metadata standards for potential export features.


      **Dependencies:**

      - External: zod (validation), sharp (image analysis), @types/mime-types, structured-data-testing-tool

      - Internal: Database schema migrations, comic generation pipeline, search service,
      user management system, file storage service


      **Risks:**

      - Schema evolution: Use flexible JSONB fields for extensible metadata alongside
      structured columns

      - Performance with large catalogs: Implement proper database indexing strategy
      and pagination

      - Metadata consistency: Validate metadata at API boundaries and during generation
      pipeline

      - Search complexity: Consider PostgreSQL full-text search vs external search
      service (Algolia/ElasticSearch)


      **Complexity Notes:**

      This is more complex than initially thought due to the need for both structured
      searchable fields and flexible extensible metadata. The integration with the
      comic generation pipeline adds complexity, as does the requirement for real-time
      updates across dashboard and storefront.


      **Key Files:**

      - packages/database/migrations/: Add comic_metadata table and indexes

      - packages/shared/types/comic.ts: Define ComicMetadata interfaces

      - apps/api/src/routes/comics/: Metadata CRUD endpoints

      - apps/dashboard/src/components/ComicEditor/: Metadata editing forms

      - apps/storefront/src/components/ComicCard/: Display metadata

      - packages/ml/src/comic-generator/: Integrate metadata creation

      '
    design_decisions:
    - decision: Hybrid metadata storage using both structured PostgreSQL columns and
        JSONB fields
      rationale: Structured columns for searchable/filterable fields (title, author,
        genre, created_at) with JSONB for extensible custom metadata and technical
        details
      alternatives_considered:
      - Pure JSONB storage
      - Separate metadata tables
      - External metadata service
    - decision: Generate metadata during comic creation pipeline with manual override
        capability
      rationale: Ensures all comics have consistent metadata while allowing user customization
        for published works
      alternatives_considered:
      - Manual-only metadata entry
      - Post-generation metadata extraction
      - AI-generated metadata only
    researched_at: '2026-02-07T19:11:27.903366'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:36:48.740302'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a comprehensive comic metadata system with a PostgreSQL schema
      supporting both structured searchable fields and flexible JSONB extensions.
      Integrate metadata generation into the comic creation pipeline while providing
      dashboard editing capabilities. Implement full-text search with proper indexing
      for performance. Use Supabase real-time subscriptions for live metadata updates
      across the platform and structured data markup for SEO optimization.

      '
    external_dependencies:
    - name: zod
      version: ^3.22.0
      reason: Runtime validation for metadata schemas and API payloads
    - name: sharp
      version: ^0.33.0
      reason: Extract technical metadata from generated comic images
    - name: '@types/mime-types'
      version: ^2.1.4
      reason: File type detection for comic assets
    - name: date-fns
      version: ^2.30.0
      reason: Consistent date formatting and manipulation for metadata
    files_to_modify:
    - path: packages/database/supabase/migrations/20240115000000_comic_metadata.sql
      changes: Create comic_metadata table, indexes, RLS policies, search view
    - path: packages/shared/types/comic.ts
      changes: Add ComicMetadata, ComicMetadataInput, SearchableMetadata interfaces
    - path: apps/api/src/routes/comics/index.ts
      changes: Add metadata CRUD endpoints, integrate search functionality
    - path: packages/ml/src/comic-generator/pipeline.ts
      changes: Add metadata extraction step using sharp for image analysis
    - path: apps/dashboard/src/components/ComicEditor/ComicEditor.tsx
      changes: Add metadata editing form with real-time validation
    - path: apps/storefront/src/components/ComicCard/ComicCard.tsx
      changes: Display rich metadata, add structured data markup
    new_files:
    - path: packages/shared/src/schemas/comic-metadata.ts
      purpose: Zod validation schemas for metadata
    - path: apps/api/src/services/metadata-service.ts
      purpose: Business logic for metadata operations and search
    - path: apps/api/src/services/metadata-extraction.ts
      purpose: Extract technical metadata from comic files
    - path: apps/api/src/utils/search-builder.ts
      purpose: Build PostgreSQL full-text search queries
    - path: apps/storefront/src/utils/structured-data.ts
      purpose: Generate JSON-LD structured data for SEO
    - path: packages/shared/src/hooks/useComicMetadata.ts
      purpose: React hook for metadata operations with real-time updates
  acceptance_criteria:
  - criterion: Comic metadata is automatically extracted and stored during generation
      with title, author, genre, page count, creation date, and technical details
    verification: Generate new comic via ML pipeline, verify metadata populated in
      database with SELECT * FROM comic_metadata WHERE comic_id = ?;
  - criterion: Dashboard comic editor allows manual metadata editing with real-time
      updates across platform
    verification: Edit comic metadata in dashboard, verify changes appear in storefront
      within 2 seconds without page refresh
  - criterion: Full-text search returns relevant comics within 500ms for catalogs
      up to 10,000 comics
    verification: Load test with 10k comics, measure search response time with 'EXPLAIN
      ANALYZE SELECT * FROM comics_search_view WHERE search_vector @@ plainto_tsquery(?);'
  - criterion: Comic cards display rich metadata including ratings, progress, and
      structured data for SEO
    verification: Check storefront comic cards show all metadata fields, validate
      JSON-LD with Google's Structured Data Testing Tool
  - criterion: API provides CRUD operations for metadata with proper validation and
      error handling
    verification: Test all API endpoints with invalid data, verify zod validation
      errors returned with 400 status codes
  testing:
    unit_tests:
    - file: packages/shared/src/__tests__/comic-metadata.test.ts
      coverage_target: 90%
      scenarios:
      - Metadata validation with zod schemas
      - Type safety for ComicMetadata interfaces
      - Default value generation
      - Invalid metadata handling
    - file: apps/api/src/routes/comics/__tests__/metadata.test.ts
      coverage_target: 85%
      scenarios:
      - CRUD operations success paths
      - Authorization checks
      - Database constraint violations
      - Search query building
    integration_tests:
    - file: apps/api/src/__tests__/integration/comic-metadata-flow.test.ts
      scenarios:
      - Comic generation to metadata storage pipeline
      - Real-time metadata updates via Supabase
      - Search indexing after metadata changes
      - File upload with metadata extraction
    manual_testing:
    - step: Create comic via ML pipeline, verify auto-generated metadata
      expected: All required fields populated, technical details extracted from images
    - step: Edit metadata in dashboard, check storefront updates
      expected: Changes appear immediately without refresh
    - step: Search comics by various metadata fields
      expected: Relevant results returned quickly with highlighted matches
    - step: Check SEO structured data on comic pages
      expected: Valid JSON-LD markup with comic metadata
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Design database schema with proper indexing strategy
      done: false
    - task: Create TypeScript interfaces and zod validation schemas
      done: false
    - task: Implement metadata service with CRUD operations
      done: false
    - task: Integrate metadata extraction into comic generation pipeline
      done: false
    - task: Build dashboard metadata editing interface
      done: false
    - task: Add metadata display to storefront comic cards
      done: false
    - task: Implement full-text search with PostgreSQL
      done: false
    - task: Add SEO structured data markup
      done: false
    - task: Set up real-time subscriptions for metadata updates
      done: false
    - task: Performance testing and optimization
      done: false
- key: T66
  title: PDF Generation
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 5
  area: comic
  dependsOn:
  - T64
  - T65
  agent_notes:
    research_findings: '**Context:**

      PDF Generation is critical for delivering the final comic product to users.
      After comics are assembled from generated panels, users need a high-quality,
      printable PDF format for reading offline, printing, or sharing. This is the
      final step in the novel-to-comic transformation pipeline and represents the
      primary deliverable users expect. Without PDF generation, Morpheus would only
      exist as a web viewer, limiting distribution and user experience.


      **Technical Approach:**

      Recommended using Puppeteer for server-side PDF generation, leveraging Chrome''s
      native PDF rendering capabilities. Create a dedicated comic viewer template
      optimized for PDF output with proper page breaks, margins, and print styles.
      Implement as a background job using a queue system to handle potentially long-running
      operations. Store generated PDFs in Supabase storage with metadata tracking.
      Use CSS Grid/Flexbox for precise panel positioning and @media print queries
      for PDF-specific styling.


      **Dependencies:**

      - External: puppeteer (PDF generation), bullmq (job queue), sharp (image optimization)

      - Internal: Comic assembly service, panel storage system, user authentication,
      file storage service


      **Risks:**

      - Memory usage: Puppeteer can consume significant RAM with large comics; implement
      pagination and memory monitoring

      - Generation time: Complex comics may take minutes to render; use job queues
      and progress tracking

      - Font licensing: Ensure comic fonts are licensed for PDF distribution; fallback
      to web-safe fonts

      - File size: High-res images can create massive PDFs; implement compression
      and optimization


      **Complexity Notes:**

      More complex than initially estimated due to print-specific layout challenges,
      memory management requirements, and the need for robust error handling. Comic
      panel positioning must translate perfectly from web to print dimensions, requiring
      careful CSS media queries and possibly different layout algorithms.


      **Key Files:**

      - apps/api/src/services/pdf-generator.ts: Core PDF generation service

      - apps/api/src/routes/comics/pdf.ts: PDF generation endpoints

      - apps/web/src/components/comic/PDFTemplate.tsx: Print-optimized comic layout

      - packages/shared/src/types/pdf.ts: PDF generation types

      - apps/api/src/jobs/pdf-generation.ts: Background job processing

      '
    design_decisions:
    - decision: Use Puppeteer over jsPDF/PDFKit
      rationale: Puppeteer leverages Chrome's mature rendering engine, handles complex
        CSS layouts better, and provides consistent output quality. Easier to maintain
        print styles alongside web styles.
      alternatives_considered:
      - jsPDF (limited layout capabilities)
      - PDFKit (requires manual positioning)
      - '@react-pdf/renderer (React-specific but limited styling)'
    - decision: Implement as background job with queue system
      rationale: PDF generation for multi-page comics can take 30+ seconds, requiring
        async processing to avoid request timeouts. Enables progress tracking and
        retry mechanisms.
      alternatives_considered:
      - Synchronous generation (timeout risk)
      - Client-side generation (performance issues)
    - decision: Create dedicated PDF template component
      rationale: Print layouts have different requirements than web viewing - page
        breaks, margins, print-safe colors, and DPI considerations require specialized
        styling.
      alternatives_considered:
      - Reuse existing comic viewer (suboptimal print output)
      - Pure HTML templates (harder to maintain)
    researched_at: '2026-02-07T19:11:51.845629'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:37:13.945148'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a PDF generation service using Puppeteer that renders a specialized
      comic template optimized for print. Implement job queue processing for async
      generation with progress tracking. Design print-specific CSS layouts that handle
      page breaks intelligently between comic panels. Store generated PDFs in Supabase
      storage with download links returned to users. Include compression and optimization
      to balance quality with file size.

      '
    external_dependencies:
    - name: puppeteer
      version: ^21.5.0
      reason: Server-side PDF generation using Chrome's rendering engine
    - name: bullmq
      version: ^4.15.0
      reason: Job queue for async PDF generation processing
    - name: sharp
      version: ^0.32.6
      reason: Image optimization and compression for PDF assets
    - name: '@types/puppeteer'
      version: ^5.4.7
      reason: TypeScript definitions for Puppeteer
    files_to_modify:
    - path: apps/api/src/routes/comics/index.ts
      changes: Add PDF generation endpoint route registration
    - path: apps/web/src/pages/comic/[id].tsx
      changes: Add PDF download button and progress tracking UI
    - path: packages/shared/src/types/comic.ts
      changes: Add PDF generation status and metadata types
    new_files:
    - path: apps/api/src/services/pdf-generator.ts
      purpose: Core PDF generation service using Puppeteer, handles comic rendering
        and file optimization
    - path: apps/api/src/routes/comics/pdf.ts
      purpose: PDF generation API endpoints for starting jobs and checking status
    - path: apps/web/src/components/comic/PDFTemplate.tsx
      purpose: Print-optimized comic layout component with CSS Grid and print media
        queries
    - path: packages/shared/src/types/pdf.ts
      purpose: PDF generation request/response types and job status enums
    - path: apps/api/src/jobs/pdf-generation.ts
      purpose: Background job processor for handling PDF generation queue
    - path: apps/api/src/middleware/pdf-limits.ts
      purpose: Rate limiting and resource monitoring for PDF generation requests
    - path: apps/web/src/hooks/usePDFGeneration.ts
      purpose: React hook for managing PDF generation state and progress polling
    - path: apps/web/src/components/comic/PDFDownloadButton.tsx
      purpose: UI component with progress tracking and download functionality
  acceptance_criteria:
  - criterion: Users can generate PDF downloads of complete comics with all panels
      properly positioned
    verification: Navigate to comic viewer, click 'Download PDF' button, verify PDF
      downloads with correct panel layout and readable text
  - criterion: PDF generation handles comics of varying lengths (1-50+ pages) without
      memory crashes
    verification: Test PDF generation with comics containing 5, 20, and 50+ panels,
      monitor server memory usage stays below 2GB
  - criterion: Generated PDFs maintain print quality with file sizes under 50MB for
      typical comics
    verification: Generate PDFs for 3 sample comics, verify file sizes <50MB and images
      remain crisp when printed at 300dpi
  - criterion: PDF generation provides real-time progress updates and handles failures
      gracefully
    verification: Start PDF generation, verify progress updates appear in UI, simulate
      server restart during generation and verify proper error handling
  - criterion: System queues multiple PDF requests and processes them without blocking
      other operations
    verification: Submit 3 PDF generation requests simultaneously, verify all complete
      successfully and API remains responsive
  testing:
    unit_tests:
    - file: apps/api/src/services/__tests__/pdf-generator.test.ts
      coverage_target: 90%
      scenarios:
      - Successful PDF generation with mock comic data
      - Memory limit exceeded error handling
      - Invalid comic ID error handling
      - Font fallback when custom fonts unavailable
      - Image compression and optimization
    - file: apps/api/src/jobs/__tests__/pdf-generation.test.ts
      coverage_target: 85%
      scenarios:
      - Job queue processing
      - Progress tracking updates
      - Job failure and retry logic
    integration_tests:
    - file: apps/api/src/__tests__/integration/pdf-workflow.test.ts
      scenarios:
      - End-to-end PDF generation from API request to file storage
      - Queue system integration with multiple concurrent jobs
      - Supabase storage upload and retrieval
    manual_testing:
    - step: Generate PDF for 10-panel comic and verify layout
      expected: PDF downloads with panels properly positioned, page breaks between
        logical sections
    - step: Test PDF generation progress tracking in browser
      expected: Progress bar updates from 0-100% with status messages
    - step: Print generated PDF and verify quality
      expected: Text remains readable, images are crisp, margins are appropriate for
        binding
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Setup Puppeteer, BullMQ dependencies and configure PDF generation queue
      done: false
    - task: Implement core PDF generator service with memory management and optimization
      done: false
    - task: Create print-optimized comic template component with CSS media queries
      done: false
    - task: Build PDF generation API endpoints with job creation and status checking
      done: false
    - task: Implement background job processing with progress tracking and error handling
      done: false
    - task: Add PDF download UI components with progress indicators to comic viewer
      done: false
    - task: Configure Supabase storage integration for PDF file management
      done: false
    - task: Implement rate limiting and resource monitoring middleware
      done: false
    - task: Write comprehensive test suite covering unit, integration, and performance
        scenarios
      done: false
    - task: Create API documentation and user guide for PDF generation feature
      done: false
- key: T67
  title: Download Manager
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 3
  area: comic
  dependsOn:
  - T66
  agent_notes:
    research_findings: '**Context:**

      The Download Manager is essential for the comic generation workflow, allowing
      users to download their completed comics in various formats (PDF, CBZ, individual
      images). This is a critical user-facing feature that completes the novel-to-comic
      transformation journey. Users need reliable, resumable downloads with progress
      tracking, especially for multi-page comics that can be large files. This also
      supports different use cases - casual readers wanting PDFs, comic enthusiasts
      preferring CBZ format, and creators needing individual page assets.


      **Technical Approach:**

      Implement a robust download system using streaming responses with Fastify''s
      built-in streaming capabilities. Use a queue-based approach with BullMQ for
      handling download preparation (comic assembly, format conversion). Store download
      metadata in Supabase with status tracking. Implement client-side download progress
      with fetch API and ReadableStream. For file generation, use libraries like PDFKit
      for PDF creation and JSZip for CBZ archives. Implement presigned URLs for direct
      S3 downloads when files are pre-generated.


      **Dependencies:**

      - External: BullMQ, PDFKit, JSZip, file-type, archiver, sharp (image processing)

      - Internal: Comic storage service, user authentication, file upload/storage
      infrastructure, comic generation pipeline


      **Risks:**

      - Memory issues with large files: Use streaming and temporary file cleanup

      - Concurrent download limits: Implement rate limiting and queue management

      - File corruption during generation: Add checksums and retry mechanisms

      - Storage costs for temporary files: Implement TTL cleanup policies

      - Browser download failures: Add resumable download support


      **Complexity Notes:**

      Initially seems straightforward but complexity increases with format variety,
      large file handling, and user experience requirements. The streaming implementation
      and queue management add significant architectural complexity. Integration with
      existing comic storage and the need for format conversion make this a medium-high
      complexity task.


      **Key Files:**

      - packages/api/src/routes/downloads.ts: Download endpoint handlers

      - packages/api/src/services/download-manager.ts: Core download logic

      - packages/api/src/services/comic-assembler.ts: Comic format generation

      - packages/dashboard/src/components/DownloadManager.tsx: UI component

      - packages/shared/src/types/download.ts: Type definitions

      '
    design_decisions:
    - decision: Use streaming responses with queue-based file preparation
      rationale: Handles large files efficiently, provides progress tracking, and
        scales with multiple concurrent downloads
      alternatives_considered:
      - Direct file serving
      - Pre-generated files only
      - Client-side assembly
    - decision: Support PDF, CBZ, and ZIP formats
      rationale: Covers main use cases - PDF for reading, CBZ for comic apps, ZIP
        for individual assets
      alternatives_considered:
      - PDF only
      - Custom format
      - EPUB support
    - decision: Temporary file generation with TTL cleanup
      rationale: Balances performance (no regeneration) with storage costs and allows
        customization per download
      alternatives_considered:
      - Always stream generate
      - Permanent file caching
      - Client-side generation
    researched_at: '2026-02-07T19:12:15.271383'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:37:38.996007'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a two-phase download system: preparation phase using BullMQ
      queues to generate requested formats, and delivery phase using Fastify streaming
      responses. Store download jobs in Supabase with status tracking, generate files
      using format-specific libraries (PDFKit, JSZip), and stream directly to clients
      with progress tracking. Use presigned S3 URLs for large files and implement
      cleanup workers for temporary storage management.

      '
    external_dependencies:
    - name: bullmq
      version: ^5.0.0
      reason: Reliable job queue for download preparation tasks
    - name: pdfkit
      version: ^0.14.0
      reason: PDF generation from comic pages
    - name: jszip
      version: ^3.10.1
      reason: CBZ archive creation and ZIP file generation
    - name: archiver
      version: ^6.0.1
      reason: Streaming archive creation for large comics
    - name: file-type
      version: ^19.0.0
      reason: MIME type detection for proper Content-Type headers
    - name: sharp
      version: ^0.33.0
      reason: Image optimization and format conversion before packaging
    files_to_modify:
    - path: packages/api/src/server.ts
      changes: Register BullMQ queues and download routes
    - path: packages/api/src/middleware/auth.ts
      changes: Add download authorization checks
    - path: packages/dashboard/src/pages/ComicViewer.tsx
      changes: Integrate DownloadManager component
    - path: packages/shared/src/types/api.ts
      changes: Add download-related API types
    new_files:
    - path: packages/api/src/routes/downloads.ts
      purpose: Download endpoint handlers and streaming responses
    - path: packages/api/src/services/download-manager.ts
      purpose: Core download orchestration, job management, and queue integration
    - path: packages/api/src/services/comic-assembler.ts
      purpose: Format-specific file generation (PDF, CBZ, ZIP)
    - path: packages/api/src/workers/download-worker.ts
      purpose: BullMQ worker for processing download jobs
    - path: packages/api/src/workers/cleanup-worker.ts
      purpose: Scheduled cleanup of temporary download files
    - path: packages/dashboard/src/components/DownloadManager.tsx
      purpose: Download UI with progress tracking and format selection
    - path: packages/dashboard/src/hooks/useDownload.ts
      purpose: Download state management and progress tracking hook
    - path: packages/shared/src/types/download.ts
      purpose: Download job types, status enums, and format specifications
    - path: packages/api/src/config/download.ts
      purpose: Download configuration (limits, timeouts, file paths)
  acceptance_criteria:
  - criterion: Users can download comics in PDF, CBZ, and ZIP (individual images)
      formats with progress tracking
    verification: 'Manual testing: Generate comic, select format, verify download
      completes with progress bar and correct file format'
  - criterion: Download system handles files up to 100MB with streaming and memory
      efficiency under 512MB RAM usage
    verification: 'Integration test: Download large comic (50+ pages), monitor memory
      usage with process.memoryUsage() assertions'
  - criterion: Failed downloads can be resumed and retried, with queue status visible
      to users
    verification: 'Manual testing: Interrupt download mid-process, verify resume functionality
      and status updates in UI'
  - criterion: Download jobs are properly queued and processed with BullMQ, handling
      up to 10 concurrent downloads per user
    verification: 'Integration test: Queue multiple downloads, verify rate limiting
      and proper job processing order'
  - criterion: Temporary files are cleaned up within 24 hours and download URLs expire
      after 1 hour
    verification: 'Unit test: Verify TTL policies and cleanup job execution, check
      S3 presigned URL expiration'
  testing:
    unit_tests:
    - file: packages/api/src/services/__tests__/download-manager.test.ts
      coverage_target: 90%
      scenarios:
      - Download job creation and status tracking
      - Format validation and error handling
      - Memory limit enforcement
      - File cleanup scheduling
    - file: packages/api/src/services/__tests__/comic-assembler.test.ts
      coverage_target: 85%
      scenarios:
      - PDF generation with metadata
      - CBZ archive creation
      - Image optimization and compression
      - Invalid comic data handling
    integration_tests:
    - file: packages/api/src/__tests__/integration/downloads.test.ts
      scenarios:
      - Complete download workflow from request to delivery
      - Queue processing with BullMQ integration
      - S3 presigned URL generation and access
      - Rate limiting and concurrent download management
    - file: packages/dashboard/src/components/__tests__/DownloadManager.test.tsx
      scenarios:
      - Progress tracking and status updates
      - Error state handling and retry UI
      - Format selection and download triggering
    manual_testing:
    - step: Generate a 20-page comic and download as PDF
      expected: PDF downloads with proper page order, metadata, and readable quality
    - step: Download same comic as CBZ format
      expected: CBZ file opens in comic readers with correct page sequence
    - step: Test download interruption and resume
      expected: Download can be resumed from interruption point
    - step: Test multiple concurrent downloads
      expected: Downloads queue properly, UI shows accurate status for each
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup BullMQ configuration and Redis connection for download queues
      done: false
    - task: Implement download job types and Supabase schema for job tracking
      done: false
    - task: Create comic-assembler service with PDF, CBZ, and ZIP generation
      done: false
    - task: Build download-manager service with queue integration and streaming
      done: false
    - task: Implement download API routes with authentication and rate limiting
      done: false
    - task: Create BullMQ workers for job processing and file cleanup
      done: false
    - task: Build DownloadManager React component with progress tracking
      done: false
    - task: Implement client-side download hook with resume capability
      done: false
    - task: Add comprehensive error handling and retry mechanisms
      done: false
    - task: Write unit and integration tests, conduct manual testing
      done: false
- key: T68
  title: Archive Creation
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 2
  area: comic
  dependsOn:
  - T64
  agent_notes:
    research_findings: "**Context:**\nArchive Creation enables users to package completed\
      \ comics into distributable formats (PDF, EPUB, CBZ/CBR) for offline reading,\
      \ sharing, or selling. This is essential for the product assembly milestone\
      \ as it transforms individual comic pages into consumable final products. Users\
      \ need archives to distribute their comics on platforms, share with readers,\
      \ or create physical prints.\n\n**Technical Approach:**\n- Use PDFKit for PDF\
      \ generation with proper comic layout and metadata\n- Implement JSZip for CBZ\
      \ (Comic Book ZIP) format creation\n- Leverage Sharp for image optimization\
      \ and format standardization before archiving\n- Create streaming archive generation\
      \ to handle large comics without memory issues\n- Implement background job processing\
      \ with BullMQ for async archive creation\n- Store archives temporarily in Supabase\
      \ Storage with signed URLs for download\n- Add progress tracking for long-running\
      \ archive operations\n\n**Dependencies:**\n- External: pdfkit, jszip, sharp,\
      \ bullmq, archiver, epub-gen\n- Internal: comic service, page service, storage\
      \ service, job queue system, notification service\n\n**Risks:**\n- Memory exhaustion:\
      \ Large comics could overwhelm server memory during archive creation\n  Mitigation:\
      \ Stream processing, chunk-based operations, memory monitoring\n- Storage costs:\
      \ Archives consume significant storage space\n  Mitigation: Temporary storage\
      \ with TTL, compression optimization, cleanup jobs\n- Generation timeouts: Complex\
      \ comics may exceed request timeouts\n  Mitigation: Background processing with\
      \ progress updates via WebSocket/SSE\n- Format compatibility: Different readers\
      \ support different features\n  Mitigation: Format validation, fallback options,\
      \ comprehensive testing\n\n**Complexity Notes:**\nMore complex than initially\
      \ estimated due to multiple output formats, memory management requirements,\
      \ and need for background processing. The streaming and optimization aspects\
      \ add significant technical complexity.\n\n**Key Files:**\n- apps/api/src/services/archive.service.ts:\
      \ Core archive generation logic\n- apps/api/src/jobs/archive.job.ts: Background\
      \ job handler\n- apps/api/src/routes/archive/: API endpoints for archive operations\n\
      - apps/dashboard/src/components/ArchiveGenerator.tsx: UI for archive creation\n\
      - packages/shared/src/types/archive.ts: Archive type definitions\n"
    design_decisions:
    - decision: Use background job processing for archive generation
      rationale: Archive creation is resource-intensive and time-consuming, requiring
        async processing to avoid request timeouts and improve user experience
      alternatives_considered:
      - Synchronous generation
      - Client-side processing
      - External service integration
    - decision: Support multiple archive formats (PDF, CBZ, EPUB)
      rationale: Different use cases require different formats - PDF for printing,
        CBZ for comic readers, EPUB for broader e-reader compatibility
      alternatives_considered:
      - PDF only
      - Single universal format
      - User-selectable single format
    - decision: Implement streaming archive generation
      rationale: Prevents memory exhaustion with large comics and enables real-time
        progress tracking
      alternatives_considered:
      - In-memory generation
      - Disk-based temporary files
      - Chunked processing
    researched_at: '2026-02-07T19:12:37.885723'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:38:07.731556'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create an archive service that generates multiple comic formats using
      streaming operations and background jobs. Implement format-specific generators
      (PDF, CBZ, EPUB) that process comic pages in chunks to manage memory usage.
      Use BullMQ for job queuing with progress tracking, store temporary archives
      in Supabase Storage, and provide real-time status updates through WebSocket
      connections. Include comprehensive validation and cleanup mechanisms.

      '
    external_dependencies:
    - name: pdfkit
      version: ^0.14.0
      reason: PDF generation with comic-specific layouts and metadata support
    - name: jszip
      version: ^3.10.1
      reason: CBZ format creation (ZIP archives containing images)
    - name: epub-gen
      version: ^0.1.0
      reason: EPUB format generation for e-reader compatibility
    - name: archiver
      version: ^6.0.1
      reason: Streaming archive creation with compression options
    - name: bullmq
      version: ^4.15.0
      reason: Background job processing with progress tracking and retry logic
    - name: sharp
      version: ^0.33.0
      reason: Image optimization and format standardization before archiving
    files_to_modify:
    - path: apps/api/src/services/storage.service.ts
      changes: Add TTL support for temporary file storage, cleanup job scheduling
    - path: apps/api/src/services/comic.service.ts
      changes: Add getComicPagesForArchive method with streaming support
    - path: apps/api/src/config/queue.ts
      changes: Configure archive job queue with BullMQ settings
    - path: apps/dashboard/src/stores/comic.store.ts
      changes: Add archive generation state management
    new_files:
    - path: apps/api/src/services/archive.service.ts
      purpose: Core archive generation logic with format-specific handlers
    - path: apps/api/src/jobs/archive.job.ts
      purpose: Background job processor for archive generation with progress tracking
    - path: apps/api/src/routes/archive/create.ts
      purpose: API endpoint to initiate archive generation
    - path: apps/api/src/routes/archive/status.ts
      purpose: API endpoint to check archive generation status
    - path: apps/api/src/routes/archive/download.ts
      purpose: API endpoint to retrieve generated archive
    - path: apps/api/src/lib/generators/pdf-generator.ts
      purpose: PDF-specific archive generation using PDFKit
    - path: apps/api/src/lib/generators/cbz-generator.ts
      purpose: CBZ format generation using JSZip
    - path: apps/api/src/lib/generators/epub-generator.ts
      purpose: EPUB format generation with comic layout
    - path: apps/api/src/middleware/archive-validation.ts
      purpose: Request validation for archive generation endpoints
    - path: apps/dashboard/src/components/ArchiveGenerator.tsx
      purpose: UI component for archive creation with progress tracking
    - path: apps/dashboard/src/components/ArchiveProgress.tsx
      purpose: Progress indicator component with WebSocket integration
    - path: apps/dashboard/src/hooks/useArchiveGeneration.ts
      purpose: React hook for archive generation state and WebSocket handling
    - path: packages/shared/src/types/archive.ts
      purpose: TypeScript definitions for archive-related data structures
    - path: apps/api/src/jobs/archive-cleanup.job.ts
      purpose: Scheduled job to clean up expired archive files
  acceptance_criteria:
  - criterion: Users can generate PDF, CBZ, and EPUB archives from completed comics
      with proper metadata and page ordering
    verification: Upload a multi-page comic, trigger archive generation for each format,
      verify downloadable files contain all pages in correct order with comic title/author
      metadata
  - criterion: Archive generation processes large comics (50+ pages) without memory
      exhaustion or timeouts
    verification: Create a 100-page comic, monitor server memory usage during archive
      generation, verify process completes within 5 minutes with <500MB peak memory
  - criterion: Background job system provides real-time progress updates during archive
      creation
    verification: Start archive generation, verify WebSocket events show progress
      percentage updates every 10% completion, final success/error notification received
  - criterion: Generated archives are temporarily stored with automatic cleanup after
      24 hours
    verification: Generate archive, verify signed download URL works immediately,
      check Supabase Storage shows TTL metadata, confirm file auto-deleted after 24
      hours
  - criterion: Archive generation handles errors gracefully with proper user feedback
    verification: Test with corrupted images, network failures, and invalid comic
      data - verify error messages displayed in UI and job marked as failed
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/archive.service.test.ts
      coverage_target: 90%
      scenarios:
      - PDF generation with valid comic data
      - CBZ creation with image optimization
      - EPUB generation with metadata
      - Error handling for missing images
      - Memory stream processing
      - Format validation
    - file: apps/api/src/__tests__/jobs/archive.job.test.ts
      coverage_target: 85%
      scenarios:
      - Job progress tracking
      - Job failure handling
      - Cleanup on completion
      - Memory management
    integration_tests:
    - file: apps/api/src/__tests__/integration/archive.integration.test.ts
      scenarios:
      - End-to-end archive generation flow
      - Storage service integration
      - Job queue processing
      - WebSocket progress updates
      - File cleanup scheduling
    - file: apps/dashboard/src/__tests__/components/ArchiveGenerator.test.tsx
      scenarios:
      - UI state management during generation
      - Progress bar updates
      - Download link handling
      - Error message display
    manual_testing:
    - step: Create comic with 20 pages, generate PDF archive
      expected: PDF downloads with all pages, proper page order, embedded metadata
    - step: Generate CBZ archive, open in comic reader app
      expected: Archive opens correctly, images display in sequence
    - step: Monitor archive generation of large comic via WebSocket
      expected: Progress updates received, completion notification shows download
        link
    - step: Wait 25 hours after archive generation
      expected: Download link returns 404, file removed from storage
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Setup external dependencies (PDFKit, JSZip, Sharp, BullMQ, epub-gen)
      done: false
    - task: Create archive service with streaming architecture and format generators
      done: false
    - task: Implement BullMQ job processing with progress tracking and memory management
      done: false
    - task: Build API endpoints for archive creation, status checking, and download
      done: false
    - task: Add temporary storage with TTL and cleanup jobs to Supabase integration
      done: false
    - task: Create React components with WebSocket progress updates and download handling
      done: false
    - task: Implement comprehensive error handling and validation across all layers
      done: false
    - task: Add unit and integration tests with memory usage monitoring
      done: false
    - task: Performance testing with large comics and concurrent archive generation
      done: false
    - task: Documentation for API endpoints, job configuration, and deployment considerations
      done: false
- key: T69
  title: Print-Ready Output
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 3
  area: comic
  dependsOn:
  - T66
  agent_notes:
    research_findings: '**Context:**

      Print-ready output enables users to physically print their generated comics
      at professional quality. This is crucial for M5 - Product Assembly as it transforms
      digital comics into tangible products that can be sold, distributed, or personally
      enjoyed. The feature must generate high-resolution PDFs optimized for commercial
      printing with proper bleed areas, color profiles (CMYK), and standardized comic
      book dimensions.


      **Technical Approach:**

      Implement a server-side PDF generation service using PDFKit or Puppeteer for
      precise layout control. Create print templates with industry-standard comic
      dimensions (6.625" x 10.25" for standard US comics), 300 DPI resolution, and
      0.125" bleed areas. Integrate with the existing comic assembly pipeline to access
      high-resolution panel images and text overlays. Use sharp for image processing
      to ensure proper resolution scaling and color space conversion.


      **Dependencies:**

      - External: @react-pdf/renderer, puppeteer, sharp, pdf-lib, canvas

      - Internal: comic assembly service, panel storage service, user dashboard, file
      storage integration


      **Risks:**

      - Memory usage: Large high-res images could cause OOM errors - implement streaming
      and chunked processing

      - Color accuracy: RGB to CMYK conversion may alter colors - provide preview
      and color profile options

      - File size: Print-ready PDFs will be large - implement compression and CDN
      storage

      - Processing time: High-res rendering is slow - use background jobs with progress
      tracking


      **Complexity Notes:**

      More complex than initially estimated due to print industry requirements. Proper
      bleed handling, color management, and font embedding add significant technical
      overhead beyond basic PDF generation.


      **Key Files:**

      - apps/api/src/services/print-service.ts: Core PDF generation logic

      - apps/api/src/routes/comics/print.ts: API endpoint for print requests

      - apps/dashboard/src/components/PrintDialog.tsx: UI for print options

      - packages/shared/src/types/print.ts: Print configuration types

      '
    design_decisions:
    - decision: Use Puppeteer for PDF generation instead of pure canvas-based approach
      rationale: Puppeteer provides better text rendering, CSS layout capabilities,
        and easier maintenance than low-level canvas operations
      alternatives_considered:
      - PDFKit with manual layout
      - '@react-pdf/renderer'
      - Canvas-based generation
    - decision: Generate PDFs server-side with background job processing
      rationale: Print-ready files require high memory/CPU and long processing times
        unsuitable for synchronous API calls
      alternatives_considered:
      - Client-side generation
      - Synchronous API processing
    - decision: Support multiple print formats (standard comic, manga, custom)
      rationale: Different markets and use cases require different dimensions and
        layouts for optimal printing
      alternatives_considered:
      - Single standard format
      - Fully custom dimensions only
    researched_at: '2026-02-07T19:12:59.429882'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:38:33.620627'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a background job service that accepts comic IDs and print configuration
      parameters. The service will fetch high-resolution panel images, apply print-specific
      layouts with bleed areas, and generate PDFs using Puppeteer with custom CSS
      templates. Implement a queue system for processing multiple print requests and
      provide real-time status updates to users via WebSocket connections.

      '
    external_dependencies:
    - name: puppeteer
      version: ^21.0.0
      reason: High-quality PDF generation with CSS layout support
    - name: sharp
      version: ^0.32.0
      reason: Image processing for resolution scaling and color space conversion
    - name: pdf-lib
      version: ^1.17.0
      reason: PDF manipulation for adding metadata and optimizing output
    - name: bull
      version: ^4.11.0
      reason: Background job queue for processing print requests
    - name: ws
      version: ^8.14.0
      reason: WebSocket support for real-time print job status updates
    files_to_modify:
    - path: apps/api/src/services/comic-assembly.ts
      changes: Add high-resolution image retrieval methods for print output
    - path: apps/api/src/config/database.ts
      changes: Add print job status tracking tables and indexes
    - path: apps/dashboard/src/pages/comics/[id].tsx
      changes: Integrate print dialog component and WebSocket status updates
    new_files:
    - path: apps/api/src/services/print-service.ts
      purpose: Core PDF generation service with Puppeteer and sharp integration
    - path: apps/api/src/jobs/print-job.ts
      purpose: Background job processor for handling print requests in queue
    - path: apps/api/src/routes/comics/print.ts
      purpose: API endpoints for print requests, status checks, and file downloads
    - path: apps/api/src/websocket/print-status.ts
      purpose: WebSocket handler for real-time print job progress updates
    - path: apps/dashboard/src/components/PrintDialog.tsx
      purpose: User interface for print configuration and job monitoring
    - path: packages/shared/src/types/print.ts
      purpose: TypeScript definitions for print configurations and job status
    - path: apps/api/src/templates/print-layout.html
      purpose: HTML template for PDF generation with CSS print styles
    - path: apps/api/src/utils/color-conversion.ts
      purpose: RGB to CMYK color profile conversion utilities
    - path: apps/api/src/middleware/print-validation.ts
      purpose: Request validation for print parameters and file size limits
  acceptance_criteria:
  - criterion: System generates print-ready PDFs with 300 DPI resolution, CMYK color
      profile, and 0.125 inch bleed areas for standard US comic dimensions (6.625x10.25
      inches)
    verification: PDF metadata validation and visual inspection of generated files
      using preflight tools
  - criterion: Background job processing handles print requests with queue management
      and real-time progress updates via WebSocket
    verification: Submit multiple print requests simultaneously and verify status
      updates in dashboard UI
  - criterion: Print dialog allows users to select paper size, color profile, and
      download options with preview functionality
    verification: Manual testing of all UI options and preview generation in apps/dashboard
      print dialog
  - criterion: Memory usage remains under 2GB per print job and processes complete
      within 5 minutes for standard 20-page comics
    verification: Performance monitoring during load testing with comics of varying
      page counts
  - criterion: Generated PDFs are compatible with commercial printing services and
      pass industry-standard preflight checks
    verification: Test PDFs with commercial printing service validation tools and
      color accuracy checks
  testing:
    unit_tests:
    - file: apps/api/src/services/__tests__/print-service.test.ts
      coverage_target: 90%
      scenarios:
      - PDF generation with various comic layouts
      - Color profile conversion RGB to CMYK
      - Bleed area calculation and application
      - Memory management and cleanup
      - Error handling for corrupted images
    - file: apps/api/src/jobs/__tests__/print-job.test.ts
      coverage_target: 85%
      scenarios:
      - Job queue processing
      - Progress tracking updates
      - Job failure recovery
      - Concurrent job handling
    integration_tests:
    - file: apps/api/src/__tests__/integration/print-workflow.test.ts
      scenarios:
      - Complete print request flow from API to PDF generation
      - WebSocket status updates during processing
      - File storage and CDN upload integration
      - Comic assembly service integration
    - file: apps/dashboard/src/__tests__/integration/print-dialog.test.ts
      scenarios:
      - Print configuration submission
      - Real-time progress display
      - PDF download functionality
    manual_testing:
    - step: Upload a multi-panel comic and request print-ready PDF
      expected: PDF generates with proper dimensions, bleed areas, and high resolution
    - step: Test color accuracy by comparing screen colors to PDF CMYK output
      expected: Minimal color shift with acceptable commercial printing standards
    - step: Submit large comic (30+ pages) and monitor memory usage
      expected: Memory stays under 2GB threshold throughout processing
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Install and configure dependencies (Puppeteer, sharp, pdf-lib, canvas)
      done: false
    - task: Create print service with PDF generation core functionality
      done: false
    - task: Implement background job system with Redis queue management
      done: false
    - task: Build WebSocket integration for real-time status updates
      done: false
    - task: Develop print dialog UI with configuration options and preview
      done: false
    - task: Add API routes for print requests, status, and file downloads
      done: false
    - task: Implement color profile conversion and image processing pipeline
      done: false
    - task: Create HTML/CSS templates for print layouts with bleed handling
      done: false
    - task: Add comprehensive error handling and memory management
      done: false
    - task: Integration testing with existing comic assembly service
      done: false
    - task: Performance optimization and load testing
      done: false
    - task: Documentation and code review
      done: false
- key: T71
  title: M4 Integration Testing
  type: Task
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p0
  effort: 5
  area: comic
  dependsOn:
  - T67
  - T68
  - T69
  agent_notes:
    research_findings: '**Context:**

      M4 Integration Testing refers to comprehensive end-to-end testing of the core
      comic generation pipeline after M4 (Comic Generation) milestone. This is critical
      because the comic generation involves complex interactions between multiple
      services: novel processing, scene extraction, character consistency, image generation
      via RunPod/Stable Diffusion, and comic panel assembly. Integration testing ensures
      these components work together seamlessly before the M5 product assembly phase.
      Without this, we risk shipping a product where individual components work but
      fail when orchestrated together.


      **Technical Approach:**

      - Use Playwright for end-to-end testing that spans the full pipeline from novel
      upload to comic output

      - Implement test data factories for consistent novel inputs and expected comic
      outputs

      - Create mock RunPod endpoints for reliable, fast image generation testing

      - Use Vitest for integration testing of backend services (novel processor →
      scene extractor → image generator → comic assembler)

      - Implement visual regression testing for comic panel layouts and quality

      - Set up staging environment with Supabase branch for isolated testing

      - Create performance benchmarks for the full pipeline (target: <2 minutes for
      10-page comic)


      **Dependencies:**

      - External: @playwright/test, msw (Mock Service Worker), sharp (image comparison),
      pdf-parse (comic output validation)

      - Internal: Novel processing service, Scene extraction service, Character consistency
      engine, RunPod integration, Comic assembly service, Supabase schemas


      **Risks:**

      - Flaky tests due to AI/ML non-deterministic outputs: Use seed values and mock
      responses for consistency

      - Test data management complexity: Implement proper test database seeding/cleanup

      - Long test execution times: Parallelize tests and use mocked external services

      - Environment drift between test/prod: Use Docker containers for consistent
      environments


      **Complexity Notes:**

      This is significantly more complex than typical integration testing due to the
      AI/ML pipeline nature. The non-deterministic outputs from LLMs and image generation
      make traditional assertion patterns difficult. Requires sophisticated mocking
      strategies and potentially fuzzy matching for outputs.


      **Key Files:**

      - tests/integration/comic-pipeline.spec.ts: Main pipeline test suite

      - tests/mocks/runpod-mock.ts: Mock RunPod API responses

      - tests/fixtures/novels/: Test novel inputs of varying complexity

      - tests/utils/comic-assertions.ts: Custom matchers for comic output validation

      - apps/backend/src/services/comic-pipeline/: Integration points to test

      - playwright.config.ts: E2E test configuration with staging environment

      '
    design_decisions:
    - decision: 'Use hybrid approach: mocked external services for speed, real services
        for critical path validation'
      rationale: Balances test reliability/speed with realistic validation of actual
        service integrations
      alternatives_considered:
      - Full mocking (too disconnected from reality)
      - Full real services (too slow/unreliable)
    - decision: Implement visual regression testing for comic outputs using perceptual
        hashing
      rationale: Comic quality is inherently visual; traditional text assertions insufficient
        for validating layout and image quality
      alternatives_considered:
      - Text-only validation
      - Manual QA only
      - Pixel-perfect comparison
    - decision: Create dedicated test database branch in Supabase for integration
        tests
      rationale: Ensures test isolation while maintaining realistic data relationships
        and constraints
      alternatives_considered:
      - In-memory database
      - Shared test database
      - Production database subset
    researched_at: '2026-02-07T19:13:25.683753'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:39:05.945047'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a multi-layer integration testing strategy starting with
      isolated service-to-service tests using Vitest, progressing to full pipeline
      tests with Playwright. Mock external AI services for consistency while maintaining
      a subset of tests against real services for validation. Use visual regression
      testing for comic output quality and implement performance benchmarks to ensure
      the pipeline meets latency requirements. Create comprehensive test data factories
      that cover edge cases like complex novels, multiple characters, and various
      genre requirements.

      '
    external_dependencies:
    - name: '@playwright/test'
      version: ^1.40.0
      reason: End-to-end testing of the complete comic generation pipeline across
        browser and API
    - name: msw
      version: ^2.0.0
      reason: Mock Service Worker for intercepting and mocking RunPod and external
        AI service calls
    - name: sharp
      version: ^0.33.0
      reason: Image processing for visual regression testing and comic output validation
    - name: pixelmatch
      version: ^5.3.0
      reason: Pixel-level image comparison for visual regression testing of comic
        panels
    - name: '@faker-js/faker'
      version: ^8.3.0
      reason: Generate realistic test data for novels, characters, and user scenarios
    files_to_modify:
    - path: apps/backend/src/services/comic-pipeline/pipeline-orchestrator.ts
      changes: Add instrumentation for test hooks, error handling improvements, performance
        metrics collection
    - path: apps/backend/src/services/novel-processor/index.ts
      changes: Add test mode flag for deterministic outputs, expose internal state
        for validation
    - path: apps/backend/src/services/runpod-integration/client.ts
      changes: Add mock mode toggle, implement test-friendly error simulation
    - path: playwright.config.ts
      changes: Add staging environment configuration, visual regression settings,
        parallel execution limits
    new_files:
    - path: tests/integration/comic-pipeline.spec.ts
      purpose: Main integration test suite covering full pipeline with mocked external
        services
    - path: tests/mocks/runpod-mock.ts
      purpose: MSW handlers for RunPod API responses, deterministic image generation
        mocks
    - path: tests/mocks/llm-mock.ts
      purpose: Mock LLM responses for scene extraction and character analysis
    - path: tests/fixtures/novels/test-novels.ts
      purpose: Test data factory for novels of varying complexity and characteristics
    - path: tests/utils/comic-assertions.ts
      purpose: Custom Jest/Vitest matchers for comic output validation and fuzzy matching
    - path: tests/utils/test-database.ts
      purpose: Database seeding, cleanup utilities for isolated test environments
    - path: tests/e2e/comic-generation.spec.ts
      purpose: End-to-end Playwright tests covering user-facing comic generation flow
    - path: tests/e2e/visual-regression.spec.ts
      purpose: Visual regression testing for comic output quality using Playwright
        screenshots
    - path: tests/integration/performance.spec.ts
      purpose: Performance benchmarking tests for pipeline execution time and resource
        usage
    - path: apps/backend/src/services/comic-pipeline/__tests__/pipeline-orchestrator.test.ts
      purpose: Unit tests for main pipeline orchestration logic
    - path: apps/backend/src/services/comic-pipeline/__tests__/service-integration.test.ts
      purpose: Unit tests for service-to-service integration points
    - path: tests/setup/test-environment.ts
      purpose: Test environment setup with Supabase branch, mock services initialization
  acceptance_criteria:
  - criterion: Complete comic pipeline processes novel input to comic output end-to-end
    verification: Run `npm run test:e2e -- --grep 'full pipeline'` - test uploads
      novel, generates 10-page comic with consistent characters
  - criterion: Pipeline completes within performance benchmarks (<2 minutes for 10-page
      comic)
    verification: Performance test in tests/integration/performance.spec.ts reports
      execution time under 120 seconds
  - criterion: Integration tests achieve 90%+ reliability with mocked external services
    verification: Run test suite 20 times - max 2 failures allowed, check CI metrics
      dashboard
  - criterion: Visual regression testing validates comic panel quality and layout
      consistency
    verification: Playwright visual comparisons pass in tests/e2e/visual-regression.spec.ts
      with <5% pixel difference threshold
  - criterion: Test coverage for integration points reaches 85%+ across all pipeline
      services
    verification: Run `npm run test:coverage` - integration test coverage report shows
      >85% for services in apps/backend/src/services/comic-pipeline/
  testing:
    unit_tests:
    - file: apps/backend/src/services/comic-pipeline/__tests__/pipeline-orchestrator.test.ts
      coverage_target: 90%
      scenarios:
      - Successful pipeline execution with mocked services
      - Service failure handling and retry logic
      - Input validation and sanitization
      - Character consistency across pipeline stages
    - file: apps/backend/src/services/comic-pipeline/__tests__/service-integration.test.ts
      coverage_target: 85%
      scenarios:
      - Novel processor to scene extractor data flow
      - Scene extractor to image generator handoff
      - Image generator to comic assembler integration
    integration_tests:
    - file: tests/integration/comic-pipeline.spec.ts
      scenarios:
      - Full pipeline with simple 3-character novel
      - Complex multi-character novel with scene transitions
      - Error recovery from RunPod API failures
      - Pipeline performance benchmarking
      - Database state consistency after pipeline completion
    - file: tests/integration/service-mocks.spec.ts
      scenarios:
      - Mock RunPod responses for consistent image generation
      - Mock LLM responses for deterministic scene extraction
      - Failure simulation and graceful degradation
    e2e_tests:
    - file: tests/e2e/comic-generation.spec.ts
      scenarios:
      - User uploads novel and receives completed comic
      - Comic download and PDF validation
      - Progress tracking throughout pipeline stages
    - file: tests/e2e/visual-regression.spec.ts
      scenarios:
      - Comic panel layout consistency
      - Character appearance consistency across panels
      - Typography and speech bubble rendering
    manual_testing:
    - step: Upload complex novel (50+ pages) through UI
      expected: Pipeline processes without memory leaks, completes in <5 minutes
    - step: Test pipeline with various novel genres (sci-fi, fantasy, romance)
      expected: Character consistency and scene interpretation appropriate for genre
    - step: Simulate high load with 5 concurrent comic generations
      expected: All pipelines complete successfully, no resource contention
  estimates:
    development: 6
    code_review: 1
    testing: 2
    documentation: 1
    total: 10
  progress:
    status: not-started
    checklist:
    - task: Setup test environment with Supabase branch and mock services
      done: false
    - task: Create test data factories and novel fixtures for various scenarios
      done: false
    - task: Implement RunPod and LLM mock services with MSW
      done: false
    - task: Build custom assertion utilities for comic output validation
      done: false
    - task: Develop integration test suite covering service-to-service interactions
      done: false
    - task: Create end-to-end Playwright tests for full user workflow
      done: false
    - task: Implement visual regression testing with screenshot comparisons
      done: false
    - task: Add performance benchmarking and monitoring instrumentation
      done: false
    - task: Setup CI/CD pipeline integration with test reporting
      done: false
    - task: Document testing procedures and troubleshooting guide
      done: false
- key: T73
  title: Variant Generation
  type: Feature
  milestone: M5 - Product Assembly
  iteration: I5
  priority: p2
  effort: 3
  area: comic
  dependsOn:
  - T64
  agent_notes:
    research_findings: "**Context:**\nVariant Generation enables creating multiple\
      \ visual interpretations of the same comic scene/panel with different artistic\
      \ styles, character designs, layouts, or visual treatments. This is crucial\
      \ for allowing creators to explore different creative directions, A/B test visual\
      \ approaches, and provide customers with customization options. In the novel-to-comic\
      \ workflow, this comes after initial comic generation but before final assembly,\
      \ allowing refinement of visual storytelling.\n\n**Technical Approach:**\n-\
      \ Extend existing RunPod Stable Diffusion pipeline with variant generation capabilities\n\
      - Implement prompt variation strategies (style modifiers, composition changes,\
      \ character appearance tweaks)\n- Create a queue-based system for batch variant\
      \ generation to manage computational costs\n- Use ControlNet for maintaining\
      \ consistent layouts while varying artistic elements\n- Implement semantic hashing\
      \ to avoid generating duplicate variants\n- Store variants with hierarchical\
      \ relationships (original -> variant tree) in Supabase\n- Create real-time preview\
      \ system using WebSocket connections for variant generation status\n\n**Dependencies:**\n\
      - External: @runpod/sdk-js, ioredis (job queue), sharp (image processing), @supabase/realtime-js\n\
      - Internal: existing panel generation service, image storage service, comic\
      \ assembly pipeline, dashboard variant comparison UI\n\n**Risks:**\n- Computational\
      \ Cost Explosion: Multiple variants per panel could dramatically increase RunPod\
      \ costs\n  Mitigation: Implement variant limits, cost estimation, and user-configurable\
      \ quality/speed tradeoffs\n- Storage Bloat: Variants multiply storage requirements\
      \ exponentially\n  Mitigation: Implement smart cleanup policies, compressed\
      \ preview generations, lazy full-resolution rendering\n- UI Complexity: Comparing/selecting\
      \ from many variants creates choice paralysis\n  Mitigation: Intelligent variant\
      \ ranking, progressive disclosure, and ML-powered similarity clustering\n\n\
      **Complexity Notes:**\nHigher complexity than initially estimated due to the\
      \ need for sophisticated variant relationship management, cost optimization\
      \ strategies, and complex UI state management for variant comparison workflows.\
      \ The technical challenge lies not in generation itself but in making variant\
      \ exploration intuitive and cost-effective.\n\n**Key Files:**\n- apps/api/src/services/variant-generation.service.ts:\
      \ Core variant generation logic\n- packages/shared/src/types/comic.types.ts:\
      \ Variant relationship type definitions\n- apps/dashboard/src/components/VariantExplorer.tsx:\
      \ Variant comparison interface\n- apps/api/src/controllers/variants.controller.ts:\
      \ Variant CRUD and generation endpoints\n- packages/ml-pipeline/src/stable-diffusion/variant-generator.ts:\
      \ ML pipeline integration\n"
    design_decisions:
    - decision: Tree-based variant storage with parent-child relationships
      rationale: Enables tracking variant genealogy, selective regeneration, and hierarchical
        organization for complex variant exploration workflows
      alternatives_considered:
      - Flat variant arrays
      - Tag-based variant grouping
      - Session-based variant storage
    - decision: Queue-based batch processing with WebSocket status updates
      rationale: Balances computational efficiency with user experience, prevents
        resource overwhelm while providing real-time feedback
      alternatives_considered:
      - Synchronous generation
      - Polling-based status
      - Email notification system
    - decision: ControlNet integration for layout consistency
      rationale: Maintains panel composition and character positioning while allowing
        artistic variation, crucial for narrative coherence
      alternatives_considered:
      - Pure prompt variation
      - Image-to-image translation
      - Style transfer approaches
    researched_at: '2026-02-07T19:13:51.129774'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:39:31.208864'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a hierarchical variant generation system that extends the
      existing Stable Diffusion pipeline with ControlNet integration for layout preservation.
      Create a queue-based architecture using Redis for managing variant generation
      jobs, with WebSocket real-time updates to the dashboard. Build a tree-structured
      storage system in Supabase to track variant relationships and implement intelligent
      preview generation to minimize storage costs. Develop a sophisticated variant
      explorer UI component that enables easy comparison and selection of generated
      variants.

      '
    external_dependencies:
    - name: '@runpod/sdk-js'
      version: ^1.4.0
      reason: Extended integration for ControlNet-enabled variant generation endpoints
    - name: ioredis
      version: ^5.3.0
      reason: Job queue management for batch variant processing and rate limiting
    - name: sharp
      version: ^0.33.0
      reason: Image processing for preview generation, comparison grids, and format
        optimization
    - name: react-compare-slider
      version: ^3.1.0
      reason: Interactive variant comparison UI component for side-by-side evaluation
    files_to_modify:
    - path: packages/shared/src/types/comic.types.ts
      changes: Add VariantNode, VariantGenerationRequest, VariantMetadata interfaces
        with hierarchical relationship fields
    - path: apps/api/src/services/panel-generation.service.ts
      changes: Integrate variant generation hooks after initial panel creation, add
        variant cost tracking
    - path: packages/ml-pipeline/src/stable-diffusion/base-generator.ts
      changes: Add ControlNet integration methods and prompt variation utilities
    new_files:
    - path: apps/api/src/services/variant-generation.service.ts
      purpose: Core service managing variant generation logic, prompt strategies,
        and RunPod orchestration
    - path: apps/api/src/controllers/variants.controller.ts
      purpose: REST endpoints for variant CRUD operations and generation triggering
    - path: apps/api/src/queues/variant-generation.queue.ts
      purpose: Redis-based job queue for asynchronous variant processing
    - path: packages/ml-pipeline/src/stable-diffusion/variant-generator.ts
      purpose: Specialized ML pipeline component for variant generation with ControlNet
    - path: apps/dashboard/src/components/VariantExplorer.tsx
      purpose: React component for variant comparison, selection, and management interface
    - path: apps/api/src/utils/semantic-hash.ts
      purpose: Utility for generating content hashes to prevent duplicate variant
        generation
    - path: database/migrations/20240115_add_variant_tables.sql
      purpose: Database schema for variant relationships and metadata storage
  acceptance_criteria:
  - criterion: Users can generate 2-5 variants of any comic panel with different artistic
      styles while maintaining consistent layout and character positioning
    verification: Create panel, trigger variant generation with different style prompts,
      verify ControlNet preserves layout structure via visual comparison
  - criterion: Variant generation jobs are queued and processed asynchronously with
      real-time status updates to dashboard
    verification: Generate variants for multiple panels simultaneously, verify Redis
      queue processing and WebSocket status updates in browser dev tools
  - criterion: Generated variants are stored hierarchically with parent-child relationships
      and accessible via API endpoints
    verification: Query GET /api/variants/{panelId} and verify response includes variant
      tree structure with original panel as root
  - criterion: Variant explorer UI displays generated variants in comparison grid
      with selection and ranking capabilities
    verification: Navigate to panel variant view, verify grid layout shows all variants,
      test selection state persistence and ranking controls
  - criterion: System implements cost controls limiting variants per panel (max 5)
      and provides generation cost estimates
    verification: Attempt to generate >5 variants, verify rejection with 400 error,
      check cost estimation API returns RunPod pricing calculations
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/variant-generation.test.ts
      coverage_target: 90%
      scenarios:
      - Prompt variation strategy generation
      - Semantic hash collision detection
      - Cost calculation accuracy
      - Variant limit enforcement
    - file: apps/dashboard/src/components/__tests__/VariantExplorer.test.tsx
      coverage_target: 85%
      scenarios:
      - Variant grid rendering with mock data
      - Selection state management
      - WebSocket connection handling
      - Loading states during generation
    integration_tests:
    - file: apps/api/src/__tests__/integration/variant-workflow.test.ts
      scenarios:
      - End-to-end variant generation from API request to stored result
      - Redis queue job processing with RunPod integration
      - Variant relationship storage in Supabase
      - Cost limit enforcement across multiple requests
    manual_testing:
    - step: Generate variants for action scene with different art styles (manga, realistic,
        cartoon)
      expected: 3 variants generated preserving character positions and scene composition
    - step: Monitor variant generation queue during high load (10+ concurrent requests)
      expected: Jobs processed in order without RunPod rate limit errors, WebSocket
        updates arrive within 2 seconds
    - step: Navigate variant explorer with generated variants
      expected: Smooth comparison experience, variants load quickly, selection persists
        across page refreshes
  estimates:
    development: 6
    code_review: 1
    testing: 2
    documentation: 1
    total: 10
  progress:
    status: not-started
    checklist:
    - task: Set up database schema and migrations for variant storage with hierarchical
        relationships
      done: false
    - task: Implement Redis queue system for variant generation jobs with proper error
        handling
      done: false
    - task: Integrate ControlNet into existing Stable Diffusion pipeline for layout
        preservation
      done: false
    - task: Build variant generation service with prompt variation strategies and
        cost controls
      done: false
    - task: Create REST API endpoints for variant CRUD operations and generation triggering
      done: false
    - task: Implement WebSocket real-time updates for variant generation status
      done: false
    - task: Develop VariantExplorer React component with comparison grid and selection
        logic
      done: false
    - task: Add semantic hashing system to prevent duplicate variant generation
      done: false
    - task: Integrate variant generation into existing comic assembly pipeline
      done: false
    - task: Implement storage cleanup policies and preview generation optimization
      done: false
- key: T74
  title: Discord Bot
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 5
  area: distribution
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      A Discord bot serves as a distribution and community engagement channel for
      Morpheus-generated comics. Discord has massive communities of comic enthusiasts,
      web novel readers, and AI art fans. The bot enables users to discover new comics,
      subscribe to series updates, purchase comics directly through Discord, and share
      favorite panels with friends. This creates a viral marketing channel and reduces
      customer acquisition costs while building an engaged community around user-generated
      content.


      **Technical Approach:**

      Build a Discord bot using discord.js v14 with TypeScript, integrated with the
      existing Fastify backend through REST APIs and webhooks. The bot should support
      slash commands for comic discovery, subscription management, and purchasing.
      Implement OAuth2 flow to link Discord users with Morpheus accounts. Use Discord''s
      embed system to showcase comic panels with rich metadata. Store Discord-specific
      data (guild configs, user preferences) in Supabase alongside existing user data.
      Implement rate limiting and queue systems to handle Discord API limits.


      **Dependencies:**

      - External: discord.js v14, @discordjs/builders, @discordjs/rest

      - Internal: Existing auth service, payment service, comic metadata APIs, user
      management system

      - Infrastructure: Discord Developer Portal app, webhook endpoints in Fastify
      backend


      **Risks:**

      - Discord API rate limits: Implement proper queuing with p-queue and exponential
      backoff

      - Bot spam/abuse: Add user cooldowns, command rate limiting, and moderation
      features

      - Payment fraud: Integrate existing payment validation, require account linking
      for purchases

      - Scale issues: Discord bots can explode in usage quickly - design for horizontal
      scaling from day one


      **Complexity Notes:**

      Initially seems straightforward but Discord''s ecosystem complexity is deceptive.
      Slash commands, permissions, guild management, and message embeds all have gotchas.
      The OAuth2 flow integration with existing Supabase auth adds significant complexity.
      Bot deployment and process management is also non-trivial compared to web services.


      **Key Files:**

      - apps/discord-bot/: New application in Turborepo structure

      - packages/shared/types/discord.ts: Shared Discord-related types

      - apps/api/src/routes/webhooks/discord.ts: Discord webhook handlers

      - apps/api/src/services/discord-integration.ts: Discord API integration service

      - apps/dashboard/src/pages/integrations/discord.tsx: Discord bot management
      UI

      '
    design_decisions:
    - decision: Separate Discord bot app in monorepo rather than embedding in main
        API
      rationale: Discord bots have different scaling patterns, deployment needs, and
        development workflows. Separating allows independent scaling and reduces API
        service complexity.
      alternatives_considered:
      - Embed in main Fastify app
      - Serverless functions approach
    - decision: Use discord.js v14 with slash commands instead of message commands
      rationale: Discord is deprecating message commands. Slash commands provide better
        UX with autocomplete and validation. v14 is the current stable release with
        best TypeScript support.
      alternatives_considered:
      - discord.py (Python)
      - older discord.js versions
      - raw Discord API
    - decision: OAuth2 account linking rather than separate Discord-only accounts
      rationale: Users should have unified experience across platforms. Linking Discord
        to existing Morpheus accounts enables full feature access and consistent payment/subscription
        management.
      alternatives_considered:
      - Discord-only lightweight accounts
      - manual account claiming
    researched_at: '2026-02-07T19:14:18.423710'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:39:58.091202'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a new Discord bot application in the Turborepo structure using
      discord.js v14 with TypeScript. Implement slash commands for comic browsing
      (/comics search, /comics subscribe), account management (/link, /profile), and
      purchasing (/buy). Use Discord OAuth2 to securely link Discord users to Morpheus
      accounts stored in Supabase. The bot communicates with the main Fastify API
      through internal service calls and handles Discord-specific data formatting.
      Deploy as a separate service with PM2 or Docker for process management and auto-restart
      capabilities.

      '
    external_dependencies:
    - name: discord.js
      version: ^14.14.1
      reason: Primary Discord bot framework with excellent TypeScript support
    - name: '@discordjs/builders'
      version: ^1.7.0
      reason: Builder utilities for slash commands and embeds
    - name: '@discordjs/rest'
      version: ^2.2.0
      reason: REST API client for Discord interactions outside of gateway
    - name: p-queue
      version: ^8.0.1
      reason: Queue system for managing Discord API rate limits
    files_to_modify:
    - path: apps/api/src/routes/webhooks/index.ts
      changes: Add Discord webhook route registration
    - path: packages/shared/types/user.ts
      changes: Add DiscordAccount interface and user relationship types
    - path: apps/dashboard/src/components/layout/Sidebar.tsx
      changes: Add Discord integrations navigation item
    new_files:
    - path: apps/discord-bot/package.json
      purpose: Discord bot dependencies and scripts
    - path: apps/discord-bot/src/index.ts
      purpose: Bot entry point and client initialization
    - path: apps/discord-bot/src/commands/comics.ts
      purpose: Comic search, browse, and subscription slash commands
    - path: apps/discord-bot/src/commands/account.ts
      purpose: User account linking and profile management commands
    - path: apps/discord-bot/src/commands/purchase.ts
      purpose: Comic purchase and payment flow commands
    - path: apps/discord-bot/src/services/auth.ts
      purpose: Discord OAuth2 and account linking service
    - path: apps/discord-bot/src/services/api-client.ts
      purpose: HTTP client for Morpheus API integration
    - path: apps/discord-bot/src/utils/embeds.ts
      purpose: Discord embed formatting utilities for comics
    - path: apps/discord-bot/src/utils/queue.ts
      purpose: Rate limiting and request queuing for Discord API
    - path: apps/api/src/routes/webhooks/discord.ts
      purpose: Discord interaction webhook handlers
    - path: apps/api/src/services/discord-integration.ts
      purpose: Discord API integration and webhook processing
    - path: packages/shared/types/discord.ts
      purpose: Discord-specific type definitions and interfaces
    - path: apps/dashboard/src/pages/integrations/discord.tsx
      purpose: Discord bot management and analytics dashboard
    - path: packages/database/migrations/20240115000000_discord_accounts.sql
      purpose: Database schema for Discord user account linking
    - path: apps/discord-bot/docker/Dockerfile
      purpose: Docker containerization for bot deployment
    - path: apps/discord-bot/ecosystem.config.js
      purpose: PM2 process management configuration
  acceptance_criteria:
  - criterion: Discord bot responds to slash commands (/comics search, /comics subscribe,
      /buy, /link, /profile) with appropriate embeds and data
    verification: Test each slash command in Discord server returns formatted embed
      with comic data from API
  - criterion: OAuth2 flow successfully links Discord users to Morpheus accounts in
      Supabase
    verification: Complete /link command flow and verify user_discord_accounts table
      populated with correct discord_id and user_id mapping
  - criterion: Bot handles rate limits and API errors gracefully without crashing
    verification: Simulate Discord API rate limit responses and verify bot queues
      requests and continues operating
  - criterion: Purchase flow through Discord validates payment and updates user subscriptions
    verification: Execute /buy command, complete payment, verify subscription created
      in database and confirmation sent to Discord
  - criterion: Bot admin dashboard displays connection status, usage metrics, and
      configuration options
    verification: Access apps/dashboard Discord integration page shows bot online
      status, command usage stats, and guild management
  testing:
    unit_tests:
    - file: apps/discord-bot/src/__tests__/commands/comics.test.ts
      coverage_target: 90%
      scenarios:
      - Search command with valid query returns formatted results
      - Subscribe command with linked account creates subscription
      - Commands with unlinked account prompt OAuth flow
      - Invalid parameters return helpful error messages
    - file: apps/discord-bot/src/__tests__/services/auth.test.ts
      coverage_target: 85%
      scenarios:
      - OAuth token exchange success
      - Account linking with existing user
      - Invalid OAuth state handling
    integration_tests:
    - file: apps/api/src/__tests__/integration/discord-webhooks.test.ts
      scenarios:
      - Discord webhook interaction creates proper API response
      - Payment webhook from Discord purchase updates subscription
      - User linking flow end-to-end with Supabase
    manual_testing:
    - step: Join test Discord server and run /comics search cyberpunk
      expected: Bot returns embed with cyberpunk comic results, thumbnail images,
        and action buttons
    - step: Run /link command without existing account link
      expected: Bot provides OAuth URL and instructions for account linking
    - step: Complete purchase flow with /buy command
      expected: Payment modal appears, processes payment, sends confirmation embed
  estimates:
    development: 5
    code_review: 1
    testing: 1.5
    documentation: 1
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Setup Discord Developer Portal application and obtain bot token
      done: false
    - task: Create Discord bot Turborepo package with TypeScript and discord.js
      done: false
    - task: Implement slash command handlers for comics, account, and purchase flows
      done: false
    - task: Build OAuth2 service for Discord account linking with Supabase integration
      done: false
    - task: Create Discord webhook endpoints in Fastify API for interaction handling
      done: false
    - task: Implement rate limiting and queue system for Discord API calls
      done: false
    - task: Build Discord embed utilities for comic display and formatting
      done: false
    - task: Create dashboard UI for Discord bot management and analytics
      done: false
    - task: Setup Docker deployment and process management with PM2
      done: false
    - task: Write comprehensive tests for commands, auth flow, and integrations
      done: false
    - task: Document bot setup, deployment, and usage instructions
      done: false
    - task: Conduct code review and security audit for OAuth and payment flows
      done: false
- key: T75
  title: Discord Command Handler
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 3
  area: distribution
  dependsOn:
  - T74
  agent_notes:
    research_findings: '**Context:**

      Discord Command Handler is essential for M6 Commerce & Distribution as it enables
      direct comic sales and distribution through Discord servers. This creates an
      additional revenue channel beyond the web storefront, allowing users to discover,
      preview, and purchase comics directly within Discord communities. It supports
      the business goal of expanding distribution channels and reaching users where
      they already engage with content.


      **Technical Approach:**

      - Use discord.js v14 as the primary Discord API wrapper

      - Implement slash command architecture with proper command registration

      - Create modular command handlers with TypeScript interfaces for type safety

      - Integrate with existing Supabase backend for user authentication and comic
      data

      - Use webhook-based event handling for scalable bot interactions

      - Implement Redis caching for frequently accessed comic metadata

      - Follow Discord''s interaction response patterns (3-second acknowledgment rule)

      - Use Discord embeds for rich comic previews with image attachments


      **Dependencies:**

      - External: discord.js, @discordjs/builders, @discordjs/rest

      - Internal: Supabase client, authentication service, comic catalog API, payment
      processing service

      - Infrastructure: Redis for caching, webhook endpoints in Fastify backend


      **Risks:**

      - Rate limiting: Discord API has strict rate limits that could impact user experience

      - Token security: Bot tokens must be securely managed and rotated

      - Interaction timeouts: Discord requires responses within 3 seconds, complex
      operations need deferring

      - Permission complexity: Discord server permissions can be complex and vary
      by server

      - Scaling challenges: Single bot instance may not handle high concurrent usage


      **Complexity Notes:**

      Higher complexity than initially estimated due to Discord''s interaction model
      requiring careful state management and the need to integrate with multiple existing
      services (auth, payments, catalog). The asynchronous nature of Discord interactions
      adds complexity to error handling and user feedback.


      **Key Files:**

      - packages/discord-bot/: New package for Discord bot functionality

      - packages/backend/src/routes/discord/: Discord webhook handlers

      - packages/shared/src/types/discord.ts: Shared Discord types

      - packages/backend/src/services/discord.ts: Discord service integration

      '
    design_decisions:
    - decision: Use slash commands instead of message-based commands
      rationale: Slash commands provide better UX with auto-completion, validation,
        and are Discord's recommended modern approach
      alternatives_considered:
      - Message-based prefix commands
      - Context menu commands only
    - decision: Separate Discord bot as independent service with webhook integration
      rationale: Allows independent scaling and deployment while maintaining loose
        coupling with main backend
      alternatives_considered:
      - Embedded bot within Fastify backend
      - Serverless Discord functions
    - decision: Implement command cooldowns and user session management
      rationale: Prevents abuse and manages Discord API rate limits while providing
        smooth user experience
      alternatives_considered:
      - No rate limiting
      - Global rate limiting only
    researched_at: '2026-02-07T19:14:40.472393'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:40:21.138306'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a dedicated Discord bot service using discord.js with slash
      command handlers for comic browsing, purchasing, and user management. Implement
      webhook-based architecture where Discord interactions trigger Fastify backend
      endpoints that handle business logic and return formatted responses. Use Redis
      for caching comic data and user sessions to minimize database queries. Integrate
      with existing Supabase authentication by linking Discord users to Morpheus accounts,
      and leverage the current payment processing pipeline for comic purchases.

      '
    external_dependencies:
    - name: discord.js
      version: ^14.14.1
      reason: Primary Discord API wrapper with TypeScript support
    - name: '@discordjs/builders'
      version: ^1.7.0
      reason: Utility for building Discord API payloads and slash commands
    - name: '@discordjs/rest'
      version: ^2.2.0
      reason: REST client for Discord API interactions and command registration
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for caching Discord interactions and user sessions
    files_to_modify:
    - path: packages/backend/src/routes/index.ts
      changes: Add Discord webhook route registration
    - path: packages/backend/src/services/auth.service.ts
      changes: Add Discord OAuth integration methods
    - path: packages/shared/src/types/user.ts
      changes: Add Discord user ID field to User interface
    - path: docker-compose.yml
      changes: Add Redis service for Discord bot caching
    new_files:
    - path: packages/discord-bot/src/index.ts
      purpose: Main Discord bot entry point and client initialization
    - path: packages/discord-bot/src/commands/browse.ts
      purpose: Browse comics command handler with pagination
    - path: packages/discord-bot/src/commands/buy.ts
      purpose: Purchase comic command handler
    - path: packages/discord-bot/src/commands/login.ts
      purpose: User authentication command handler
    - path: packages/discord-bot/src/services/discord-auth.service.ts
      purpose: Discord user authentication and account linking
    - path: packages/discord-bot/src/services/comic-cache.service.ts
      purpose: Redis-based comic metadata caching
    - path: packages/discord-bot/src/utils/embed-builder.ts
      purpose: Discord embed formatting utilities
    - path: packages/discord-bot/src/types/commands.ts
      purpose: TypeScript interfaces for Discord commands
    - path: packages/backend/src/routes/discord/webhooks.ts
      purpose: Discord interaction webhook handlers
    - path: packages/backend/src/routes/discord/auth.ts
      purpose: Discord OAuth callback endpoints
    - path: packages/backend/src/services/discord.service.ts
      purpose: Backend Discord integration service
    - path: packages/shared/src/types/discord.ts
      purpose: Shared Discord-related type definitions
    - path: packages/discord-bot/package.json
      purpose: Discord bot package configuration
    - path: packages/discord-bot/Dockerfile
      purpose: Discord bot containerization
  acceptance_criteria:
  - criterion: Bot responds to /browse command with paginated comic listings showing
      title, price, and preview
    verification: Execute /browse in Discord server, verify embed shows comics with
      navigation buttons
  - criterion: Users can purchase comics via /buy command with Stripe integration
      and receive download links
    verification: Run /buy [comic-id], complete payment flow, verify comic delivery
      to DM
  - criterion: Bot handles Discord rate limits gracefully without crashing or losing
      user interactions
    verification: Load test with 50 concurrent interactions, verify no 429 errors
      or timeouts
  - criterion: User authentication links Discord accounts to existing Morpheus accounts
      via /login command
    verification: Execute /login, complete OAuth flow, verify user data synced in
      Supabase
  - criterion: Bot responds to all interactions within Discord's 3-second timeout
      requirement
    verification: Monitor interaction response times, all must acknowledge within
      3000ms
  testing:
    unit_tests:
    - file: packages/discord-bot/src/__tests__/commands/browse.test.ts
      coverage_target: 90%
      scenarios:
      - Browse command with valid pagination
      - Browse command with no comics available
      - Browse command with invalid page number
    - file: packages/discord-bot/src/__tests__/commands/buy.test.ts
      coverage_target: 90%
      scenarios:
      - Successful purchase flow
      - Payment failure handling
      - Invalid comic ID error
    - file: packages/discord-bot/src/__tests__/services/discord-auth.test.ts
      coverage_target: 85%
      scenarios:
      - Discord user linking to existing account
      - New user registration via Discord
      - Invalid authentication token handling
    integration_tests:
    - file: packages/backend/src/__tests__/integration/discord-webhooks.test.ts
      scenarios:
      - Discord interaction webhook processing
      - Payment completion webhook flow
      - Authentication callback integration
    manual_testing:
    - step: Invite bot to test Discord server with proper permissions
      expected: Bot appears online and slash commands are available
    - step: Test complete purchase flow from browse to delivery
      expected: User receives comic download link via DM after payment
    - step: Test error scenarios (invalid commands, network issues)
      expected: User-friendly error messages displayed in Discord
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Set up Discord bot package structure and dependencies
      done: false
    - task: Implement Discord bot client initialization and slash command registration
      done: false
    - task: Create browse command with paginated comic listings and embeds
      done: false
    - task: Implement user authentication flow linking Discord to Morpheus accounts
      done: false
    - task: Build purchase command with Stripe integration and comic delivery
      done: false
    - task: Set up Redis caching service for comic metadata
      done: false
    - task: Create Discord webhook handlers in Fastify backend
      done: false
    - task: Implement error handling and rate limiting protection
      done: false
    - task: Write comprehensive test suite and documentation
      done: false
    - task: Deploy bot and conduct end-to-end testing
      done: false
- key: T76
  title: Reddit Integration
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p2
  effort: 3
  area: distribution
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Reddit integration enables automated sharing of completed comics to relevant
      subreddits, driving organic discovery and user acquisition. This addresses the
      challenge of reaching niche audiences who are passionate about specific genres/topics
      that align with the comic content. Reddit''s community-driven nature makes it
      ideal for showcasing AI-generated comics to engaged audiences who appreciate
      creativity and technology.


      **Technical Approach:**

      Use Reddit''s OAuth 2.0 API via PRAW (Python Reddit API Wrapper) or Snoowrap
      (Node.js) to authenticate and post comics. Implement a queue-based system using
      Redis/Bull for scheduled posting to avoid rate limits. Create a content categorization
      service that maps comic genres/themes to appropriate subreddits. Build admin
      dashboard components for managing subreddit lists, posting schedules, and performance
      analytics.


      **Dependencies:**

      - External: snoowrap (Reddit API), ioredis (Redis client), bull (job queue),
      image-size (comic dimensions)

      - Internal: comic-service (accessing completed comics), auth-service (storing
      Reddit credentials), analytics-service (tracking engagement)


      **Risks:**

      - Rate limiting (600 requests/10 minutes): implement exponential backoff and
      job queuing

      - Subreddit moderation/spam detection: require manual approval workflow before
      auto-posting

      - API changes/deprecation: abstract Reddit client behind service interface for
      easy swapping

      - Content policy violations: implement content filtering and community guidelines
      checker


      **Complexity Notes:**

      More complex than initially estimated due to Reddit''s strict anti-spam policies
      and varied subreddit rules. Requires sophisticated content matching algorithms
      and careful rate limiting. The social aspect adds complexity around timing optimization
      and community engagement tracking.


      **Key Files:**

      - apps/api/src/services/reddit-service.ts: core Reddit API integration

      - apps/dashboard/src/components/distribution/RedditSettings.tsx: admin configuration
      UI

      - packages/shared/src/types/distribution.ts: Reddit posting configuration types

      - apps/api/src/jobs/reddit-publisher.ts: background job for scheduled posting

      '
    design_decisions:
    - decision: Use Node.js Snoowrap over direct API calls
      rationale: Handles OAuth refresh, rate limiting, and provides TypeScript support
        that aligns with our stack
      alternatives_considered:
      - Direct fetch() to Reddit API
      - Python PRAW with microservice
    - decision: Queue-based publishing with Redis/Bull
      rationale: Ensures posts are spread out to respect rate limits and allows retry
        logic for failed posts
      alternatives_considered:
      - Direct immediate posting
      - Cron-based scheduling
    - decision: Manual approval workflow for subreddit posts
      rationale: Prevents spam flags and allows content optimization before posting
        to communities
      alternatives_considered:
      - Fully automated posting
      - AI-based content filtering only
    researched_at: '2026-02-07T19:15:02.161859'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:40:47.349929'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a Reddit distribution service that integrates with the existing
      comic completion pipeline. When a comic is published, trigger a job to analyze
      content and suggest appropriate subreddits based on genre/theme mapping. Queue
      posts with optimal timing (analyzing subreddit activity patterns) and include
      engagement tracking. Build dashboard components for configuring Reddit credentials,
      managing subreddit lists, reviewing pending posts, and viewing performance analytics.

      '
    external_dependencies:
    - name: snoowrap
      version: ^1.23.0
      reason: Official Reddit API wrapper with OAuth 2.0 support and TypeScript definitions
    - name: bull
      version: ^4.12.0
      reason: Redis-based job queue for managing scheduled Reddit posts and rate limiting
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for job queue storage and caching subreddit metadata
    - name: image-size
      version: ^1.0.2
      reason: Extracting comic image dimensions for optimal Reddit display formatting
    files_to_modify:
    - path: apps/api/src/services/comic-service.ts
      changes: Add hook to trigger Reddit distribution job after comic publication
    - path: packages/shared/src/types/comic.ts
      changes: Add reddit_distribution_status field to Comic interface
    - path: apps/api/src/config/redis.ts
      changes: Add Reddit job queue configuration
    - path: apps/dashboard/src/routes/distribution.tsx
      changes: Add Reddit settings route and navigation
    new_files:
    - path: apps/api/src/services/reddit-service.ts
      purpose: Core Reddit API integration, OAuth handling, posting logic
    - path: apps/api/src/services/content-categorization-service.ts
      purpose: Map comic genres/themes to appropriate subreddit recommendations
    - path: apps/api/src/jobs/reddit-publisher.ts
      purpose: Background job processor for scheduled Reddit posting
    - path: apps/api/src/middleware/reddit-rate-limiter.ts
      purpose: Custom rate limiting middleware for Reddit API calls
    - path: apps/dashboard/src/components/distribution/RedditSettings.tsx
      purpose: Admin UI for Reddit OAuth, subreddit management, posting schedules
    - path: apps/dashboard/src/components/distribution/RedditQueue.tsx
      purpose: Admin UI for reviewing and approving pending Reddit posts
    - path: apps/dashboard/src/components/distribution/RedditAnalytics.tsx
      purpose: Display Reddit posting performance metrics and engagement data
    - path: packages/shared/src/types/distribution.ts
      purpose: TypeScript interfaces for Reddit configuration and posting data
    - path: apps/api/src/routes/distribution/reddit.ts
      purpose: API endpoints for Reddit configuration and queue management
    - path: apps/api/src/db/migrations/20240115_add_reddit_distribution.sql
      purpose: Database schema for Reddit credentials, subreddit lists, posting queue
  acceptance_criteria:
  - criterion: Comics can be automatically shared to appropriate subreddits after
      publication
    verification: Publish a test comic, verify it appears in configured subreddit
      queue and gets posted within scheduled timeframe
  - criterion: Rate limiting is properly handled with max 600 requests per 10 minutes
    verification: Load test with 1000+ posts, verify exponential backoff triggers
      and no API errors occur
  - criterion: Admin dashboard allows configuration of Reddit credentials, subreddit
      lists, and posting schedules
    verification: Navigate to /dashboard/distribution/reddit, verify all CRUD operations
      work for subreddits and scheduling
  - criterion: Content categorization correctly maps comic genres to relevant subreddits
    verification: Test comics with genres 'sci-fi', 'comedy', 'horror' map to appropriate
      subreddit suggestions
  - criterion: Manual approval workflow prevents spam and policy violations
    verification: Verify posts require admin approval before going live, and rejected
      posts don't get requeued
  testing:
    unit_tests:
    - file: apps/api/src/services/__tests__/reddit-service.test.ts
      coverage_target: 90%
      scenarios:
      - OAuth authentication flow
      - Subreddit validation and posting
      - Rate limit handling with exponential backoff
      - Content categorization algorithm
      - Error handling for API failures
    - file: apps/api/src/jobs/__tests__/reddit-publisher.test.ts
      coverage_target: 85%
      scenarios:
      - Job queue processing
      - Scheduled posting with optimal timing
      - Failed post retry logic
    integration_tests:
    - file: apps/api/src/__tests__/integration/reddit-distribution.test.ts
      scenarios:
      - End-to-end comic publication to Reddit
      - Admin approval workflow
      - Analytics tracking integration
    - file: apps/dashboard/src/components/distribution/__tests__/RedditSettings.integration.test.tsx
      scenarios:
      - Reddit credential configuration flow
      - Subreddit management CRUD operations
    manual_testing:
    - step: Configure Reddit OAuth credentials in dashboard
      expected: Successful connection to Reddit API, user account verified
    - step: Publish test comic with 'sci-fi' genre
      expected: Comic queued for r/scifi, r/comics, r/AIGeneratedArt subreddits
    - step: Approve queued post in admin dashboard
      expected: Post goes live on Reddit within 5 minutes, analytics start tracking
    - step: Test rate limiting by queuing 100+ posts rapidly
      expected: Posts distributed over time, no API errors, exponential backoff visible
        in logs
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup Reddit OAuth application and obtain API credentials
      done: false
    - task: Install and configure snoowrap, ioredis, bull dependencies
      done: false
    - task: Implement core RedditService with authentication and posting methods
      done: false
    - task: Create content categorization service for subreddit mapping
      done: false
    - task: Build job queue system with Redis and Bull for scheduled posting
      done: false
    - task: Implement rate limiting middleware with exponential backoff
      done: false
    - task: Create database schema and migrations for Reddit distribution data
      done: false
    - task: Build admin dashboard components for Reddit configuration
      done: false
    - task: Integrate Reddit distribution trigger with comic publication flow
      done: false
    - task: Add manual approval workflow and queue management UI
      done: false
    - task: Implement analytics tracking for Reddit post performance
      done: false
    - task: Write comprehensive tests for all components
      done: false
    - task: Create documentation for Reddit integration setup and usage
      done: false
    - task: Conduct security review of OAuth implementation
      done: false
- key: T77
  title: Browser Extension
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 5
  area: distribution
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      A browser extension for Morpheus serves multiple distribution and engagement
      purposes:

      - Allows users to quickly transform web articles, blog posts, or selected text
      into comics without leaving their browsing context

      - Provides seamless integration with the Morpheus platform for authenticated
      users

      - Creates a new acquisition funnel by capturing users during their natural reading
      flow

      - Enables bookmarking/saving content for later transformation in the main platform

      - Supports viral growth through easy sharing of transformed comics directly
      from any webpage


      **Technical Approach:**

      - Manifest V3 extension (Chrome/Edge primary, Firefox secondary) using TypeScript

      - Content script injection for text selection and UI overlay

      - Background service worker for API communication with Morpheus backend

      - Popup interface for quick transformations and user account management

      - Shared utilities package from main monorepo for API clients and type definitions

      - Chrome Extension API for storage, tabs, and activeTab permissions

      - PostMessage communication between content script and popup/background


      **Dependencies:**

      - External: @types/chrome, webextension-polyfill, @morpheus/shared-types, @morpheus/api-client

      - Internal: Authentication service integration, Comic generation API endpoints,
      User preferences/settings service


      **Risks:**

      - Content Security Policy conflicts: Use extension-specific CSP and avoid inline
      scripts

      - Cross-origin API calls: Implement proper CORS handling and use background
      script as proxy

      - Text extraction complexity: Different websites have varying DOM structures
      requiring robust selection logic

      - Performance on content-heavy sites: Lazy load extension features and minimize
      DOM manipulation

      - Store approval delays: Plan 2-4 week review cycles for Chrome Web Store and
      Firefox Add-ons


      **Complexity Notes:**

      Higher complexity than initially estimated due to:

      - Multi-browser compatibility requirements

      - Complex text extraction and context preservation

      - Authentication flow in extension context (OAuth redirect handling)

      - Real-time communication between extension components

      - Store submission and review processes

      However, leveraging existing Morpheus API reduces backend complexity.


      **Key Files:**

      - apps/browser-extension/manifest.json: Extension configuration and permissions

      - apps/browser-extension/src/content-script.ts: Text selection and DOM interaction

      - apps/browser-extension/src/background.ts: Service worker for API communication

      - apps/browser-extension/src/popup/: React-based popup interface

      - packages/shared/src/browser-extension/: Shared types and utilities

      - apps/api/src/routes/extension/: Extension-specific API endpoints

      '
    design_decisions:
    - decision: Manifest V3 with service worker architecture
      rationale: Future-proof approach required by Chrome, better security model,
        persistent background capabilities
      alternatives_considered:
      - Manifest V2 (deprecated)
      - Hybrid V2/V3 approach
    - decision: React for popup UI with shared components
      rationale: Consistency with main platform, reuse existing design system, faster
        development
      alternatives_considered:
      - Vanilla JS/HTML
      - Vue.js
      - Svelte
    - decision: Content script injection vs. always-on presence
      rationale: Better performance, user privacy, and reduced memory footprint
      alternatives_considered:
      - Always-active content script
      - Declarative content API only
    researched_at: '2026-02-07T19:15:25.865718'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:41:13.776974'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a Manifest V3 browser extension with TypeScript, featuring a
      content script for text selection/extraction, a service worker background script
      for API communication, and a React-based popup interface. Leverage the existing
      Morpheus API through a shared client package, implement secure authentication
      via extension storage API, and use PostMessage for component communication.
      Package as separate builds for Chrome and Firefox with webextension-polyfill
      for compatibility.

      '
    external_dependencies:
    - name: webextension-polyfill
      version: ^0.10.0
      reason: Cross-browser API compatibility between Chrome and Firefox
    - name: '@types/chrome'
      version: ^0.0.254
      reason: TypeScript definitions for Chrome Extension APIs
    - name: react
      version: ^18.2.0
      reason: UI framework for popup interface, consistent with main platform
    - name: '@crxjs/vite-plugin'
      version: ^2.0.0
      reason: Vite integration for extension development and hot reload
    files_to_modify:
    - path: apps/api/src/routes/index.ts
      changes: Add extension routes import and mount at /api/extension
    - path: packages/shared/src/types/index.ts
      changes: Export browser extension types and API interfaces
    - path: packages/api-client/src/index.ts
      changes: Add extension-specific API client methods for authentication and comic
        generation
    new_files:
    - path: apps/browser-extension/manifest.json
      purpose: Manifest V3 configuration with permissions for activeTab, storage,
        and host permissions
    - path: apps/browser-extension/src/content-script.ts
      purpose: Inject UI overlay, handle text selection, communicate with background
        script
    - path: apps/browser-extension/src/background.ts
      purpose: Service worker for API communication, storage management, tab coordination
    - path: apps/browser-extension/src/popup/index.tsx
      purpose: React-based popup interface for quick transformations and account management
    - path: apps/browser-extension/src/popup/components/TextPreview.tsx
      purpose: Component to show selected text and transformation options
    - path: apps/browser-extension/src/popup/components/AuthPanel.tsx
      purpose: Handle login/logout and account status display
    - path: apps/browser-extension/src/shared/messaging.ts
      purpose: Type-safe message passing between extension components
    - path: apps/browser-extension/src/shared/storage.ts
      purpose: Wrapper for Chrome storage API with TypeScript types
    - path: apps/browser-extension/webpack.config.js
      purpose: Build configuration for TypeScript compilation and asset bundling
    - path: apps/browser-extension/package.json
      purpose: Extension-specific dependencies and build scripts
    - path: apps/api/src/routes/extension/auth.ts
      purpose: Extension-specific authentication endpoints
    - path: apps/api/src/routes/extension/transform.ts
      purpose: Text-to-comic transformation endpoint for extension
    - path: packages/shared/src/browser-extension/types.ts
      purpose: Shared TypeScript interfaces for extension messages and data structures
  acceptance_criteria:
  - criterion: Extension allows users to select text on any webpage and transform
      it into a comic with one click
    verification: 'Manual test: select text on 5 different websites (news, blog, social
      media), click extension icon, verify comic generation API is called and preview
      is shown'
  - criterion: Extension authenticates users and syncs with main Morpheus platform
      account
    verification: 'Manual test: login through extension popup, verify JWT token stored
      in chrome.storage.sync, test API calls include proper authentication headers'
  - criterion: Extension works across Chrome, Edge, and Firefox with identical functionality
    verification: 'Automated test: run extension test suite on all three browsers
      using webextension-polyfill compatibility layer'
  - criterion: Content script performance does not degrade page load times by more
      than 50ms
    verification: 'Performance test: measure page load time with/without extension
      on 10 popular websites using Chrome DevTools Performance API'
  - criterion: Extension passes store review guidelines and security requirements
    verification: Complete security audit checklist, test all permissions are justified,
      verify CSP compliance, submit to Chrome Web Store test environment
  testing:
    unit_tests:
    - file: apps/browser-extension/src/__tests__/content-script.test.ts
      coverage_target: 90%
      scenarios:
      - Text selection extraction from various DOM structures
      - Message passing between content script and background
      - Error handling for CSP-blocked operations
      - Performance with large text selections
    - file: apps/browser-extension/src/__tests__/background.test.ts
      coverage_target: 85%
      scenarios:
      - API request proxying with authentication
      - Storage management for user preferences
      - Tab communication and activeTab permission handling
    integration_tests:
    - file: apps/browser-extension/src/__tests__/integration/extension-flow.test.ts
      scenarios:
      - Complete text-to-comic transformation flow
      - Authentication flow with OAuth redirect handling
      - Cross-browser compatibility with webextension-polyfill
    manual_testing:
    - step: Install unpacked extension in Chrome, select text on Wikipedia article,
        click extension icon
      expected: Popup opens with selected text preview and 'Create Comic' button
    - step: Click 'Create Comic' while logged out
      expected: Authentication prompt appears, redirects to Morpheus OAuth flow
    - step: Test extension on content-heavy site like Reddit or Twitter
      expected: No noticeable performance degradation, text selection works smoothly
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup browser extension app structure and webpack build configuration
      done: false
    - task: Implement Manifest V3 configuration with required permissions
      done: false
    - task: Create content script for text selection and DOM interaction
      done: false
    - task: Build background service worker for API communication and storage
      done: false
    - task: Develop React-based popup interface with authentication flow
      done: false
    - task: Integrate with existing Morpheus API through shared client package
      done: false
    - task: Add extension-specific API endpoints for authentication and transformation
      done: false
    - task: Implement cross-browser compatibility with webextension-polyfill
      done: false
    - task: Create comprehensive test suite covering unit and integration scenarios
      done: false
    - task: Package extension builds for Chrome Web Store and Firefox Add-ons submission
      done: false
- key: T78
  title: Extension Comics Viewer
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 3
  area: distribution
  dependsOn:
  - T77
  agent_notes:
    research_findings: '**Context:**

      This task involves building a browser extension that allows users to read comics
      purchased through the Morpheus platform directly in their browser. This is critical
      for M6 (Commerce & Distribution) as it provides a seamless reading experience
      post-purchase, potentially increasing user engagement and retention. The extension
      would integrate with the existing comic viewer from the storefront but as a
      standalone reading application, similar to how Kindle Cloud Reader works for
      eBooks.


      **Technical Approach:**

      Build a Manifest V3 browser extension using React + TypeScript with a popup
      interface and dedicated comic reader tab/window. Use the existing comic viewer
      components from the Next.js storefront as a foundation. Implement secure authentication
      via Supabase Auth with extension-specific session management. Create a dedicated
      comics sync service that fetches purchased comics metadata and handles offline
      caching using Chrome Storage API. Implement progressive loading for comic pages
      to optimize performance.


      **Dependencies:**

      - External: @types/chrome@^0.0.246, webextension-polyfill@^0.10.0, react@^18,
      @supabase/supabase-js@^2

      - Internal: Shared UI components from storefront, authentication utilities,
      comic viewer components, API client for Morpheus backend


      **Risks:**

      - Cross-origin security: Chrome''s strict CSP policies may block image loading
      from Supabase storage

      - Storage limitations: Chrome extension storage limits may impact offline comic
      caching

      - Authentication complexity: Managing Supabase sessions across extension contexts
      (popup, content script, background)

      - Performance issues: Large comic files may cause memory issues in extension
      context


      **Complexity Notes:**

      This is more complex than initially estimated due to browser extension security
      constraints, cross-origin policies, and the need to adapt existing React components
      to extension architecture. The authentication flow alone requires significant
      custom work to bridge Supabase Auth with Chrome extension APIs.


      **Key Files:**

      - apps/extension/: New workspace for the browser extension

      - apps/extension/src/popup/: React-based popup interface

      - apps/extension/src/background/: Service worker for API calls and auth

      - apps/extension/src/content/: Content scripts if needed

      - packages/ui/comic-viewer/: Extract and adapt existing comic viewer components

      - apps/api/src/routes/extension/: New API endpoints for extension-specific needs

      '
    design_decisions:
    - decision: Use Manifest V3 with React in popup and separate reader tab
      rationale: Manifest V3 is required for Chrome Web Store, React provides familiar
        development experience and component reuse from storefront
      alternatives_considered:
      - Vanilla JS popup
      - Single-page extension in popup only
      - Manifest V2
    - decision: Chrome Storage API for comic metadata caching with IndexedDB for large
        assets
      rationale: Chrome Storage syncs across devices, IndexedDB handles large comic
        page images efficiently
      alternatives_considered:
      - localStorage only
      - Chrome Storage for everything
      - External cloud sync
    - decision: Dedicated extension API endpoints with CORS-enabled authentication
      rationale: Extensions need specific CORS headers and may require different auth
        flows than web apps
      alternatives_considered:
      - Reuse existing API endpoints
      - Proxy through background script
      - Direct Supabase calls
    researched_at: '2026-02-07T19:15:49.101624'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:41:35.522044'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a new Turborepo workspace for the browser extension using React
      and TypeScript. Extract comic viewer components from the storefront into shared
      packages. Implement a Chrome extension with popup interface for library browsing
      and dedicated tabs for comic reading. Use Chrome Storage API for metadata sync
      and IndexedDB for comic page caching. Create extension-specific API endpoints
      in the Fastify backend with proper CORS configuration and Supabase Auth integration
      adapted for extension context.

      '
    external_dependencies:
    - name: '@types/chrome'
      version: ^0.0.246
      reason: TypeScript definitions for Chrome extension APIs
    - name: webextension-polyfill
      version: ^0.10.0
      reason: Cross-browser compatibility for extension APIs with Promise support
    - name: '@crxjs/vite-plugin'
      version: ^2.0.0
      reason: Vite plugin for building Chrome extensions with HMR support
    - name: idb
      version: ^7.1.1
      reason: IndexedDB wrapper for efficient comic page storage and retrieval
    files_to_modify:
    - path: packages/ui/src/comic-viewer/ComicViewer.tsx
      changes: Extract into extension-compatible component, remove Next.js dependencies
    - path: packages/auth/src/supabase.ts
      changes: Add extension-specific auth methods and session management
    - path: apps/api/src/routes/comics.ts
      changes: Add CORS headers for extension origin, extension-specific endpoints
    new_files:
    - path: apps/extension/manifest.json
      purpose: Chrome extension manifest V3 configuration
    - path: apps/extension/src/background/service-worker.ts
      purpose: Handle API calls, auth state, and cross-tab communication
    - path: apps/extension/src/popup/Popup.tsx
      purpose: Main popup interface for library browsing
    - path: apps/extension/src/reader/Reader.tsx
      purpose: Full-page comic reader component
    - path: apps/extension/src/services/storage.ts
      purpose: Chrome storage API wrapper and caching logic
    - path: apps/extension/src/services/comics-sync.ts
      purpose: Sync purchased comics metadata with backend
    - path: apps/extension/webpack.config.js
      purpose: Webpack configuration for extension build
    - path: packages/extension-auth/src/index.ts
      purpose: Shared authentication utilities for extension context
    - path: apps/api/src/routes/extension/auth.ts
      purpose: Extension-specific authentication endpoints
    - path: apps/api/src/routes/extension/comics.ts
      purpose: Extension-optimized comics API with metadata focus
  acceptance_criteria:
  - criterion: Extension successfully authenticates users with existing Morpheus accounts
    verification: 'Manual test: Install extension, click popup, login with test account,
      verify authentication state persists'
  - criterion: Users can view their purchased comics library in the extension popup
    verification: 'Manual test: After login, popup displays list of purchased comics
      with thumbnails and titles from API'
  - criterion: Comics open in dedicated reader tabs with full navigation controls
    verification: 'Manual test: Click comic from popup, new tab opens with reader
      showing pages, navigation arrows work'
  - criterion: Comic pages load progressively without blocking the UI
    verification: 'Performance test: Monitor network tab, verify pages load in chunks,
      UI remains responsive during loading'
  - criterion: Extension works offline for previously cached comics
    verification: 'Manual test: Read comic online, disconnect internet, reopen comic,
      verify pages still display'
  testing:
    unit_tests:
    - file: apps/extension/src/__tests__/auth.test.ts
      coverage_target: 90%
      scenarios:
      - Successful authentication flow
      - Token refresh handling
      - Logout cleanup
      - Authentication errors
    - file: apps/extension/src/__tests__/storage.test.ts
      coverage_target: 85%
      scenarios:
      - Comics metadata sync
      - Page caching and retrieval
      - Storage quota management
      - Cache invalidation
    integration_tests:
    - file: apps/extension/src/__tests__/integration/reader.test.ts
      scenarios:
      - Full comic reading flow
      - Cross-tab communication
      - API integration with auth
    manual_testing:
    - step: Install extension in Chrome dev mode
      expected: Extension appears in toolbar, popup opens on click
    - step: Test authentication flow end-to-end
      expected: Login redirects work, tokens stored securely
    - step: Verify comic reading experience
      expected: Smooth page navigation, proper image loading, responsive controls
    - step: Test offline functionality
      expected: Previously viewed comics work without internet
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup extension workspace and build configuration
      done: false
    - task: Extract and adapt comic viewer components to shared package
      done: false
    - task: Implement extension authentication service with Supabase integration
      done: false
    - task: Create popup interface with comics library display
      done: false
    - task: Build comic reader tab with progressive loading
      done: false
    - task: Implement Chrome storage integration and offline caching
      done: false
    - task: Add extension-specific API endpoints with CORS support
      done: false
    - task: Create background service worker for cross-tab communication
      done: false
    - task: Comprehensive testing across Chrome extension contexts
      done: false
    - task: Documentation and deployment preparation
      done: false
- key: T79
  title: WordPress Plugin
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p2
  effort: 5
  area: distribution
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      A WordPress plugin would allow Morpheus-generated comics to be embedded and
      sold directly within WordPress sites, expanding distribution beyond the native
      platform. This addresses the need to reach WordPress''s massive ecosystem (43%
      of all websites) where many authors, publishers, and content creators already
      have established audiences. The plugin would enable seamless integration of
      Morpheus comics into existing WordPress workflows, potentially driving significant
      user acquisition and revenue.


      **Technical Approach:**

      Build a WordPress plugin that communicates with Morpheus''s existing API infrastructure.
      The plugin should provide:

      - Widget/block for embedding comics in posts/pages

      - Commerce integration for selling comics directly

      - User authentication bridge to Morpheus accounts

      - Admin dashboard for managing comic libraries

      - Shortcode support for flexible placement


      Use WordPress REST API and Gutenberg block development standards. Leverage existing
      Morpheus API endpoints rather than duplicating logic. Follow WordPress plugin
      security and performance best practices.


      **Dependencies:**

      - External: WordPress 6.0+, WooCommerce (optional), WordPress REST API

      - Internal: Morpheus API authentication service, comic metadata APIs, payment
      processing service, user management system


      **Risks:**

      - WordPress version compatibility: Use WordPress coding standards and test across
      multiple versions

      - Security vulnerabilities: Implement proper nonce verification, sanitization,
      and capability checks

      - Performance impact: Cache API responses, lazy load images, optimize database
      queries

      - Plugin conflicts: Use unique prefixes, avoid global namespace pollution

      - Maintenance burden: WordPress updates frequently, requiring ongoing compatibility
      testing


      **Complexity Notes:**

      More complex than initially estimated due to WordPress''s unique architecture,
      security requirements, and the need to bridge two different authentication systems.
      The commerce integration adds significant complexity, especially if supporting
      both native Morpheus payments and WooCommerce integration.


      **Key Files:**

      - packages/wordpress-plugin/: New package for plugin development

      - apps/api/src/routes/wordpress/: WordPress-specific API endpoints

      - apps/api/src/services/wordpress-auth.ts: WordPress user authentication bridge

      - packages/shared/src/types/wordpress.ts: WordPress-specific type definitions

      '
    design_decisions:
    - decision: Build as a standalone WordPress plugin with API integration
      rationale: Maintains separation of concerns, leverages existing Morpheus infrastructure,
        follows WordPress best practices
      alternatives_considered:
      - WordPress theme integration
      - Headless WordPress approach
      - iFrame embedding solution
    - decision: Use Gutenberg blocks for comic embedding
      rationale: Modern WordPress standard, provides rich editing experience, better
        SEO than shortcodes
      alternatives_considered:
      - Shortcodes only
      - Classic editor widgets
      - Custom post types
    - decision: Optional WooCommerce integration for payments
      rationale: Many WordPress sites already use WooCommerce, provides familiar checkout
        experience
      alternatives_considered:
      - Morpheus-only payments
      - Multiple payment gateway integrations
      - PayPal/Stripe direct integration
    researched_at: '2026-02-07T19:16:09.944146'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:42:00.872828'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Develop a WordPress plugin that registers custom Gutenberg blocks for
      comic display and purchase. The plugin will authenticate with Morpheus APIs
      using OAuth or API keys, cache comic metadata locally, and provide shortcodes
      for backward compatibility. Commerce functionality will support both native
      Morpheus payments and optional WooCommerce integration. The plugin architecture
      will be modular with separate components for authentication, content display,
      and commerce.

      '
    external_dependencies:
    - name: '@wordpress/scripts'
      version: ^27.0.0
      reason: WordPress build tooling for modern plugin development
    - name: '@wordpress/block-editor'
      version: ^12.0.0
      reason: Gutenberg block development components
    - name: axios
      version: ^1.6.0
      reason: HTTP client for Morpheus API communication
    - name: '@wordpress/api-fetch'
      version: ^6.0.0
      reason: WordPress-native API communication
    files_to_modify:
    - path: apps/api/src/routes/index.ts
      changes: Add WordPress-specific route imports
    - path: apps/api/src/middleware/auth.ts
      changes: Add WordPress API key authentication method
    - path: packages/shared/src/types/index.ts
      changes: Export WordPress-specific types
    new_files:
    - path: packages/wordpress-plugin/morpheus-comics.php
      purpose: Main plugin file with headers and initialization
    - path: packages/wordpress-plugin/includes/class-morpheus-api.php
      purpose: Handle all Morpheus API communications and caching
    - path: packages/wordpress-plugin/includes/class-morpheus-auth.php
      purpose: Manage authentication bridge between WordPress and Morpheus
    - path: packages/wordpress-plugin/includes/class-morpheus-commerce.php
      purpose: Handle payment processing and WooCommerce integration
    - path: packages/wordpress-plugin/blocks/comic-display/index.js
      purpose: Gutenberg block for displaying comics
    - path: packages/wordpress-plugin/blocks/comic-display/block.json
      purpose: Block configuration and attributes
    - path: packages/wordpress-plugin/admin/class-morpheus-admin.php
      purpose: WordPress admin dashboard and settings
    - path: packages/wordpress-plugin/public/class-morpheus-public.php
      purpose: Frontend functionality and shortcodes
    - path: apps/api/src/routes/wordpress/auth.ts
      purpose: WordPress-specific authentication endpoints
    - path: apps/api/src/routes/wordpress/comics.ts
      purpose: WordPress-optimized comic data endpoints
    - path: apps/api/src/routes/wordpress/webhooks.ts
      purpose: Handle WordPress plugin webhooks and callbacks
    - path: apps/api/src/services/wordpress-auth.ts
      purpose: Service for managing WordPress user authentication bridge
    - path: packages/shared/src/types/wordpress.ts
      purpose: TypeScript definitions for WordPress integration
    - path: packages/wordpress-plugin/assets/css/morpheus-blocks.css
      purpose: Styling for comic blocks and frontend elements
    - path: packages/wordpress-plugin/assets/js/morpheus-frontend.js
      purpose: Frontend JavaScript for commerce and interactivity
  acceptance_criteria:
  - criterion: WordPress plugin successfully installs and activates on WordPress 6.0+
      sites
    verification: Install plugin via WordPress admin, verify activation without errors,
      check plugin appears in installed plugins list
  - criterion: Gutenberg block renders Morpheus comics with proper authentication
    verification: Add comic block to post, authenticate with Morpheus API key, verify
      comic displays with metadata and images
  - criterion: Commerce integration allows purchasing comics through WordPress site
    verification: Complete purchase flow from WordPress frontend, verify payment processing
      and user receives comic access
  - criterion: Plugin maintains performance standards with <2s page load impact
    verification: Load test WordPress pages with comic blocks, measure performance
      with WordPress query monitor
  - criterion: Authentication bridge allows WordPress users to access purchased Morpheus
      comics
    verification: WordPress user purchases comic, verify access syncs with Morpheus
      account, test login bridge functionality
  testing:
    unit_tests:
    - file: packages/wordpress-plugin/tests/phpunit/test-morpheus-api.php
      coverage_target: 85%
      scenarios:
      - API authentication success/failure
      - Comic metadata fetching and caching
      - User authentication bridge
      - Payment processing integration
    - file: packages/wordpress-plugin/tests/jest/blocks.test.js
      coverage_target: 80%
      scenarios:
      - Gutenberg block rendering
      - Block attribute validation
      - Frontend display logic
    integration_tests:
    - file: apps/api/src/__tests__/integration/wordpress.test.ts
      scenarios:
      - WordPress API endpoints authentication
      - Comic data synchronization flow
      - Payment webhook processing
    - file: packages/wordpress-plugin/tests/integration/test-woocommerce.php
      scenarios:
      - WooCommerce integration if installed
      - Fallback to native payments when WooCommerce unavailable
    manual_testing:
    - step: Install plugin on fresh WordPress site
      expected: Plugin activates without errors, settings page appears
    - step: Configure API credentials and add comic block to post
      expected: Comic displays with proper formatting and purchase button
    - step: Complete purchase flow as logged-out user
      expected: User prompted to create account, payment processes, comic access granted
    - step: Test plugin with popular WordPress themes
      expected: Comic blocks display correctly across different theme styles
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup WordPress plugin directory structure and build tooling
      done: false
    - task: Create main plugin file with proper WordPress headers and activation hooks
      done: false
    - task: Implement Morpheus API communication layer with caching
      done: false
    - task: Build Gutenberg block for comic display with admin interface
      done: false
    - task: Develop authentication bridge between WordPress and Morpheus accounts
      done: false
    - task: Implement commerce integration with payment processing
      done: false
    - task: Create WordPress admin settings and management interface
      done: false
    - task: Add shortcode support for backward compatibility
      done: false
    - task: Build corresponding API endpoints in Morpheus backend
      done: false
    - task: Implement comprehensive testing suite and documentation
      done: false
- key: T80
  title: Plugin Content Display
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p2
  effort: 3
  area: distribution
  dependsOn:
  - T79
  agent_notes:
    research_findings: '**Context:**

      Plugin Content Display enables third-party plugins to contribute content to
      the Morpheus storefront and dashboard. This allows external developers to extend
      the platform with custom comic formats, reader enhancements, analytics widgets,
      or commerce integrations. It''s essential for creating an extensible ecosystem
      where partners can add value without core platform changes.


      **Technical Approach:**

      Implement a secure plugin system using React''s dynamic component loading with
      iframe sandboxing for untrusted content. Use a plugin manifest system with strict
      CSP policies and a message-passing API for plugin-platform communication. Store
      plugin metadata in Supabase with version control and approval workflows. Use
      microfrontend patterns with module federation for isolated plugin rendering.


      **Dependencies:**

      - External: @module-federation/nextjs-mf, postmate, joi, helmet

      - Internal: auth service, content management system, storefront layout components,
      admin dashboard framework


      **Risks:**

      - Security vulnerabilities: Implement strict CSP, iframe sandboxing, and input
      validation

      - Performance degradation: Lazy loading, resource limits, and monitoring

      - Plugin compatibility: Version management and breaking change detection

      - Content quality control: Review process and automated scanning


      **Complexity Notes:**

      More complex than initially appears due to security requirements and the need
      for a complete plugin lifecycle management system. Requires careful architecture
      to balance extensibility with platform stability.


      **Key Files:**

      - packages/web/components/PluginRenderer.tsx: Core plugin display component

      - packages/api/src/routes/plugins.ts: Plugin management API

      - packages/database/migrations/: Plugin metadata schema

      - packages/web/pages/admin/plugins/: Plugin management dashboard

      '
    design_decisions:
    - decision: Use iframe-based sandboxing with postMessage communication
      rationale: Provides strongest security isolation while maintaining flexibility
        for plugin developers
      alternatives_considered:
      - Web Components
      - Direct React component loading
      - Server-side rendering
    - decision: Implement plugin manifest with strict validation
      rationale: Ensures plugin compatibility and enables automated security scanning
      alternatives_considered:
      - Runtime discovery
      - Convention-based loading
    researched_at: '2026-02-07T19:16:27.004125'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:42:25.889146'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a secure plugin architecture using iframe sandboxing for untrusted
      content and React portals for trusted plugins. Implement a plugin registry with
      manifest validation, version control, and approval workflows. Create a unified
      plugin API that allows content injection into predefined slots throughout the
      storefront and dashboard. Use module federation for performance-optimized loading
      and implement comprehensive monitoring for plugin health and security.

      '
    external_dependencies:
    - name: '@module-federation/nextjs-mf'
      version: ^8.0.0
      reason: Enable microfrontend architecture for plugin isolation
    - name: postmate
      version: ^1.5.2
      reason: Secure iframe communication between plugins and host
    - name: joi
      version: ^17.11.0
      reason: Plugin manifest validation and schema enforcement
    - name: helmet
      version: ^7.1.0
      reason: Enhanced CSP and security headers for plugin content
    - name: react-error-boundary
      version: ^4.0.11
      reason: Isolate plugin errors from main application
    files_to_modify:
    - path: packages/web/pages/_app.tsx
      changes: Add plugin context provider and CSP meta tags
    - path: packages/web/components/Layout/StorefrontLayout.tsx
      changes: Add plugin slot components for header, sidebar, footer
    - path: packages/web/pages/admin/index.tsx
      changes: Add plugin management navigation and dashboard widgets
    - path: packages/api/src/middleware/auth.ts
      changes: Add plugin API authentication and permission checks
    new_files:
    - path: packages/web/components/PluginRenderer.tsx
      purpose: Core component for rendering plugins in iframes with security policies
    - path: packages/web/components/PluginSlot.tsx
      purpose: Slot component for injecting plugin content into layouts
    - path: packages/web/hooks/usePluginMessaging.ts
      purpose: Hook for secure message passing between plugins and platform
    - path: packages/api/src/routes/plugins.ts
      purpose: REST API for plugin management, upload, approval, and activation
    - path: packages/api/src/services/PluginService.ts
      purpose: Business logic for plugin lifecycle management and validation
    - path: packages/api/src/services/PluginSecurityService.ts
      purpose: Security validation, CSP generation, and sandboxing policies
    - path: packages/database/migrations/20240115000000_create_plugins_tables.sql
      purpose: Database schema for plugin metadata, versions, and approvals
    - path: packages/web/pages/admin/plugins/index.tsx
      purpose: Plugin management dashboard for admins
    - path: packages/web/pages/admin/plugins/[id].tsx
      purpose: Individual plugin details and configuration page
    - path: packages/shared/types/plugin.ts
      purpose: TypeScript interfaces for plugin manifests and API responses
    - path: packages/api/src/validators/plugin-manifest.ts
      purpose: Joi schemas for validating plugin manifests and configurations
    - path: packages/web/utils/plugin-loader.ts
      purpose: Module federation utilities for dynamic plugin loading
  acceptance_criteria:
  - criterion: Plugin system can securely load and display third-party content in
      iframe sandboxes with CSP policies
    verification: Load test plugin with malicious scripts - verify iframe prevents
      XSS and CSP blocks unauthorized requests
  - criterion: Plugin registry supports manifest validation, version control, and
      approval workflows
    verification: Upload plugin with invalid manifest - verify validation errors.
      Check version history and approval status in admin dashboard
  - criterion: Plugins can inject content into predefined slots (storefront header,
      comic reader, dashboard widgets)
    verification: Install approved plugin - verify content appears in correct slots
      with proper styling isolation
  - criterion: Plugin performance monitoring tracks resource usage and loading times
      under 2s
    verification: Monitor plugin metrics dashboard - verify CPU/memory limits enforced
      and loading performance < 2000ms
  - criterion: Message-passing API enables secure plugin-platform communication
    verification: Test plugin API calls - verify authentication, rate limiting, and
      data validation work correctly
  testing:
    unit_tests:
    - file: packages/web/components/__tests__/PluginRenderer.test.tsx
      coverage_target: 90%
      scenarios:
      - Plugin loading and iframe creation
      - Security policy enforcement
      - Error boundary handling
      - Message passing validation
    - file: packages/api/src/__tests__/plugins.test.ts
      coverage_target: 85%
      scenarios:
      - Manifest validation
      - Plugin CRUD operations
      - Version management
      - Permission checks
    integration_tests:
    - file: packages/api/src/__tests__/integration/plugin-lifecycle.test.ts
      scenarios:
      - Complete plugin upload, approval, and activation flow
      - Plugin communication with platform APIs
      - Security policy enforcement across services
    - file: packages/web/__tests__/integration/plugin-display.test.ts
      scenarios:
      - Plugin rendering in different slot contexts
      - Multiple plugin interactions
    manual_testing:
    - step: Upload test plugin via admin dashboard
      expected: Plugin appears in pending approval queue with manifest details
    - step: Approve plugin and install on storefront
      expected: Plugin content renders in designated slots without breaking layout
    - step: Test plugin API communication
      expected: Plugin can fetch user data and display personalized content securely
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Create database schema and migrations for plugin metadata storage
      done: false
    - task: Implement PluginService with manifest validation and CRUD operations
      done: false
    - task: Build PluginRenderer component with iframe sandboxing and CSP policies
      done: false
    - task: Create plugin management API endpoints with authentication
      done: false
    - task: Develop admin dashboard for plugin approval and configuration
      done: false
    - task: Implement plugin slot system in storefront and dashboard layouts
      done: false
    - task: Build secure message-passing API for plugin-platform communication
      done: false
    - task: Add performance monitoring and resource limiting for plugins
      done: false
    - task: Create comprehensive test suite including security testing
      done: false
    - task: Write plugin development documentation and API reference
      done: false
- key: T81
  title: Social Sharing
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 2
  area: distribution
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Social sharing enables users to promote their generated comics across social
      platforms, driving organic growth and user acquisition for Morpheus. This is
      critical for viral marketing - when users share their AI-generated comics on
      Twitter, Instagram, Facebook, etc., it showcases the platform''s capabilities
      to potential customers. The feature should support both individual comic panels
      and full comic collections, with optimized images, engaging captions, and proper
      attribution/branding.


      **Technical Approach:**

      Implement a multi-platform social sharing system using Web Share API for native
      mobile sharing, with fallbacks to platform-specific URLs. Create optimized share
      images using Canvas API or server-side image generation. Use OpenGraph/Twitter
      Card meta tags for rich previews. Build a share modal component with customizable
      captions, platform selection, and tracking analytics. Integrate with existing
      comic viewer and dashboard components.


      **Dependencies:**

      - External: react-share (social platform URLs), html2canvas (image generation),
      next-seo (meta tags), canvas-confetti (celebration effects)

      - Internal: Comic viewer components, image optimization service, analytics tracking,
      user authentication, comic metadata APIs


      **Risks:**

      - Image generation performance: Pre-generate share images during comic creation
      to avoid real-time delays

      - Platform policy changes: Abstract sharing logic to easily adapt to API changes

      - Mobile compatibility issues: Extensively test Web Share API fallbacks across
      devices

      - Analytics tracking failures: Implement robust error handling and retry mechanisms


      **Complexity Notes:**

      Initially seems straightforward but complexity increases with optimized image
      generation, cross-platform compatibility, and analytics integration. The image
      optimization for different platform requirements (aspect ratios, file sizes)
      adds significant technical depth.


      **Key Files:**

      - apps/storefront/components/ShareModal.tsx: Main sharing interface

      - apps/storefront/lib/social-sharing.ts: Platform-specific sharing logic

      - apps/backend/routes/comics/share.ts: Share image generation API

      - packages/ui/components/ShareButton.tsx: Reusable share trigger component

      '
    design_decisions:
    - decision: Use Web Share API with platform-specific URL fallbacks
      rationale: Provides native mobile experience while ensuring compatibility across
        all devices and platforms
      alternatives_considered:
      - Platform-specific SDKs
      - Pure URL-based sharing
      - Third-party sharing widgets
    - decision: Generate optimized share images server-side using Canvas/Puppeteer
      rationale: Ensures consistent image quality, handles complex comic layouts,
        and reduces client-side performance impact
      alternatives_considered:
      - Client-side canvas generation
      - Static template images
      - Real-time screenshot capture
    researched_at: '2026-02-07T19:16:48.178462'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:42:50.072178'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a comprehensive social sharing system that generates optimized
      share images for each comic, provides a responsive share modal with platform
      selection, and tracks sharing analytics. Use Web Share API for mobile-first
      experience with URL-based fallbacks for desktop. Implement server-side image
      generation to create platform-optimized visuals with comic previews, branding,
      and call-to-action text. Integrate sharing triggers throughout the user journey
      - comic viewer, dashboard, and completion flows.

      '
    external_dependencies:
    - name: react-share
      version: ^5.0.3
      reason: Provides pre-built sharing URLs and components for major social platforms
    - name: html2canvas
      version: ^1.4.1
      reason: Client-side fallback for generating share images from DOM elements
    - name: puppeteer
      version: ^21.0.0
      reason: Server-side screenshot generation for high-quality share images
    - name: canvas
      version: ^2.11.2
      reason: Node.js canvas implementation for custom share image generation
    files_to_modify:
    - path: apps/storefront/src/components/ComicViewer.tsx
      changes: Add ShareButton component to toolbar, pass comic data to share modal
    - path: apps/storefront/src/pages/comics/[id].tsx
      changes: Add OpenGraph meta tags using next-seo, include share image URL in
        head
    - path: apps/backend/src/routes/comics/index.ts
      changes: Add share image generation endpoint, integrate with existing comic
        routes
    - path: packages/ui/src/components/index.ts
      changes: Export new ShareButton component
    new_files:
    - path: apps/storefront/src/components/ShareModal.tsx
      purpose: Main sharing interface with platform selection and custom captions
    - path: apps/storefront/src/lib/social-sharing.ts
      purpose: Platform-specific URL generation and Web Share API integration
    - path: apps/backend/src/services/share-image-generator.ts
      purpose: Canvas-based image generation with platform optimization
    - path: apps/backend/src/routes/comics/share.ts
      purpose: API endpoints for share image generation and analytics
    - path: packages/ui/src/components/ShareButton.tsx
      purpose: Reusable share trigger button with customizable styling
    - path: apps/storefront/src/hooks/useShareAnalytics.ts
      purpose: Custom hook for tracking share events and user engagement
    - path: apps/backend/src/types/social-sharing.ts
      purpose: TypeScript interfaces for share platforms, image specs, and analytics
        events
  acceptance_criteria:
  - criterion: Users can share individual comic panels and full collections via social
      platforms with optimized images
    verification: 'Manual test: Create comic → Click share → Verify Twitter/Facebook/Instagram
      links generate proper previews with comic imagery and metadata'
  - criterion: Web Share API works on mobile devices with URL fallbacks on desktop
    verification: 'Cross-browser testing: Mobile Safari/Chrome shows native share
      sheet, desktop shows custom modal with platform buttons'
  - criterion: Share images are generated with platform-specific optimization (1200x630
      for Facebook, 1024x512 for Twitter)
    verification: Inspect network requests to /api/comics/[id]/share-image endpoint,
      verify different aspect ratios returned based on platform parameter
  - criterion: Sharing analytics track platform, comic ID, and user engagement with
      >95% success rate
    verification: Check analytics dashboard after test shares, verify events logged
      in apps/backend/src/services/analytics.ts with proper metadata
  - criterion: Share modal loads within 2 seconds and image generation completes within
      5 seconds
    verification: 'Performance testing: Measure time from share button click to modal
      display, and API response time for share image generation'
  testing:
    unit_tests:
    - file: apps/storefront/src/__tests__/components/ShareModal.test.tsx
      coverage_target: 90%
      scenarios:
      - Modal opens with correct comic data
      - Platform selection updates share URLs
      - Custom caption editing
      - Analytics tracking on share clicks
      - Web Share API availability detection
    - file: apps/backend/src/__tests__/routes/comics/share.test.ts
      coverage_target: 85%
      scenarios:
      - Share image generation with valid comic ID
      - Platform-specific image optimization
      - Error handling for missing comics
      - Canvas rendering performance
    integration_tests:
    - file: apps/storefront/src/__tests__/integration/social-sharing.test.ts
      scenarios:
      - End-to-end share flow from comic viewer
      - Share URL generation and OpenGraph meta tags
      - Analytics event pipeline
    manual_testing:
    - step: Share comic on mobile Safari using Web Share API
      expected: Native iOS share sheet appears with comic preview image
    - step: Share comic on desktop Chrome via Facebook button
      expected: Facebook share dialog opens with optimized 1200x630 comic image and
        generated caption
    - step: Verify OpenGraph meta tags on shared comic URLs
      expected: Facebook debugger shows proper title, description, and image from
        comic metadata
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Set up dependencies (react-share, html2canvas, canvas-confetti)
      done: false
    - task: Create ShareButton UI component with platform icons
      done: false
    - task: Implement ShareModal with Web Share API detection and fallbacks
      done: false
    - task: Build server-side share image generation service with Canvas API
      done: false
    - task: Add share endpoints to comics API with platform-specific optimization
      done: false
    - task: Integrate sharing triggers into ComicViewer and dashboard components
      done: false
    - task: Implement analytics tracking for share events and engagement
      done: false
    - task: Add OpenGraph meta tags to shared comic pages
      done: false
    - task: Cross-platform testing on mobile and desktop browsers
      done: false
    - task: Performance optimization and error handling
      done: false
- key: T82
  title: Analytics Tracking
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 2
  area: distribution
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      Analytics tracking is crucial for a commerce platform to understand user behavior,
      conversion funnels, revenue attribution, and product performance. For Morpheus,
      this means tracking how users discover comics, engage with transformation features,
      complete purchases, and consume content. This data drives product decisions,
      marketing optimization, and revenue growth strategies.


      **Technical Approach:**

      Implement a multi-layered analytics architecture:

      - Client-side tracking with privacy-first approach using PostHog or Mixpanel

      - Server-side event tracking for critical business events (purchases, transformations)

      - Custom analytics service for internal metrics (processing times, AI costs,
      success rates)

      - Real-time dashboards for business metrics

      - GDPR/CCPA compliant data collection with consent management


      **Dependencies:**

      - External: PostHog SDK, Google Analytics 4, Stripe webhooks for revenue tracking

      - Internal: User authentication system, payment processing, transformation pipeline
      events, Supabase for event storage


      **Risks:**

      - Privacy compliance: Implement consent banners, data anonymization, and opt-out
      mechanisms

      - Performance impact: Use lazy loading, event batching, and CDN delivery for
      tracking scripts

      - Data accuracy: Ensure server-side validation of critical events to prevent
      client-side manipulation

      - Cost escalation: Monitor API usage and implement sampling for high-volume
      events


      **Complexity Notes:**

      More complex than initially estimated due to privacy regulations and the need
      for both user behavior tracking and ML pipeline analytics. Requires coordination
      between frontend, backend, and ML services.


      **Key Files:**

      - apps/dashboard/lib/analytics.ts: Client-side tracking utilities

      - apps/storefront/components/Analytics.tsx: E-commerce tracking wrapper

      - apps/backend/src/services/analytics.ts: Server-side event processing

      - packages/shared/types/analytics.ts: Event schema definitions

      '
    design_decisions:
    - decision: Use PostHog as primary analytics platform with custom backend events
      rationale: PostHog offers privacy compliance, real-time analytics, and self-hosted
        options while supporting both product analytics and feature flags
      alternatives_considered:
      - Google Analytics + Mixpanel
      - Custom analytics with ClickHouse
      - Amplitude
    - decision: Implement event-driven architecture with Supabase real-time for internal
        dashboards
      rationale: Leverages existing Supabase infrastructure and provides real-time
        updates for business-critical metrics
      alternatives_considered:
      - Redis Streams
      - Custom WebSocket service
      - Polling-based updates
    researched_at: '2026-02-07T19:17:06.519081'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:43:13.510564'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a unified analytics layer that captures both user interactions
      and system events. Client-side tracking handles user behavior with privacy controls,
      while server-side events ensure accuracy for business metrics. Implement custom
      dashboards for transformation analytics (success rates, processing times, costs)
      alongside standard e-commerce metrics (conversion rates, revenue, user retention).

      '
    external_dependencies:
    - name: posthog-js
      version: ^1.96.0
      reason: Client-side analytics tracking with privacy features
    - name: posthog-node
      version: ^3.6.0
      reason: Server-side event tracking for backend services
    - name: '@vercel/analytics'
      version: ^1.1.0
      reason: Performance and Core Web Vitals tracking for Next.js apps
    - name: react-gtag
      version: ^1.0.1
      reason: Google Analytics integration for marketing attribution
    files_to_modify:
    - path: apps/storefront/pages/_app.tsx
      changes: Add Analytics provider wrapper and consent management
    - path: apps/dashboard/pages/_app.tsx
      changes: Add internal analytics tracking for admin actions
    - path: apps/backend/src/routes/payments.ts
      changes: Add server-side purchase event tracking
    - path: apps/backend/src/routes/transformations.ts
      changes: Add transformation analytics events
    - path: apps/backend/src/middleware/auth.ts
      changes: Add user authentication events
    new_files:
    - path: packages/shared/types/analytics.ts
      purpose: Shared TypeScript interfaces for analytics events and schema validation
    - path: apps/backend/src/services/analytics.ts
      purpose: Server-side analytics service with event processing, validation, and
        external API integration
    - path: apps/dashboard/lib/analytics.ts
      purpose: Client-side analytics utilities with privacy controls and PostHog integration
    - path: apps/storefront/components/Analytics.tsx
      purpose: E-commerce specific tracking components and hooks
    - path: apps/storefront/components/ConsentBanner.tsx
      purpose: GDPR/CCPA compliant consent management UI
    - path: apps/dashboard/pages/admin/analytics.tsx
      purpose: Real-time analytics dashboard for business metrics
    - path: apps/backend/src/routes/analytics.ts
      purpose: Analytics API endpoints for dashboard data and user preferences
    - path: apps/backend/src/jobs/analytics-aggregation.ts
      purpose: Background job for aggregating analytics data and generating reports
    - path: packages/shared/config/analytics.ts
      purpose: Analytics configuration constants and environment variables
  acceptance_criteria:
  - criterion: Client-side analytics tracks user interactions (page views, clicks,
      form submissions) with privacy consent management
    verification: Check browser developer tools for PostHog events, verify consent
      banner appears, test opt-out functionality
  - criterion: Server-side analytics captures critical business events (purchases,
      transformations, user registrations) with 100% accuracy
    verification: Compare server-side event logs with database records for purchases
      and transformations over 24h period
  - criterion: Real-time analytics dashboard displays key metrics (revenue, conversion
      rates, transformation success rates) with <5 second data freshness
    verification: Navigate to /admin/analytics, perform test purchase, verify metrics
      update within 5 seconds
  - criterion: GDPR/CCPA compliance implemented with data anonymization, consent management,
      and user data deletion
    verification: Test consent banner, verify anonymized IP addresses in logs, test
      data deletion API endpoint
  - criterion: Analytics performance impact <100ms additional page load time and <1MB
      additional bundle size
    verification: Run Lighthouse performance audit before/after, measure bundle size
      with webpack-bundle-analyzer
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/analytics.test.ts
      coverage_target: 90%
      scenarios:
      - Event validation and sanitization
      - Batch event processing
      - Privacy compliance filtering
      - Event schema validation
    - file: apps/dashboard/lib/__tests__/analytics.test.ts
      coverage_target: 85%
      scenarios:
      - Client-side event tracking
      - Consent management
      - Event batching and retry logic
    integration_tests:
    - file: apps/backend/src/__tests__/integration/analytics-flow.test.ts
      scenarios:
      - End-to-end purchase tracking (client -> server -> dashboard)
      - Transformation analytics pipeline
      - Privacy consent flow integration
    - file: apps/storefront/__tests__/integration/ecommerce-tracking.test.ts
      scenarios:
      - Shopping cart events
      - Purchase completion tracking
      - User journey tracking
    manual_testing:
    - step: Complete a purchase flow while monitoring analytics events
      expected: Events logged in PostHog, server logs, and real-time dashboard
    - step: Test GDPR consent banner and opt-out functionality
      expected: No tracking events sent after opt-out, consent preferences saved
    - step: Monitor transformation analytics during AI comic processing
      expected: Success rates, processing times, and costs tracked accurately
  estimates:
    development: 6
    code_review: 1
    testing: 2
    documentation: 1
    total: 10
  progress:
    status: not-started
    checklist:
    - task: Setup PostHog account and configure SDK integration
      done: false
    - task: Create shared analytics types and event schema definitions
      done: false
    - task: Implement server-side analytics service with event validation
      done: false
    - task: Build client-side analytics utilities with privacy controls
      done: false
    - task: Create GDPR-compliant consent management system
      done: false
    - task: Integrate analytics tracking into storefront purchase flow
      done: false
    - task: Add transformation pipeline analytics events
      done: false
    - task: Build real-time analytics dashboard with key business metrics
      done: false
    - task: Implement analytics data aggregation background jobs
      done: false
    - task: Add comprehensive test coverage and performance monitoring
      done: false
- key: T83
  title: Stripe Integration
  type: Feature
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p0
  effort: 5
  area: ecommerce
  dependsOn:
  - T25
  agent_notes:
    research_findings: "**Context:**\nStripe integration is crucial for M6 Commerce\
      \ & Distribution as it enables Morpheus to monetize the novel-to-comic transformation\
      \ platform. This allows users to purchase comic generation credits, subscribe\
      \ to premium tiers, or buy finished comic products. Without payment processing,\
      \ the platform cannot generate revenue or offer tiered services based on usage/quality\
      \ levels.\n\n**Technical Approach:**\n- Use Stripe's official Node.js SDK with\
      \ TypeScript bindings for backend payment processing\n- Implement Stripe Elements/Payment\
      \ Element in Next.js frontend for secure card collection\n- Create webhook endpoints\
      \ in Fastify to handle payment status updates and subscription changes\n- Store\
      \ payment metadata (customer_id, subscription_id, payment_intent_id) in Supabase\n\
      - Use Stripe's test mode during development with proper environment variable\
      \ configuration\n- Implement proper error handling for failed payments, expired\
      \ cards, and insufficient funds\n- Add payment event logging and monitoring\
      \ for debugging and compliance\n\n**Dependencies:**\n- External: stripe (Node.js\
      \ SDK), @stripe/stripe-js (frontend), @stripe/react-stripe-js (React components)\n\
      - Internal: Authentication service (user identification), Credit/subscription\
      \ management system, Email notification service, Admin dashboard for payment\
      \ monitoring\n\n**Risks:**\n- PCI Compliance: Mitigate by using Stripe Elements\
      \ (never handle raw card data)\n- Webhook security: Verify webhook signatures\
      \ to prevent fraud/replay attacks  \n- Failed payment handling: Implement robust\
      \ retry logic and user notification systems\n- Currency/taxation: Start with\
      \ USD only, plan for international expansion later\n- Subscription edge cases:\
      \ Handle failed renewals, plan changes, cancellations gracefully\n\n**Complexity\
      \ Notes:**\nThis is moderately complex due to multiple integration points (frontend\
      \ payment forms, backend processing, webhook handling, database updates) and\
      \ the critical nature of payment processing. The asynchronous nature of webhooks\
      \ adds complexity for state synchronization. However, Stripe's excellent documentation\
      \ and SDKs reduce implementation complexity significantly.\n\n**Key Files:**\n\
      - apps/backend/src/routes/payments/: Payment API routes and webhook handlers\n\
      - apps/backend/src/services/stripe.ts: Stripe service wrapper with error handling\n\
      - apps/dashboard/src/components/payments/: Payment forms and subscription management\
      \ UI\n- apps/backend/src/types/stripe.ts: TypeScript interfaces for Stripe objects\n\
      - packages/database/schema.sql: Add payment_customers, subscriptions, payment_intents\
      \ tables\n"
    design_decisions:
    - decision: Use Stripe Payment Element instead of individual Elements
      rationale: Payment Element provides better UX with built-in payment method selection,
        reduces frontend complexity, and handles international payment methods automatically
      alternatives_considered:
      - Individual Stripe Elements (Card, IBAN, etc.)
      - Stripe Checkout (redirect flow)
      - Alternative payment processor (PayPal, Square)
    - decision: Implement webhook-first architecture for payment status updates
      rationale: Webhooks provide reliable, asynchronous notification of payment events
        even if user closes browser. Essential for subscription billing and failed
        payment handling
      alternatives_considered:
      - Polling Stripe API
      - Frontend-only payment confirmation
      - Hybrid approach with immediate + webhook confirmation
    - decision: Store Stripe customer ID and subscription metadata in Supabase
      rationale: Enables fast user payment history queries, offline payment status
        checks, and reduces Stripe API calls. Maintains referential integrity with
        user accounts
      alternatives_considered:
      - Query Stripe API on-demand
      - Cache payment data in Redis
      - Separate payments microservice
    researched_at: '2026-02-07T19:17:32.140534'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:43:36.353242'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a payment service layer in the Fastify backend that wraps Stripe
      SDK operations with proper error handling and logging. Build React payment components
      using Stripe Elements for secure card collection in the dashboard. Implement
      webhook endpoints to process payment confirmations, failed charges, and subscription
      updates asynchronously. Store essential payment metadata in Supabase while keeping
      Stripe as the source of truth for sensitive payment data.

      '
    external_dependencies:
    - name: stripe
      version: ^14.0.0
      reason: Official Stripe Node.js SDK for backend payment processing and webhook
        handling
    - name: '@stripe/stripe-js'
      version: ^2.0.0
      reason: Stripe JavaScript SDK for frontend payment element initialization
    - name: '@stripe/react-stripe-js'
      version: ^2.0.0
      reason: React components and hooks for Stripe Elements integration
    - name: micro
      version: ^10.0.1
      reason: Lightweight HTTP handler for processing Stripe webhooks with raw body
        access
    files_to_modify:
    - path: apps/backend/package.json
      changes: Add stripe dependency
    - path: apps/dashboard/package.json
      changes: Add @stripe/stripe-js and @stripe/react-stripe-js dependencies
    - path: packages/database/schema.sql
      changes: Add payment_customers, payment_intents, webhook_events tables
    - path: apps/backend/src/app.ts
      changes: Register payment routes and webhook handlers
    - path: apps/backend/.env.example
      changes: Add Stripe API keys and webhook secret variables
    new_files:
    - path: apps/backend/src/services/stripe.ts
      purpose: Stripe SDK wrapper with error handling and logging
    - path: apps/backend/src/routes/payments/index.ts
      purpose: Payment API routes (create payment intent, get payment status)
    - path: apps/backend/src/routes/payments/webhooks.ts
      purpose: Stripe webhook handlers for payment events
    - path: apps/backend/src/types/stripe.ts
      purpose: TypeScript interfaces for payment data structures
    - path: apps/dashboard/src/components/payments/PaymentForm.tsx
      purpose: Stripe Elements payment form component
    - path: apps/dashboard/src/components/payments/CreditPurchase.tsx
      purpose: Credit purchase flow with payment integration
    - path: apps/dashboard/src/hooks/usePayments.ts
      purpose: React hook for payment operations and state management
    - path: apps/backend/src/middleware/stripe-webhook.ts
      purpose: Webhook signature verification middleware
    - path: apps/backend/src/utils/payment-logger.ts
      purpose: Structured logging for payment events and compliance
  acceptance_criteria:
  - criterion: Users can successfully purchase comic generation credits using Stripe
      payment forms
    verification: 'Manual test: Complete credit purchase flow in dashboard, verify
      credits added to user account and payment recorded in Stripe'
  - criterion: Webhook handlers properly sync payment status changes from Stripe to
      Supabase
    verification: 'Integration test: Trigger webhook events in Stripe test mode, verify
      database updates match payment status'
  - criterion: Failed payments display appropriate error messages and don't charge
      users
    verification: 'Manual test: Use Stripe test cards for declined payments, verify
      user sees error and no charge appears in Stripe dashboard'
  - criterion: Payment processing handles concurrent requests without data corruption
    verification: 'Load test: Send 10+ simultaneous payment requests, verify all succeed
      or fail gracefully with proper error handling'
  - criterion: All payment events are logged with sufficient detail for debugging
      and compliance
    verification: Check logs contain payment_intent_id, user_id, amount, timestamp
      for all payment operations
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/stripe.test.ts
      coverage_target: 90%
      scenarios:
      - Create payment intent success
      - Handle Stripe API errors
      - Webhook signature verification
      - Payment metadata formatting
    - file: apps/backend/src/__tests__/routes/payments.test.ts
      coverage_target: 85%
      scenarios:
      - Create payment endpoint validation
      - Webhook processing logic
      - Authentication required checks
    integration_tests:
    - file: apps/backend/src/__tests__/integration/stripe-flow.test.ts
      scenarios:
      - Complete payment flow with webhook confirmation
      - Failed payment handling and cleanup
      - User credit balance updates after payment
    manual_testing:
    - step: Test successful credit purchase with valid card (4242 4242 4242 4242)
      expected: Payment succeeds, credits added, confirmation email sent
    - step: Test declined payment with test card (4000 0000 0000 0002)
      expected: Payment fails gracefully with user-friendly error message
    - step: Test webhook delivery failure recovery
      expected: System handles webhook retries and eventual consistency
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Set up Stripe account and obtain test API keys
      done: false
    - task: Create database schema for payment tracking tables
      done: false
    - task: Implement Stripe service wrapper with core payment operations
      done: false
    - task: Build payment API routes for creating payment intents
      done: false
    - task: Create webhook endpoints with signature verification
      done: false
    - task: Develop React payment form components using Stripe Elements
      done: false
    - task: Integrate payment flow with user credit system
      done: false
    - task: Add comprehensive error handling and user feedback
      done: false
    - task: Implement payment event logging and monitoring
      done: false
    - task: Write unit and integration tests for payment flows
      done: false
    - task: Manual testing with Stripe test cards and scenarios
      done: false
    - task: Security review focusing on PCI compliance and webhook verification
      done: false
- key: T84
  title: Stripe Webhook Handler
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p0
  effort: 3
  area: ecommerce
  dependsOn:
  - T83
  agent_notes:
    research_findings: '**Context:**

      Stripe webhook handlers are essential for maintaining data consistency between
      Stripe''s payment system and Morpheus''s database. When users purchase comics,
      subscribe to services, or payment states change, Stripe sends webhooks to notify
      our system. Without proper webhook handling, we''d have inconsistent payment
      states, failed order fulfillment, and poor user experience. This is critical
      for the commerce milestone as it enables real-time payment processing, subscription
      management, and automated comic delivery.


      **Technical Approach:**

      Use Fastify''s raw body parsing with Stripe''s webhook signature verification
      for security. Implement idempotent event processing using Supabase to store
      processed webhook IDs. Create dedicated handlers for each event type (payment_intent.succeeded,
      customer.subscription.updated, etc.) following the command pattern. Use Fastify''s
      async/await with proper error handling and dead letter queues for failed webhooks.
      Leverage TypeScript''s strict typing with Stripe''s official types for type
      safety.


      **Dependencies:**

      - External: stripe ^14.0.0, @fastify/raw-body ^4.0.0, zod ^3.22.0 for validation

      - Internal: Supabase client, order processing service, user management service,
      email notification service, comic delivery pipeline


      **Risks:**

      - Duplicate processing: Use idempotency keys and database constraints to prevent
      duplicate order fulfillment

      - Webhook replay attacks: Implement proper signature verification and timestamp
      validation

      - Event ordering issues: Design handlers to be order-independent where possible,
      use database transactions

      - Performance bottlenecks: Queue heavy operations (image processing, email)
      asynchronously

      - Failed webhook delivery: Implement retry logic and monitoring for webhook
      failures


      **Complexity Notes:**

      More complex than initially estimated due to the need for robust idempotency,
      error handling, and integration with multiple Morpheus services (orders, users,
      comics, notifications). The variety of Stripe events and their interdependencies
      adds significant complexity.


      **Key Files:**

      - apps/api/src/routes/webhooks/stripe.ts: Main webhook endpoint

      - apps/api/src/services/stripe-webhook-handler.ts: Event processing logic

      - apps/api/src/types/stripe-events.ts: TypeScript types for webhook payloads

      - packages/database/src/schema.sql: Webhook events tracking table

      '
    design_decisions:
    - decision: Use event-driven architecture with dedicated handlers per event type
      rationale: Provides separation of concerns, easier testing, and allows for independent
        scaling of different webhook types
      alternatives_considered:
      - Single monolithic handler
      - Queue-based processing with workers
    - decision: Store webhook events in database for idempotency and audit trail
      rationale: Prevents duplicate processing, enables debugging, and provides audit
        compliance for financial transactions
      alternatives_considered:
      - In-memory cache
      - Redis-based deduplication
    researched_at: '2026-02-07T19:17:52.840002'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:43:57.723104'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a Fastify route at /webhooks/stripe that verifies webhook signatures
      using Stripe''s SDK. Parse the raw request body and route events to specific
      handlers based on event type. Each handler will be idempotent by checking against
      a webhook_events table in Supabase before processing. Use database transactions
      to ensure consistency between webhook processing and business logic updates.
      Queue heavy operations like email notifications and comic delivery asynchronously.

      '
    external_dependencies:
    - name: stripe
      version: ^14.0.0
      reason: Official Stripe SDK for webhook signature verification and type definitions
    - name: '@fastify/raw-body'
      version: ^4.0.0
      reason: Required to access raw request body for Stripe signature verification
    - name: zod
      version: ^3.22.0
      reason: Runtime validation of webhook payloads to ensure data integrity
    files_to_modify:
    - path: apps/api/src/app.ts
      changes: Register @fastify/raw-body plugin and webhook route
    - path: packages/database/src/migrations/000X_webhook_events.sql
      changes: Create webhook_events table for idempotency tracking
    - path: apps/api/src/config/env.ts
      changes: Add STRIPE_WEBHOOK_SECRET environment variable validation
    new_files:
    - path: apps/api/src/routes/webhooks/stripe.ts
      purpose: Main webhook endpoint with signature verification and event routing
    - path: apps/api/src/services/stripe-webhook-handler.ts
      purpose: Core webhook processing logic with idempotency and error handling
    - path: apps/api/src/services/webhook-handlers/index.ts
      purpose: Export all individual event handlers
    - path: apps/api/src/services/webhook-handlers/payment-intent.ts
      purpose: Handle payment_intent.* events for order processing
    - path: apps/api/src/services/webhook-handlers/subscription.ts
      purpose: Handle customer.subscription.* events for subscription management
    - path: apps/api/src/services/webhook-handlers/invoice.ts
      purpose: Handle invoice.* events for billing notifications
    - path: apps/api/src/types/stripe-events.ts
      purpose: TypeScript types and Zod schemas for webhook event validation
    - path: apps/api/src/utils/webhook-idempotency.ts
      purpose: Utility functions for idempotency key generation and checking
    - path: apps/api/src/middleware/webhook-auth.ts
      purpose: Stripe webhook signature verification middleware
  acceptance_criteria:
  - criterion: Webhook endpoint successfully receives and verifies Stripe webhook
      signatures
    verification: 'Test with stripe CLI: `stripe listen --forward-to localhost:3000/webhooks/stripe`
      and trigger test events'
  - criterion: Payment successful events create orders and deliver comics without
      duplicates
    verification: Send duplicate payment_intent.succeeded events and verify only one
      order is created in database
  - criterion: Subscription events properly update user subscription status and permissions
    verification: Trigger customer.subscription.updated event and verify user access
      changes in database
  - criterion: Failed webhook processing is logged and queued for retry
    verification: Simulate handler failure and verify error logging and retry queue
      entry
  - criterion: All webhook events are idempotent and can be safely replayed
    verification: Replay the same webhook event multiple times and verify system state
      remains consistent
  testing:
    unit_tests:
    - file: apps/api/src/__tests__/services/stripe-webhook-handler.test.ts
      coverage_target: 90%
      scenarios:
      - Event signature verification success/failure
      - Idempotency key handling
      - Each event type handler logic
      - Database transaction rollback on errors
      - Invalid event payload handling
    - file: apps/api/src/__tests__/routes/webhooks/stripe.test.ts
      coverage_target: 85%
      scenarios:
      - Valid webhook request processing
      - Invalid signature rejection
      - Malformed payload handling
      - Rate limiting behavior
    integration_tests:
    - file: apps/api/src/__tests__/integration/stripe-webhooks.test.ts
      scenarios:
      - End-to-end payment flow with order creation
      - Subscription lifecycle management
      - Failed payment handling and user notification
      - Comic delivery pipeline trigger
    manual_testing:
    - step: Configure Stripe webhook endpoint in dashboard pointing to staging environment
      expected: Webhook events appear in logs with successful processing
    - step: Complete test purchase through Stripe checkout
      expected: Order created, comic delivered, confirmation email sent
    - step: Cancel subscription through Stripe dashboard
      expected: User access revoked, cancellation email sent
  estimates:
    development: 4
    code_review: 1
    testing: 1.5
    documentation: 0.5
    total: 7
  progress:
    status: not-started
    checklist:
    - task: Setup Stripe webhook configuration and environment variables
      done: false
    - task: Create database migration for webhook_events tracking table
      done: false
    - task: Implement webhook signature verification middleware
      done: false
    - task: Create main webhook endpoint with event routing
      done: false
    - task: Implement individual event handlers (payment_intent, subscription, invoice)
      done: false
    - task: Add idempotency checking and database transaction handling
      done: false
    - task: Implement error handling, logging, and retry queue integration
      done: false
    - task: Create comprehensive unit and integration tests
      done: false
    - task: Add monitoring and alerting for webhook failures
      done: false
    - task: Documentation and deployment configuration
      done: false
- key: T85
  title: Storefront UI
  type: Feature
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p0
  effort: 5
  area: ecommerce
  dependsOn: []
  agent_notes:
    research_findings: '**Context:**

      The Storefront UI is the customer-facing interface where users browse and purchase
      transformed comics. This is distinct from the admin dashboard and serves as
      the primary revenue-generating interface. Given the creative nature of novel-to-comic
      transformations, the storefront needs to showcase visual content effectively,
      support digital product sales, and provide a smooth purchasing experience for
      comic enthusiasts.


      **Technical Approach:**

      Build a Next.js 16 app router-based storefront with server-side rendering for
      SEO. Use React Server Components for performance, implement progressive image
      loading for comic previews, and integrate with Stripe/payment providers. Follow
      headless commerce patterns with Supabase as the product catalog backend. Implement
      comic-specific UI patterns like panel previews, reading samples, and visual
      search/filtering.


      **Dependencies:**

      - External: @stripe/stripe-js, next-auth, framer-motion, react-query/tanstack-query,
      next/image optimization

      - Internal: Shared component library from dashboard, Supabase client utilities,
      comic metadata services, user authentication system


      **Risks:**

      - Image loading performance: implement progressive loading, WebP conversion,
      and CDN caching

      - SEO for dynamic comic content: use Next.js metadata API and structured data

      - Payment security: leverage Stripe''s secure checkout, avoid handling sensitive
      data

      - Mobile responsiveness for comic viewing: extensive testing needed for touch
      interactions


      **Complexity Notes:**

      Higher complexity than typical e-commerce due to visual content requirements.
      Comic previews, reading experiences, and visual search add significant UI complexity.
      However, digital-only products simplify inventory management and shipping logic.


      **Key Files:**

      - apps/storefront/: New Next.js application

      - packages/ui/: Shared components for comic display

      - packages/database/: Product catalog schemas

      - apps/storefront/app/comics/[id]/: Individual comic pages

      '
    design_decisions:
    - decision: Next.js App Router with React Server Components
      rationale: Optimal SEO for comic discovery, better performance for image-heavy
        content, streaming UI for better UX
      alternatives_considered:
      - SPA with client-side routing
      - Static site generation only
    - decision: Stripe Checkout for payments
      rationale: Industry standard, handles PCI compliance, supports digital products
        well, good Next.js integration
      alternatives_considered:
      - PayPal only
      - Custom payment processing
      - Shopify headless
    - decision: Progressive image loading with Next.js Image
      rationale: Critical for comic preview performance, built-in optimization, responsive
        images
      alternatives_considered:
      - Third-party image CDN
      - Custom lazy loading
      - Full resolution loading
    researched_at: '2026-02-07T19:18:14.642391'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:44:25.468227'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a headless commerce storefront using Next.js 16 App Router
      with server-side rendering for comic catalog pages. Use React Server Components
      to fetch product data server-side while leveraging client components for interactive
      features like comic readers and shopping cart. Integrate Stripe for secure payment
      processing and implement progressive image loading for comic previews. Build
      responsive comic reading experiences optimized for both desktop and mobile viewing.

      '
    external_dependencies:
    - name: '@stripe/stripe-js'
      version: ^3.0.0
      reason: Secure payment processing for digital comic sales
    - name: '@tanstack/react-query'
      version: ^5.0.0
      reason: Client-side data fetching, caching for cart and user state
    - name: framer-motion
      version: ^11.0.0
      reason: Smooth animations for comic panel transitions and UI interactions
    - name: react-intersection-observer
      version: ^9.0.0
      reason: Efficient lazy loading for comic grid and image galleries
    - name: next-auth
      version: ^4.24.0
      reason: User authentication integration with dashboard accounts
    - name: sharp
      version: ^0.33.0
      reason: Image optimization for comic previews and thumbnails
    new_files:
    - path: apps/storefront/next.config.js
      purpose: Next.js configuration with image optimization and Stripe domains
    - path: apps/storefront/app/layout.tsx
      purpose: Root layout with shared navigation and cart context
    - path: apps/storefront/app/page.tsx
      purpose: Homepage with featured comics and search
    - path: apps/storefront/app/comics/page.tsx
      purpose: Comic catalog with search/filter/pagination
    - path: apps/storefront/app/comics/[id]/page.tsx
      purpose: Individual comic detail page with purchase options
    - path: apps/storefront/app/comics/[id]/read/page.tsx
      purpose: Comic reader for purchased content
    - path: apps/storefront/app/cart/page.tsx
      purpose: Shopping cart review and modification
    - path: apps/storefront/app/checkout/page.tsx
      purpose: Stripe checkout integration
    - path: apps/storefront/app/api/webhooks/stripe/route.ts
      purpose: Handle Stripe webhook events for payment processing
    - path: apps/storefront/src/components/comic-card.tsx
      purpose: Reusable comic display component for catalog
    - path: apps/storefront/src/components/comic-reader.tsx
      purpose: Full-screen comic reading interface
    - path: apps/storefront/src/components/cart-provider.tsx
      purpose: React context for cart state management
    - path: apps/storefront/src/components/search-filters.tsx
      purpose: Comic search and filtering UI
    - path: apps/storefront/src/lib/stripe-client.ts
      purpose: Stripe client configuration and utilities
    - path: apps/storefront/src/lib/cart-utils.ts
      purpose: Cart calculation and persistence utilities
    - path: apps/storefront/src/lib/comic-api.ts
      purpose: API client for comic catalog and user library
    - path: apps/storefront/src/hooks/use-cart.ts
      purpose: Custom hook for cart operations
    - path: apps/storefront/src/hooks/use-comic-reader.ts
      purpose: Custom hook for reader navigation and controls
    - path: apps/storefront/tailwind.config.js
      purpose: Tailwind configuration for storefront styling
    - path: apps/storefront/package.json
      purpose: Dependencies including Next.js 16, Stripe, Framer Motion
    files_to_modify:
    - path: packages/database/src/schema/comics.ts
      changes: Add storefront-specific fields like featured status, preview panels
    - path: packages/ui/src/components/index.ts
      changes: Export shared components for comic display and reading
    - path: packages/database/src/schema/users.ts
      changes: Add user library/purchases relationship tables
  acceptance_criteria:
  - criterion: Storefront displays comic catalog with search, filtering, and pagination
    verification: Navigate to /comics, verify search works, filters by genre/price
      work, pagination loads more comics
  - criterion: Individual comic pages show preview panels, metadata, and purchase
      options
    verification: Visit /comics/[id], verify preview images load progressively, metadata
      displays, buy button works
  - criterion: Shopping cart and checkout flow completes successfully with Stripe
    verification: Add items to cart, proceed to checkout, complete test purchase with
      Stripe test card
  - criterion: Comic reader provides smooth reading experience on desktop and mobile
    verification: Open purchased comic, verify panel navigation, zoom controls, responsive
      layout on mobile
  - criterion: SEO optimization with proper metadata and structured data for comic
      pages
    verification: Run Lighthouse audit, verify meta tags, check Google structured
      data testing tool
  testing:
    unit_tests:
    - file: apps/storefront/src/components/__tests__/comic-card.test.tsx
      coverage_target: 85%
      scenarios:
      - Comic card renders with all metadata
      - Handles missing preview images gracefully
      - Price formatting and currency display
    - file: apps/storefront/src/lib/__tests__/cart-utils.test.ts
      coverage_target: 90%
      scenarios:
      - Add/remove items from cart
      - Calculate totals with tax
      - Cart persistence in localStorage
    integration_tests:
    - file: apps/storefront/src/__tests__/integration/purchase-flow.test.tsx
      scenarios:
      - Complete purchase flow from catalog to confirmation
      - Stripe webhook processing for successful payment
    - file: apps/storefront/src/__tests__/integration/comic-reader.test.tsx
      scenarios:
      - Comic reader loads purchased content
      - Navigation between panels works
      - Access control for unpurchased comics
    manual_testing:
    - step: Test responsive design on various screen sizes
      expected: Comic previews and reader adapt properly to mobile/tablet/desktop
    - step: Verify image loading performance with slow network
      expected: Progressive loading shows placeholders, images load without blocking
        UI
    - step: Test payment flow with various card types
      expected: Stripe handles different payment methods, errors display properly
  estimates:
    development: 12
    code_review: 2
    testing: 3
    documentation: 1
    total: 18
  progress:
    status: not-started
    checklist:
    - task: Setup Next.js 16 storefront app with App Router structure
      done: false
    - task: Implement comic catalog pages with search and filtering
      done: false
    - task: Build individual comic detail pages with preview functionality
      done: false
    - task: Create shopping cart system with local storage persistence
      done: false
    - task: Integrate Stripe checkout and payment processing
      done: false
    - task: Develop comic reader component with panel navigation
      done: false
    - task: Implement progressive image loading and optimization
      done: false
    - task: Add SEO optimization with metadata and structured data
      done: false
    - task: Build responsive design for mobile comic reading
      done: false
    - task: Set up comprehensive testing suite and documentation
      done: false
- key: T86
  title: Cart Management
  type: Feature
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p0
  effort: 3
  area: ecommerce
  dependsOn:
  - T85
  agent_notes:
    research_findings: '**Context:**

      Cart Management is essential for Morpheus''s comic storefront, allowing users
      to add transformed comics to their cart, modify quantities, persist cart state
      across sessions, and proceed to checkout. This enables the core monetization
      flow where users purchase their AI-generated comics. Without robust cart functionality,
      users cannot complete purchases, making this critical for the commerce milestone.


      **Technical Approach:**

      Implement a hybrid cart system with client-side state management (Zustand) for
      immediate UX and server-side persistence for logged-in users. Use optimistic
      updates for cart operations with rollback on failure. Store cart data in Supabase
      with proper user association and guest cart handling via localStorage/cookies.
      Implement cart validation to ensure comic availability and pricing accuracy.


      **Dependencies:**

      - External: [@tanstack/react-query, zustand, @supabase/supabase-js, stripe,
      zod]

      - Internal: [authentication service, comic catalog API, user management, pricing
      engine]


      **Risks:**

      - Cart abandonment due to session loss: implement robust persistence with guest
      cart migration

      - Race conditions on concurrent cart updates: use optimistic locking with version
      numbers

      - Price manipulation attacks: server-side price validation on every cart operation

      - Performance with large carts: implement cart item limits and pagination


      **Complexity Notes:**

      More complex than initially estimated due to guest/authenticated user cart merging,
      real-time price updates, inventory management integration, and cross-device
      cart synchronization requirements.


      **Key Files:**

      - apps/storefront/stores/cart-store.ts: Zustand cart state management

      - apps/backend/src/routes/cart/: Cart API endpoints (CRUD operations)

      - packages/database/migrations/: Cart and cart_items table schemas

      - apps/storefront/components/cart/: Cart UI components

      - apps/storefront/hooks/use-cart.ts: Cart operations hook with react-query

      '
    design_decisions:
    - decision: Hybrid client/server cart with Zustand + Supabase persistence
      rationale: Provides immediate UX with optimistic updates while ensuring data
        persistence and cross-device synchronization
      alternatives_considered:
      - Pure server-side cart
      - Redux Toolkit with RTK Query
      - Context API only
    - decision: Guest cart migration on authentication
      rationale: Prevents cart loss when users sign up/login mid-session, improving
        conversion rates
      alternatives_considered:
      - Force login before adding to cart
      - Separate guest/user carts permanently
    - decision: Server-side price and availability validation
      rationale: Prevents price manipulation and ensures data integrity at checkout
      alternatives_considered:
      - Client-side validation only
      - Validation only at checkout
    researched_at: '2026-02-07T19:18:34.952561'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:44:51.281263'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a cart system with Zustand for client state and Supabase for
      persistence. Create RESTful cart endpoints in Fastify with proper authentication
      middleware. Implement optimistic updates with rollback capabilities and real-time
      price/availability validation. Use React Query for server state synchronization
      and implement guest cart migration logic on user authentication. Add comprehensive
      cart validation and inventory checks.

      '
    external_dependencies:
    - name: zustand
      version: ^4.4.7
      reason: Lightweight state management for cart operations with persistence middleware
    - name: '@tanstack/react-query'
      version: ^5.17.0
      reason: Server state management and optimistic updates for cart synchronization
    - name: zod
      version: ^3.22.4
      reason: Cart data validation schemas for both client and server
    - name: immer
      version: ^10.0.3
      reason: Immutable state updates for complex cart operations
    files_to_modify:
    - path: packages/database/supabase/migrations/20240115000000_create_cart_tables.sql
      changes: Add cart and cart_items tables with proper indexes and RLS policies
    - path: apps/backend/src/types/index.ts
      changes: Add Cart, CartItem, and CartOperation type definitions
    - path: apps/storefront/lib/supabase.ts
      changes: Add cart-related database types and queries
    new_files:
    - path: apps/storefront/stores/cart-store.ts
      purpose: Zustand store for cart state management with persistence
    - path: apps/storefront/hooks/use-cart.ts
      purpose: React Query hooks for cart operations and server synchronization
    - path: apps/backend/src/routes/cart/index.ts
      purpose: Cart API routes with CRUD operations
    - path: apps/backend/src/routes/cart/handlers.ts
      purpose: Cart route handlers with validation and business logic
    - path: apps/backend/src/services/cart-service.ts
      purpose: Cart business logic, validation, and database operations
    - path: apps/storefront/components/cart/CartDrawer.tsx
      purpose: Sliding cart drawer component with item list
    - path: apps/storefront/components/cart/CartItem.tsx
      purpose: Individual cart item with quantity controls
    - path: apps/storefront/components/cart/CartSummary.tsx
      purpose: Cart totals and checkout button component
    - path: apps/storefront/components/cart/AddToCartButton.tsx
      purpose: Reusable add-to-cart button with loading states
    - path: packages/shared/schemas/cart.ts
      purpose: Zod schemas for cart validation across frontend/backend
    - path: apps/storefront/utils/cart-persistence.ts
      purpose: Guest cart localStorage management utilities
    - path: apps/backend/src/middleware/cart-validation.ts
      purpose: Middleware for validating cart operations and pricing
  acceptance_criteria:
  - criterion: Users can add comics to cart with immediate UI feedback and persistent
      state across sessions
    verification: Add comic to cart, refresh browser, verify cart persists. Check
      localStorage for guest users and database for authenticated users
  - criterion: Guest cart seamlessly merges with user cart upon authentication without
      data loss
    verification: Add items as guest, login, verify all guest items appear in authenticated
      cart with proper deduplication
  - criterion: Cart operations handle concurrent updates and network failures gracefully
      with optimistic updates
    verification: Simulate network failure during cart update, verify UI shows loading
      state and rolls back on failure
  - criterion: Cart validates pricing and availability server-side to prevent manipulation
    verification: Modify cart item price in localStorage/database directly, attempt
      checkout, verify server rejects with current pricing
  - criterion: Cart performance remains responsive with up to 50 items and implements
      proper pagination
    verification: Add 50+ items to cart, measure load time <500ms, verify pagination
      UI appears after 20 items
  testing:
    unit_tests:
    - file: apps/storefront/stores/__tests__/cart-store.test.ts
      coverage_target: 90%
      scenarios:
      - Add/remove/update cart items
      - Cart total calculations
      - Optimistic updates and rollbacks
      - Guest cart persistence
    - file: apps/backend/src/services/__tests__/cart-service.test.ts
      coverage_target: 85%
      scenarios:
      - Cart CRUD operations
      - Price validation
      - Cart merging logic
      - Inventory checks
    integration_tests:
    - file: apps/backend/src/__tests__/integration/cart-api.test.ts
      scenarios:
      - Full cart lifecycle with authentication
      - Guest to authenticated user cart migration
      - Concurrent cart updates handling
    - file: apps/storefront/components/cart/__tests__/cart-integration.test.ts
      scenarios:
      - Cart UI with real API calls
      - Error state handling
      - Loading states
    e2e_tests:
    - file: apps/storefront/__tests__/e2e/cart-flow.spec.ts
      scenarios:
      - Complete add-to-cart to checkout flow
      - Cross-device cart synchronization
      - Cart abandonment and recovery
    manual_testing:
    - step: Add 25+ items to cart as guest user
      expected: Cart shows pagination, performance remains smooth
    - step: Close browser, reopen, login with different account
      expected: Guest cart merges with user cart, no duplicates
    - step: Disconnect network, modify cart, reconnect
      expected: Changes sync properly, conflicts resolved
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 0.5
    total: 8.5
  progress:
    status: not-started
    checklist:
    - task: Create database schema and migrations for cart tables
      done: false
    - task: Implement Zustand cart store with persistence logic
      done: false
    - task: Build cart API endpoints with authentication and validation
      done: false
    - task: Create cart service layer with business logic
      done: false
    - task: Develop React Query hooks for cart operations
      done: false
    - task: Build cart UI components (drawer, items, summary)
      done: false
    - task: Implement guest-to-authenticated cart migration
      done: false
    - task: Add optimistic updates with error handling
      done: false
    - task: Implement cart validation and inventory checks
      done: false
    - task: Write comprehensive tests and documentation
      done: false
- key: T87
  title: Checkout Flow
  type: Feature
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p0
  effort: 5
  area: ecommerce
  dependsOn:
  - T83
  - T86
  agent_notes:
    research_findings: '**Context:**

      The Checkout Flow is a critical ecommerce component that enables users to purchase
      comic books generated from novels on the Morpheus platform. This solves the
      monetization challenge by providing a secure, user-friendly payment experience
      that converts browsers into customers. Given this is in the Commerce & Distribution
      milestone, it''s essential for the platform''s business viability and must handle
      various payment methods, tax calculations, digital delivery, and order management.


      **Technical Approach:**

      Implement a multi-step checkout process using React Hook Form for state management,
      Stripe for payment processing, and Supabase for order persistence. Use Next.js
      API routes as a backend-for-frontend (BFF) pattern to orchestrate between Fastify
      backend services, Stripe webhooks, and order fulfillment. Implement optimistic
      UI updates with proper error boundaries and loading states. Follow atomic transaction
      patterns for order creation to ensure data consistency.


      **Dependencies:**

      - External: Stripe SDK, React Hook Form, Zod validation, React Query for API
      state

      - Internal: Authentication service, Product catalog service, User management,
      Email service, Digital asset delivery service


      **Risks:**

      - Payment processing failures: Implement idempotency keys and webhook retry
      logic

      - Cart abandonment: Add session persistence and recovery mechanisms

      - Tax calculation complexity: Use Stripe Tax or integrate with tax service APIs

      - Digital delivery issues: Implement robust asset generation status tracking

      - PCI compliance: Ensure all payment data flows through Stripe, never store
      card details


      **Complexity Notes:**

      Higher complexity than initially estimated due to digital goods delivery requirements.
      Unlike physical products, comic delivery involves coordinating with ML pipeline
      completion, asset generation, and potential re-generation requests. Multi-tenant
      considerations for white-label storefronts add additional complexity layers.


      **Key Files:**

      - apps/storefront/src/components/checkout/: Checkout flow components

      - apps/storefront/src/pages/api/checkout/: Payment processing APIs

      - apps/backend/src/routes/orders/: Order management endpoints

      - packages/database/src/schema/: Order and payment tables

      - apps/storefront/src/hooks/useCheckout.ts: Checkout state management

      '
    design_decisions:
    - decision: Use Stripe Checkout vs Custom Implementation
      rationale: Custom implementation provides better UX integration with comic previews
        and allows for complex digital delivery workflows
      alternatives_considered:
      - Stripe Checkout (hosted)
      - PayPal Smart Buttons
      - Square Web Payments SDK
    - decision: Multi-step vs Single-page checkout
      rationale: Multi-step reduces cognitive load for digital goods and allows progressive
        validation of complex comic customization options
      alternatives_considered:
      - Single-page accordion
      - Modal-based checkout
      - Separate checkout page
    - decision: Optimistic UI updates for cart operations
      rationale: Improves perceived performance for cart modifications while maintaining
        data consistency through conflict resolution
      alternatives_considered:
      - Pessimistic updates
      - Real-time sync with WebSockets
      - Periodic polling
    researched_at: '2026-02-07T19:18:59.504167'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:45:21.410233'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a 4-step checkout flow (Cart Review → Customer Info → Payment
      → Confirmation) using React Hook Form with Zod schemas for validation. Integrate
      Stripe Elements for PCI-compliant payment collection, with Next.js API routes
      handling server-side payment intent creation and webhook processing. Implement
      order state machine in Fastify backend to track order lifecycle from pending
      → paid → processing → delivered. Use Supabase Row Level Security for order access
      control and real-time subscriptions for order status updates.

      '
    external_dependencies:
    - name: '@stripe/stripe-js'
      version: ^2.1.0
      reason: Client-side Stripe integration for secure payment processing
    - name: '@stripe/react-stripe-js'
      version: ^2.3.0
      reason: React components for Stripe Elements integration
    - name: stripe
      version: ^14.0.0
      reason: Server-side Stripe SDK for payment processing and webhooks
    - name: react-hook-form
      version: ^7.47.0
      reason: Form state management with excellent TypeScript support
    - name: '@hookform/resolvers'
      version: ^3.3.0
      reason: Zod integration for form validation schemas
    - name: zod
      version: ^3.22.0
      reason: Runtime type validation for checkout forms and API endpoints
    - name: '@tanstack/react-query'
      version: ^5.0.0
      reason: API state management for cart and order operations
    files_to_modify:
    - path: apps/backend/src/app.ts
      changes: Register order routes and Stripe webhook endpoints
    - path: packages/database/src/schema/index.ts
      changes: Add orders, order_items, payments, and digital_deliveries tables
    - path: apps/storefront/src/pages/_app.tsx
      changes: Add React Query client configuration for checkout state
    - path: apps/backend/src/plugins/auth.ts
      changes: Add guest checkout session handling
    new_files:
    - path: apps/storefront/src/components/checkout/CheckoutFlow.tsx
      purpose: Main checkout component orchestrating 4-step process
    - path: apps/storefront/src/components/checkout/CartReview.tsx
      purpose: 'Step 1: Cart items display and quantity modification'
    - path: apps/storefront/src/components/checkout/CustomerInfo.tsx
      purpose: 'Step 2: Billing/shipping information collection'
    - path: apps/storefront/src/components/checkout/PaymentStep.tsx
      purpose: 'Step 3: Stripe Elements integration for payment'
    - path: apps/storefront/src/components/checkout/OrderConfirmation.tsx
      purpose: 'Step 4: Order success and digital delivery status'
    - path: apps/storefront/src/hooks/useCheckout.ts
      purpose: Checkout state management with React Hook Form
    - path: apps/storefront/src/pages/api/checkout/create-payment-intent.ts
      purpose: Create Stripe payment intent with order details
    - path: apps/storefront/src/pages/api/checkout/confirm-payment.ts
      purpose: Confirm payment and create order record
    - path: apps/storefront/src/pages/api/webhooks/stripe.ts
      purpose: Handle Stripe webhook events for payment status
    - path: apps/backend/src/routes/orders/index.ts
      purpose: Order CRUD operations and status management
    - path: apps/backend/src/services/OrderService.ts
      purpose: Business logic for order lifecycle management
    - path: apps/backend/src/services/PaymentService.ts
      purpose: Stripe integration and payment processing
    - path: apps/backend/src/services/DigitalDeliveryService.ts
      purpose: Coordinate asset generation and delivery
    - path: apps/backend/src/lib/order-state-machine.ts
      purpose: Define order state transitions and validation
    - path: packages/shared/src/types/checkout.ts
      purpose: TypeScript interfaces for checkout flow
    - path: packages/shared/src/schemas/checkout.ts
      purpose: Zod validation schemas for checkout forms
  acceptance_criteria:
  - criterion: User can complete end-to-end checkout for digital comic purchase with
      cart review, customer info, payment, and order confirmation steps
    verification: E2E test covers full flow from cart to delivered digital asset download
      link
  - criterion: Payment processing handles success, failure, and pending states with
      proper error messages and retry mechanisms
    verification: Integration tests with Stripe test cards (4242424242424242 success,
      4000000000000002 decline) verify all payment states
  - criterion: Order state machine transitions correctly through pending → paid →
      processing → delivered with real-time updates
    verification: Unit tests verify state transitions and integration tests confirm
      Supabase real-time subscriptions update UI
  - criterion: Cart abandonment recovery allows users to resume checkout within 24
      hours of session creation
    verification: 'Manual test: start checkout, close browser, return within 24h and
      verify cart/progress restoration'
  - criterion: Digital asset delivery provides download link within 5 minutes of successful
      payment for pre-generated comics
    verification: Performance test measures time from webhook receipt to email delivery
      with download link
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/order-service.test.ts
      coverage_target: 90%
      scenarios:
      - Order creation with valid cart items
      - Order state transitions
      - Payment intent creation and updates
      - Digital asset linking
      - Tax calculation integration
    - file: apps/storefront/src/hooks/__tests__/useCheckout.test.ts
      coverage_target: 85%
      scenarios:
      - Form validation with Zod schemas
      - Step navigation and persistence
      - Error state handling
      - Cart total calculations
    integration_tests:
    - file: apps/backend/src/__tests__/integration/checkout-flow.test.ts
      scenarios:
      - Complete checkout flow with Stripe test environment
      - Webhook processing for payment events
      - Order fulfillment pipeline integration
      - Email notification delivery
    - file: apps/storefront/src/__tests__/integration/checkout-ui.test.ts
      scenarios:
      - Multi-step form progression
      - Real-time order status updates
      - Payment error handling UI
    e2e_tests:
    - file: apps/storefront/src/__tests__/e2e/checkout.spec.ts
      scenarios:
      - Guest checkout with email receipt
      - Authenticated user checkout with order history
      - Cart abandonment and recovery
      - Failed payment retry flow
    manual_testing:
    - step: Test with various Stripe test cards for different failure modes
      expected: Appropriate error messages and recovery options displayed
    - step: Verify tax calculations for different billing addresses
      expected: Correct tax rates applied based on jurisdiction
    - step: Test checkout flow on mobile devices
      expected: Responsive design works across iOS/Android browsers
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Setup database schema for orders, payments, and digital deliveries
      done: false
    - task: Implement order state machine and business logic services
      done: false
    - task: Create Stripe integration for payment processing and webhooks
      done: false
    - task: Build React checkout flow components with form validation
      done: false
    - task: Implement Next.js API routes for payment orchestration
      done: false
    - task: Add digital asset delivery coordination with ML pipeline
      done: false
    - task: Implement cart persistence and abandonment recovery
      done: false
    - task: Add real-time order status updates with Supabase subscriptions
      done: false
    - task: Create comprehensive test suite covering unit, integration, and e2e
      done: false
    - task: Document checkout API endpoints and integration patterns
      done: false
- key: T88
  title: Order Management
  type: Feature
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p0
  effort: 3
  area: ecommerce
  dependsOn:
  - T87
  agent_notes:
    research_findings: "**Context:**\nOrder Management is the core engine for handling\
      \ comic purchases in Morpheus. It needs to manage the complete order lifecycle\
      \ from cart creation through payment processing, fulfillment tracking, and digital\
      \ delivery of comic assets. This is critical for monetizing the platform and\
      \ providing users with a seamless purchasing experience for generated comics.\n\
      \n**Technical Approach:**\nImplement a state machine-based order system with\
      \ clear transitions (pending → processing → paid → fulfilled → delivered). Use\
      \ Supabase RLS for multi-tenant order isolation, integrate Stripe for payments,\
      \ and leverage event-driven architecture with PostgreSQL triggers for order\
      \ state changes. Design RESTful APIs following JSON:API spec for consistency\
      \ with existing Morpheus patterns.\n\n**Dependencies:**\n- External: Stripe\
      \ SDK, zod validation, date-fns for date handling, uuid for order IDs\n- Internal:\
      \ User authentication service, Comic catalog service, Notification system, Email\
      \ service\n\n**Risks:**\n- Payment processing failures: Implement idempotent\
      \ operations and webhook handling\n- Race conditions on inventory: Use PostgreSQL\
      \ row-level locking and atomic transactions  \n- Order state corruption: Strict\
      \ state machine validation with database constraints\n- Digital asset delivery:\
      \ Secure token-based download links with expiration\n\n**Complexity Notes:**\n\
      More complex than initially appears due to payment flow edge cases, refund handling,\
      \ and digital rights management. The state machine logic and webhook processing\
      \ add significant complexity beyond basic CRUD operations.\n\n**Key Files:**\n\
      - apps/api/src/routes/orders/: Order management endpoints\n- packages/database/migrations/:\
      \ Order tables and triggers\n- apps/dashboard/src/pages/orders/: Admin order\
      \ management UI\n- apps/storefront/src/components/checkout/: Customer checkout\
      \ flow\n"
    design_decisions:
    - decision: Use PostgreSQL enum for order status with state machine validation
      rationale: Ensures data integrity and prevents invalid state transitions at
        the database level
      alternatives_considered:
      - String status field
      - Separate order_events table
      - Redis state storage
    - decision: Implement Stripe webhooks for payment confirmation rather than polling
      rationale: Real-time payment updates and reduced API calls, following Stripe
        best practices
      alternatives_considered:
      - Payment polling
      - Client-side confirmation only
      - Dual webhook + polling
    - decision: Generate secure download tokens for digital comic delivery
      rationale: Prevents unauthorized access while allowing legitimate purchases
        to download
      alternatives_considered:
      - Direct file URLs
      - Session-based access
      - JWT tokens
    researched_at: '2026-02-07T19:19:18.767832'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:45:48.686327'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a comprehensive order management system centered around a PostgreSQL-backed
      state machine. Integrate Stripe for payment processing with webhook-driven status
      updates. Create admin dashboards for order monitoring and customer-facing checkout
      flows. Implement secure digital delivery through tokenized download links with
      expiration. Use event-driven architecture to trigger notifications, analytics,
      and fulfillment processes.

      '
    external_dependencies:
    - name: stripe
      version: ^14.0.0
      reason: Payment processing and subscription management
    - name: zod
      version: ^3.22.0
      reason: Order payload validation and type safety
    - name: uuid
      version: ^9.0.0
      reason: Generate unique order IDs and download tokens
    - name: date-fns
      version: ^2.30.0
      reason: Order date handling and token expiration logic
    files_to_modify:
    - path: packages/database/migrations/001_initial.sql
      changes: Add orders, order_items, order_state_history tables with RLS policies
    - path: apps/api/src/lib/database.ts
      changes: Add order-related database types and helper functions
    - path: apps/api/src/middleware/auth.ts
      changes: Extend authentication to support order access validation
    new_files:
    - path: apps/api/src/services/order-service.ts
      purpose: Core order management logic, state machine, business rules
    - path: apps/api/src/services/payment-service.ts
      purpose: Stripe integration, payment processing, webhook handling
    - path: apps/api/src/services/digital-delivery-service.ts
      purpose: Secure download link generation, asset delivery management
    - path: apps/api/src/routes/orders/index.ts
      purpose: Order CRUD API endpoints with JSON:API compliance
    - path: apps/api/src/routes/orders/checkout.ts
      purpose: Checkout flow endpoints, cart to order conversion
    - path: apps/api/src/routes/orders/downloads.ts
      purpose: Digital asset download endpoints with token validation
    - path: apps/api/src/routes/webhooks/stripe.ts
      purpose: Stripe webhook endpoint for payment status updates
    - path: apps/api/src/schemas/order-schemas.ts
      purpose: Zod validation schemas for order operations
    - path: apps/dashboard/src/pages/orders/index.tsx
      purpose: Admin order list view with filtering and pagination
    - path: apps/dashboard/src/pages/orders/[id].tsx
      purpose: Admin order detail view with status management
    - path: apps/dashboard/src/components/orders/OrderStateMachine.tsx
      purpose: Visual order state machine component for admin
    - path: apps/storefront/src/components/checkout/CheckoutFlow.tsx
      purpose: Customer checkout flow with Stripe Elements integration
    - path: apps/storefront/src/components/checkout/OrderConfirmation.tsx
      purpose: Order confirmation page with download access
    - path: apps/storefront/src/pages/orders/[id]/download.tsx
      purpose: Secure download page with token validation
    - path: packages/shared/src/types/order.ts
      purpose: Shared TypeScript types for order entities
    - path: packages/shared/src/constants/order-states.ts
      purpose: Order state machine constants and transitions
  acceptance_criteria:
  - criterion: Complete order lifecycle from cart creation to digital delivery with
      state transitions (pending → processing → paid → fulfilled → delivered)
    verification: Integration test creating order, processing payment via Stripe,
      and generating secure download link
  - criterion: Stripe payment integration with webhook handling for payment status
      updates and idempotent operations
    verification: Webhook endpoint processes duplicate events safely, payment failures
      rollback order state
  - criterion: Multi-tenant order isolation with row-level security ensuring users
      only access their orders
    verification: Database query with different user tokens returns only user-specific
      orders
  - criterion: Admin dashboard displays order management with filtering, status updates,
      and refund processing
    verification: Manual testing of admin UI showing order list, detail view, and
      status change actions
  - criterion: Digital asset delivery through secure tokenized download links with
      24-hour expiration
    verification: Download link works within expiration window, returns 403 after
      expiration
  testing:
    unit_tests:
    - file: apps/api/src/services/__tests__/order-service.test.ts
      coverage_target: 90%
      scenarios:
      - Order state machine transitions
      - Payment webhook processing
      - Order validation and error handling
      - Digital delivery token generation
    - file: apps/api/src/routes/orders/__tests__/orders.test.ts
      coverage_target: 85%
      scenarios:
      - Order CRUD operations
      - Authentication and authorization
      - Input validation with zod schemas
    integration_tests:
    - file: apps/api/src/__tests__/integration/order-flow.test.ts
      scenarios:
      - Complete order lifecycle with Stripe test mode
      - Webhook processing with order state updates
      - Multi-user order isolation
    - file: apps/api/src/__tests__/integration/payment-webhooks.test.ts
      scenarios:
      - Stripe webhook signature validation
      - Duplicate webhook handling
      - Failed payment processing
    e2e_tests:
    - file: apps/storefront/cypress/e2e/checkout-flow.cy.ts
      scenarios:
      - Customer checkout with Stripe payment
      - Order confirmation and download access
      - Failed payment handling
    manual_testing:
    - step: Create order through storefront checkout flow
      expected: Order appears in admin dashboard with pending status
    - step: Process test payment through Stripe
      expected: Order status updates to paid, download link becomes available
    - step: Test download link expiration after 24 hours
      expected: Link returns 403 forbidden error
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Design and create database schema with orders, order_items, and state_history
        tables
      done: false
    - task: Implement core order service with state machine logic and validation
      done: false
    - task: Set up Stripe integration with payment processing and webhook handling
      done: false
    - task: Create order management API endpoints with proper authentication and RLS
      done: false
    - task: Implement digital delivery service with secure tokenized downloads
      done: false
    - task: Build admin dashboard for order management and monitoring
      done: false
    - task: Create customer checkout flow with Stripe Elements integration
      done: false
    - task: Add comprehensive error handling and edge case management
      done: false
    - task: Implement unit and integration tests for all order flows
      done: false
    - task: Add monitoring, logging, and analytics for order processing
      done: false
- key: T89
  title: Inventory Tracking
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 2
  area: ecommerce
  dependsOn:
  - T88
  agent_notes:
    research_findings: "**Context:**\nInventory tracking is essential for the Morpheus\
      \ commerce platform to manage physical comic book stock, digital asset availability,\
      \ and print-on-demand fulfillment. This system needs to track comic book inventory\
      \ across multiple formats (digital downloads, physical prints, limited editions),\
      \ prevent overselling, handle reservation/hold periods during checkout, and\
      \ provide real-time stock visibility to both customers and administrators. Given\
      \ the novel-to-comic transformation workflow, inventory must also account for\
      \ generated assets and their licensing/usage rights.\n\n**Technical Approach:**\n\
      Implement a multi-tier inventory system using PostgreSQL with row-level security,\
      \ Redis for real-time stock caching, and event-driven architecture for stock\
      \ updates. Use Supabase real-time subscriptions for live inventory updates in\
      \ the dashboard. Implement optimistic concurrency control with database constraints\
      \ to prevent race conditions during high-traffic scenarios. Design separate\
      \ inventory pools for digital assets (unlimited/licensed), physical stock (finite\
      \ quantities), and print-on-demand (capacity-based availability). Integration\
      \ with payment processing should include inventory reservation during checkout\
      \ flow with TTL-based automatic release.\n\n**Dependencies:**\n- External: ioredis,\
      \ pg-boss, decimal.js, date-fns\n- Internal: Supabase client, authentication\
      \ middleware, order management system, payment processing service, admin dashboard\
      \ components, storefront product pages\n\n**Risks:**\n- Race conditions during\
      \ concurrent purchases: Use database-level constraints and Redis distributed\
      \ locks\n- Cache invalidation complexity: Implement event-driven cache updates\
      \ with fallback to database queries  \n- Inventory reservation abuse: Add rate\
      \ limiting and automated cleanup of expired reservations\n- Complex multi-format\
      \ tracking: Design clear separation of concerns between digital/physical/POD\
      \ inventory types\n- Real-time sync failures: Build reconciliation jobs to detect\
      \ and fix inventory discrepancies\n\n**Complexity Notes:**\nThis task is more\
      \ complex than initially estimated due to the need to handle multiple inventory\
      \ types (digital assets, physical books, print-on-demand capacity), real-time\
      \ synchronization across distributed systems, and integration with both the\
      \ creative workflow (asset generation) and commerce pipeline (orders, payments,\
      \ fulfillment). The event-driven architecture required for proper inventory\
      \ management adds significant complexity.\n\n**Key Files:**\n- packages/database/supabase/migrations/:\
      \ Add inventory tables schema\n- apps/api/src/services/inventory/: Core inventory\
      \ management service\n- apps/api/src/routes/inventory/: REST API endpoints for\
      \ inventory operations\n- apps/dashboard/src/components/inventory/: Admin inventory\
      \ management UI\n- apps/storefront/src/components/product/: Product availability\
      \ display\n- packages/shared/src/types/inventory.ts: Shared inventory type definitions\n"
    design_decisions:
    - decision: Use hybrid SQL + Redis architecture for inventory tracking
      rationale: PostgreSQL provides ACID guarantees and complex queries needed for
        reporting, while Redis enables real-time stock checking and reservation management
        with sub-millisecond latency
      alternatives_considered:
      - Pure SQL with aggressive caching
      - Event sourcing with separate read/write models
      - Third-party inventory service integration
    - decision: Implement three distinct inventory pools (digital, physical, POD)
      rationale: Different inventory types have fundamentally different constraints
        and business rules - digital has licensing limits, physical has finite stock,
        POD has capacity/queue management
      alternatives_considered:
      - Single unified inventory table with type flags
      - Completely separate microservices per type
    - decision: Use pessimistic locking with TTL for checkout reservations
      rationale: Prevents overselling during payment processing while automatically
        cleaning up abandoned carts, critical for limited edition releases
      alternatives_considered:
      - Optimistic concurrency with retry logic
      - No reservation system with real-time availability checks
    researched_at: '2026-02-07T19:19:45.224981'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:46:21.017186'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Build a dual-layer inventory system with PostgreSQL as the source of
      truth and Redis for real-time operations. Create separate inventory pools for
      digital assets, physical stock, and print-on-demand capacity, each with specialized
      business logic. Implement event-driven stock updates using Supabase real-time
      features and background job processing. Use distributed locking for checkout
      reservations with automatic TTL cleanup. Integrate tightly with the order management
      system and provide real-time stock visibility through WebSocket connections
      to both admin dashboard and storefront.

      '
    external_dependencies:
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for real-time inventory caching and distributed locking
    - name: pg-boss
      version: ^9.0.0
      reason: PostgreSQL-based job queue for inventory reconciliation and cleanup
        tasks
    - name: decimal.js
      version: ^10.4.0
      reason: Precise decimal arithmetic for inventory quantities and financial calculations
    - name: date-fns
      version: ^2.30.0
      reason: Date manipulation for reservation TTL and inventory reporting time ranges
    files_to_modify:
    - path: packages/database/supabase/migrations/20240115000000_inventory_system.sql
      changes: Add inventory_pools, inventory_items, inventory_reservations, inventory_logs
        tables with RLS policies
    - path: packages/shared/src/types/index.ts
      changes: Export inventory types for cross-package usage
    - path: apps/api/src/middleware/auth.ts
      changes: Add inventory management permissions for admin users
    - path: apps/dashboard/src/components/layout/navigation.tsx
      changes: Add inventory management navigation items
    - path: apps/storefront/src/components/product/ProductCard.tsx
      changes: Display real-time stock status and availability
    new_files:
    - path: packages/shared/src/types/inventory.ts
      purpose: TypeScript definitions for inventory entities, enums, and API responses
    - path: packages/database/supabase/migrations/20240115000000_inventory_system.sql
      purpose: Database schema for multi-tier inventory system with constraints
    - path: apps/api/src/services/inventory/inventory-service.ts
      purpose: Core inventory management business logic and operations
    - path: apps/api/src/services/inventory/inventory-pools.ts
      purpose: Specialized handlers for digital, physical, and POD inventory types
    - path: apps/api/src/services/inventory/inventory-cache.ts
      purpose: Redis caching layer with distributed locking mechanisms
    - path: apps/api/src/services/inventory/inventory-reconciliation.ts
      purpose: Background job for detecting and fixing inventory discrepancies
    - path: apps/api/src/routes/inventory/index.ts
      purpose: REST API endpoints for inventory operations and queries
    - path: apps/api/src/routes/inventory/admin.ts
      purpose: Administrative inventory management endpoints
    - path: apps/api/src/routes/inventory/public.ts
      purpose: Public inventory status endpoints for storefront
    - path: apps/api/src/jobs/inventory-cleanup.ts
      purpose: Background job for cleaning expired reservations
    - path: apps/dashboard/src/components/inventory/InventoryDashboard.tsx
      purpose: Main inventory management interface for administrators
    - path: apps/dashboard/src/components/inventory/StockLevels.tsx
      purpose: Real-time stock level monitoring component
    - path: apps/dashboard/src/components/inventory/InventoryAlerts.tsx
      purpose: Low stock and out-of-stock alert system
    - path: apps/dashboard/src/hooks/useInventorySubscription.ts
      purpose: Real-time inventory updates via Supabase subscriptions
    - path: apps/storefront/src/components/product/StockIndicator.tsx
      purpose: Customer-facing stock availability display
    - path: apps/storefront/src/hooks/useProductAvailability.ts
      purpose: Real-time product availability tracking for customers
  acceptance_criteria:
  - criterion: Inventory system accurately tracks stock levels for digital assets
      (unlimited/licensed), physical books (finite), and print-on-demand capacity
      across all operations
    verification: Run integration tests that simulate purchases, reservations, and
      stock updates - verify database consistency and Redis cache alignment
  - criterion: Concurrent purchase attempts cannot oversell inventory, even under
      high load (100+ simultaneous requests)
    verification: Execute load test with artillery targeting checkout endpoints -
      verify no negative stock levels in database
  - criterion: Inventory reservations during checkout automatically expire after 15
      minutes with stock returned to available pool
    verification: Create reservation, wait 15+ minutes, verify stock returned via
      GET /api/inventory/products/{id} endpoint
  - criterion: Real-time inventory updates are pushed to admin dashboard and storefront
      within 2 seconds of stock changes
    verification: Monitor WebSocket connections while making inventory changes - measure
      update latency with browser dev tools
  - criterion: Inventory reconciliation job detects and fixes discrepancies between
      Redis cache and PostgreSQL within 5 minutes
    verification: Manually corrupt Redis cache data, run reconciliation job, verify
      corrections logged and data restored
  testing:
    unit_tests:
    - file: apps/api/src/services/inventory/__tests__/inventory-service.test.ts
      coverage_target: 90%
      scenarios:
      - Stock allocation and deallocation
      - Reservation creation and expiry
      - Multi-format inventory handling
      - Concurrency control with locks
      - Cache invalidation logic
    - file: apps/api/src/services/inventory/__tests__/inventory-pools.test.ts
      coverage_target: 85%
      scenarios:
      - Digital asset pool operations
      - Physical stock pool operations
      - Print-on-demand capacity checks
      - Pool-specific business rules
    integration_tests:
    - file: apps/api/src/__tests__/integration/inventory-flow.test.ts
      scenarios:
      - Complete purchase flow with stock deduction
      - Checkout reservation and timeout handling
      - Real-time updates via Supabase subscriptions
      - Redis cache consistency after operations
      - Inventory reconciliation process
    - file: apps/api/src/__tests__/integration/concurrent-purchases.test.ts
      scenarios:
      - Race condition prevention during simultaneous checkouts
      - Distributed lock behavior under load
    manual_testing:
    - step: 'Create product with limited physical stock (qty: 5), attempt 10 simultaneous
        purchases via storefront'
      expected: Only 5 purchases succeed, others receive out-of-stock error
    - step: Monitor admin dashboard while making purchases on storefront
      expected: Stock levels update in real-time without page refresh
    - step: Start checkout process, abandon cart for 20 minutes, return to product
        page
      expected: Reserved stock returned to available inventory
  estimates:
    development: 8
    code_review: 1.5
    testing: 2
    documentation: 1
    total: 12.5
  progress:
    status: not-started
    checklist:
    - task: Design and implement database schema with inventory tables, constraints,
        and RLS policies
      done: false
    - task: Create core inventory service with multi-pool architecture and business
        logic
      done: false
    - task: Implement Redis caching layer with distributed locking for concurrency
        control
      done: false
    - task: Build REST API endpoints for inventory operations (admin and public)
      done: false
    - task: Develop real-time subscription system for live inventory updates
      done: false
    - task: Create admin dashboard components for inventory management and monitoring
      done: false
    - task: Implement storefront components for displaying product availability
      done: false
    - task: Build background jobs for reservation cleanup and inventory reconciliation
      done: false
    - task: Integrate inventory system with checkout flow and order management
      done: false
    - task: Comprehensive testing including load testing for concurrency scenarios
      done: false
    - task: Documentation for API endpoints, admin workflows, and system architecture
      done: false
    - task: Code review and security audit of inventory operations
      done: false
- key: T90
  title: Fulfillment Integration
  type: Task
  milestone: M6 - Commerce & Distribution
  iteration: I6
  priority: p1
  effort: 3
  area: ecommerce
  dependsOn:
  - T88
  agent_notes:
    research_findings: '**Context:**

      Fulfillment Integration enables Morpheus to automatically process and ship physical
      comic book orders to customers. This is critical for the commerce platform as
      it bridges the gap between digital comic generation and physical product delivery.
      Without this, the platform would be limited to digital-only sales, significantly
      reducing revenue potential and market reach. The integration needs to handle
      order routing, inventory management, print job specifications, shipping calculations,
      and tracking updates.


      **Technical Approach:**

      Implement a fulfillment service layer that integrates with print-on-demand providers
      (Printful, Gooten) and traditional fulfillment centers via webhooks and REST
      APIs. Use a queue-based architecture with BullMQ for order processing, implement
      fulfillment provider abstraction layer for multi-vendor support, and create
      order status synchronization with real-time updates via WebSocket connections
      to the dashboard. Leverage Supabase edge functions for webhook processing and
      implement retry mechanisms with exponential backoff for failed fulfillment requests.


      **Dependencies:**

      - External: [@printful/printful-sdk, axios, bullmq, ioredis, zod, webhook-validator]

      - Internal: [orders service, payment service, notification service, comic generation
      pipeline, user dashboard components]


      **Risks:**

      - Provider API downtime: Implement circuit breaker pattern with multiple provider
      fallbacks

      - Print quality issues: Add pre-flight validation for comic assets and print
      specifications

      - Inventory sync delays: Cache inventory data with TTL and implement real-time
      webhooks

      - Cost calculation errors: Create comprehensive testing suite for pricing logic
      and validate against provider APIs


      **Complexity Notes:**

      More complex than initially estimated due to multi-provider integration requirements
      and the need for robust error handling across the fulfillment pipeline. The
      comic-to-print specification mapping adds additional complexity, as generated
      comics need to be optimized for physical printing (color profiles, DPI, bleed
      areas).


      **Key Files:**

      - apps/api/src/services/fulfillment/: Core fulfillment service implementation

      - apps/api/src/queues/fulfillment-queue.ts: Order processing queue management

      - apps/api/src/routes/webhooks/fulfillment.ts: Provider webhook handlers

      - apps/dashboard/src/components/orders/OrderTracking.tsx: Real-time order status
      UI

      - packages/database/migrations/: Order fulfillment tables and indexes

      '
    design_decisions:
    - decision: Multi-provider abstraction layer with primary/fallback routing
      rationale: Reduces vendor lock-in, improves reliability, and enables cost optimization
        by routing to best provider per order type
      alternatives_considered:
      - Single provider integration
      - Manual fulfillment workflow
      - Direct provider SDK usage
    - decision: Queue-based order processing with Redis/BullMQ
      rationale: Handles high order volumes, provides retry mechanisms, and enables
        distributed processing across multiple API instances
      alternatives_considered:
      - Synchronous processing
      - Database-based queues
      - Cloud pub/sub services
    researched_at: '2026-02-07T19:20:07.502055'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:46:44.064784'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Create a fulfillment service that abstracts multiple print-on-demand
      providers behind a unified interface. Orders are queued for processing, where
      comic assets are optimized for print, fulfillment providers are selected based
      on cost/availability, and orders are submitted via provider APIs. Implement
      webhook endpoints to receive status updates and sync order states back to the
      database. Provide real-time order tracking through WebSocket connections to
      the dashboard.

      '
    external_dependencies:
    - name: '@printful/printful-sdk'
      version: ^1.5.0
      reason: Official SDK for Printful print-on-demand integration
    - name: bullmq
      version: ^4.15.0
      reason: Redis-based queue system for reliable order processing
    - name: ioredis
      version: ^5.3.2
      reason: Redis client for queue management and caching
    - name: sharp
      version: ^0.33.0
      reason: Image processing for print optimization (DPI, color profiles)
    - name: pdf2pic
      version: ^3.1.0
      reason: Convert generated comic PDFs to print-ready formats
    - name: webhook-validator
      version: ^2.1.0
      reason: Validate incoming webhooks from fulfillment providers
    files_to_modify:
    - path: packages/database/schema.sql
      changes: Add fulfillment_orders, fulfillment_providers, and order_tracking tables
    - path: apps/api/src/routes/orders.ts
      changes: Add fulfillment trigger endpoints and status retrieval
    - path: apps/dashboard/src/components/orders/OrderList.tsx
      changes: Add fulfillment status column and tracking links
    new_files:
    - path: apps/api/src/services/fulfillment/fulfillment-service.ts
      purpose: Core fulfillment orchestration and provider management
    - path: apps/api/src/services/fulfillment/providers/printful-adapter.ts
      purpose: Printful API integration and order submission
    - path: apps/api/src/services/fulfillment/providers/gooten-adapter.ts
      purpose: Gooten API integration and order submission
    - path: apps/api/src/services/fulfillment/providers/base-provider.ts
      purpose: Abstract base class for fulfillment provider implementations
    - path: apps/api/src/services/fulfillment/comic-optimizer.ts
      purpose: Comic asset optimization for print specifications
    - path: apps/api/src/services/fulfillment/cost-calculator.ts
      purpose: Multi-provider cost comparison and optimization
    - path: apps/api/src/queues/fulfillment-queue.ts
      purpose: BullMQ implementation for order processing pipeline
    - path: apps/api/src/routes/webhooks/fulfillment.ts
      purpose: Provider webhook handlers for status updates
    - path: apps/api/src/middleware/webhook-validator.ts
      purpose: Webhook signature validation for security
    - path: apps/dashboard/src/components/orders/OrderTracking.tsx
      purpose: Real-time order status and tracking display
    - path: apps/dashboard/src/hooks/useOrderStatus.ts
      purpose: WebSocket hook for real-time order updates
    - path: packages/database/migrations/20241201_fulfillment_tables.sql
      purpose: Database schema for fulfillment system
  acceptance_criteria:
  - criterion: Orders can be automatically processed and routed to optimal fulfillment
      providers based on cost and availability
    verification: Create test order via API, verify provider selection logic in logs,
      confirm order submitted to correct provider with proper specifications
  - criterion: Comic assets are automatically optimized for print with correct specifications
      (300 DPI, CMYK, bleed areas)
    verification: Upload test comic, trigger fulfillment, verify generated print-ready
      PDF meets provider requirements using automated validation
  - criterion: Real-time order status updates are synchronized from providers and
      displayed in dashboard
    verification: Place order, simulate provider webhooks, confirm status updates
      appear in dashboard within 5 seconds
  - criterion: Failed fulfillment requests are automatically retried with exponential
      backoff and fallback providers
    verification: Simulate provider API failures, verify retry attempts logged with
      increasing delays, confirm fallback to secondary provider
  - criterion: Order tracking information is accurate and includes shipping details,
      estimated delivery, and provider tracking numbers
    verification: Complete order fulfillment flow, verify tracking data matches provider
      response, test tracking URL generation
  testing:
    unit_tests:
    - file: apps/api/src/services/fulfillment/__tests__/fulfillment-service.test.ts
      coverage_target: 90%
      scenarios:
      - Provider selection algorithm
      - Comic asset optimization
      - Error handling and retries
      - Cost calculation accuracy
    - file: apps/api/src/services/fulfillment/__tests__/provider-adapter.test.ts
      coverage_target: 85%
      scenarios:
      - Printful API integration
      - Gooten API integration
      - Webhook signature validation
    - file: apps/api/src/queues/__tests__/fulfillment-queue.test.ts
      coverage_target: 85%
      scenarios:
      - Queue job processing
      - Job failure handling
      - Priority queue management
    integration_tests:
    - file: apps/api/src/__tests__/integration/fulfillment-flow.test.ts
      scenarios:
      - End-to-end order fulfillment
      - Webhook processing and status sync
      - Multi-provider fallback
      - Real-time status updates
    manual_testing:
    - step: Create test order with physical comic book
      expected: Order appears in fulfillment queue and processes successfully
    - step: Simulate provider webhook with status update
      expected: Order status updates in database and dashboard shows new status
    - step: Test provider API failure scenario
      expected: System falls back to secondary provider and processes order
  estimates:
    development: 8
    code_review: 1.5
    testing: 2.5
    documentation: 1
    total: 13
  progress:
    status: not-started
    checklist:
    - task: Set up database schema and migrations for fulfillment tables
      done: false
    - task: Implement base provider interface and abstract classes
      done: false
    - task: Create Printful and Gooten API adapters with authentication
      done: false
    - task: Build comic asset optimization service for print specifications
      done: false
    - task: Implement fulfillment service with provider selection logic
      done: false
    - task: Set up BullMQ queue system for order processing
      done: false
    - task: Create webhook endpoints and signature validation
      done: false
    - task: Build dashboard components for order tracking
      done: false
    - task: Implement WebSocket connections for real-time updates
      done: false
    - task: Add comprehensive error handling and retry mechanisms
      done: false
- key: T91
  title: Final Integration Testing
  type: Task
  milestone: M7 - Launch & Release
  iteration: I7
  priority: p0
  effort: 5
  area: release
  dependsOn: []
  agent_notes:
    research_findings: "**Context:**\nFinal Integration Testing is the critical pre-launch\
      \ validation phase that ensures all Morpheus components work together seamlessly\
      \ in production-like conditions. This addresses the gap between unit/component\
      \ testing and real-world usage by testing complete user journeys from novel\
      \ upload through comic generation and delivery. It's essential for launch confidence,\
      \ preventing production incidents, and validating that our complex ML pipeline\
      \ integrates properly with the web platform.\n\n**Technical Approach:**\n- E2E\
      \ testing with Playwright covering complete user workflows (upload novel → AI\
      \ processing → comic generation → payment → delivery)\n- Load testing using\
      \ Artillery.io to validate system performance under concurrent users\n- Integration\
      \ testing of external services (OpenAI/Anthropic APIs, RunPod, Supabase, Stripe)\n\
      - Cross-browser testing on Chrome, Firefox, Safari\n- Mobile responsiveness\
      \ validation for storefront\n- API contract testing between frontend/backend\
      \ using Pact or similar\n- Database migration testing and data integrity validation\n\
      - Error handling and recovery testing (network failures, API timeouts, payment\
      \ failures)\n\n**Dependencies:**\n- External: playwright, @playwright/test,\
      \ artillery, pact-js, lighthouse-ci, axe-playwright\n- Internal: All backend\
      \ services, frontend applications, database schemas, ML pipeline components\n\
      - Infrastructure: Staging environment identical to production, test data sets,\
      \ mock payment processing\n\n**Risks:**\n- Test environment drift: staging differs\
      \ from production leading to false confidence\n- Flaky tests: ML API timeouts\
      \ or rate limits causing intermittent failures  \n- Data privacy: using real\
      \ user data in tests violating compliance\n- Performance bottlenecks: load testing\
      \ revealing scalability issues too close to launch\n- External service dependencies:\
      \ third-party API changes breaking integration tests\n\n**Complexity Notes:**\n\
      This is significantly more complex than typical integration testing due to:\n\
      - Async ML processing workflows with variable completion times\n- Multiple external\
      \ API dependencies with different rate limits/behaviors\n- Complex state management\
      \ across novel processing stages\n- Payment flow integration requiring careful\
      \ test isolation\nHowever, existing Playwright setup and modular architecture\
      \ reduce implementation complexity.\n\n**Key Files:**\n- apps/web/tests/e2e/:\
      \ Complete user journey tests\n- packages/backend/tests/integration/: API integration\
      \ test suites\n- apps/dashboard/tests/workflows/: Admin workflow testing\n-\
      \ .github/workflows/integration-tests.yml: CI pipeline integration\n- artillery-config.yml:\
      \ Load testing configuration\n- playwright.config.ts: Cross-browser test configuration\n"
    design_decisions:
    - decision: Use Playwright for E2E testing with parallel execution
      rationale: Already integrated in codebase, excellent TypeScript support, reliable
        for complex async workflows like ML processing
      alternatives_considered:
      - Cypress
      - Puppeteer
      - Selenium WebDriver
    - decision: Implement staged testing approach (smoke → integration → load → E2E)
      rationale: Fails fast on basic issues before running expensive full-workflow
        tests, optimizes CI pipeline runtime
      alternatives_considered:
      - Run all tests in parallel
      - Single comprehensive test suite
    - decision: Mock external ML APIs for some tests, use real APIs for critical path
        validation
      rationale: Balances test reliability with realistic validation, prevents rate
        limiting during CI
      alternatives_considered:
      - Always use real APIs
      - Always use mocks
    researched_at: '2026-02-07T19:20:32.637662'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:47:09.276326'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a multi-layered testing strategy starting with API contract
      validation, followed by component integration tests, then full E2E user workflows
      with Playwright. Use Artillery for load testing concurrent novel processing.
      Create staging environment mirroring production with realistic test data. Implement
      comprehensive error scenario testing including payment failures, ML API timeouts,
      and network issues.

      '
    external_dependencies:
    - name: '@playwright/test'
      version: ^1.40.0
      reason: E2E testing framework for cross-browser user workflow validation
    - name: artillery
      version: ^2.0.0
      reason: Load testing platform for validating system performance under concurrent
        users
    - name: '@axe-core/playwright'
      version: ^4.8.0
      reason: Accessibility testing integration for compliance validation
    - name: lighthouse
      version: ^11.4.0
      reason: Performance auditing for storefront and dashboard applications
    - name: '@pact-foundation/pact'
      version: ^12.1.0
      reason: Contract testing between frontend and backend API boundaries
    files_to_modify:
    - path: playwright.config.ts
      changes: Add cross-browser configuration, timeout settings for ML processing
    - path: packages/backend/src/services/novel-processor.ts
      changes: Add test hooks and error simulation capabilities
    - path: apps/web/src/lib/api-client.ts
      changes: Add retry logic and better error handling for integration tests
    - path: .github/workflows/ci.yml
      changes: Integrate E2E and load testing into deployment pipeline
    new_files:
    - path: apps/web/tests/e2e/complete-user-journey.spec.ts
      purpose: Primary E2E test covering full user workflow from upload to delivery
    - path: apps/web/tests/e2e/error-scenarios.spec.ts
      purpose: Test error handling, recovery, and edge cases
    - path: apps/web/tests/e2e/cross-browser.spec.ts
      purpose: Browser compatibility testing with responsive design validation
    - path: packages/backend/tests/integration/novel-processing.test.ts
      purpose: Backend service integration testing with external APIs
    - path: packages/backend/tests/integration/payment-flow.test.ts
      purpose: Stripe integration testing with webhook validation
    - path: apps/web/tests/integration/api-contracts.test.ts
      purpose: API contract testing using Pact framework
    - path: artillery-config.yml
      purpose: Load testing configuration for concurrent user scenarios
    - path: apps/web/tests/fixtures/test-novels/
      purpose: Standardized test data for consistent testing
    - path: packages/testing-utils/src/mock-services.ts
      purpose: Reusable mock implementations for external services
    - path: apps/web/tests/helpers/test-setup.ts
      purpose: Common test setup utilities and database seeding
    - path: packages/backend/tests/integration/database-integrity.test.ts
      purpose: Test data consistency and migration validation
    - path: .github/workflows/integration-tests.yml
      purpose: Dedicated CI workflow for integration testing
  acceptance_criteria:
  - criterion: Complete E2E user workflows pass with 95% reliability across Chrome,
      Firefox, Safari
    verification: npm run test:e2e --browsers=all passes 19/20 runs minimum
  - criterion: System handles 50 concurrent novel processing requests without failures
    verification: artillery run artillery-config.yml shows 0% error rate, <5s response
      times
  - criterion: All critical error scenarios are handled gracefully with proper user
      feedback
    verification: 'Error scenario tests pass: payment failures, ML API timeouts, network
      issues'
  - criterion: API contracts are validated between all frontend/backend interfaces
    verification: npm run test:contracts passes with 100% contract compliance
  - criterion: Production deployment pipeline includes automated integration test
      gates
    verification: GitHub Actions workflow blocks deployment on integration test failures
  testing:
    unit_tests:
    - file: packages/backend/src/__tests__/services/novel-processor.test.ts
      coverage_target: 90%
      scenarios:
      - Novel upload and validation
      - ML API timeout handling
      - Processing state transitions
    - file: apps/web/src/__tests__/components/NovelUpload.test.ts
      coverage_target: 85%
      scenarios:
      - File upload validation
      - Progress tracking
      - Error state rendering
    integration_tests:
    - file: packages/backend/tests/integration/novel-processing.test.ts
      scenarios:
      - End-to-end novel processing pipeline
      - Payment integration with Stripe webhooks
      - Database state consistency
    - file: apps/web/tests/integration/api-contracts.test.ts
      scenarios:
      - Frontend-backend API contract validation
      - External service integration
    e2e_tests:
    - file: apps/web/tests/e2e/complete-user-journey.spec.ts
      scenarios:
      - Upload novel → Process → Generate comic → Purchase → Download
      - User registration and authentication flow
      - Admin dashboard comic management
    - file: apps/web/tests/e2e/error-scenarios.spec.ts
      scenarios:
      - Payment failure recovery
      - ML API timeout handling
      - Network interruption resilience
    load_tests:
    - file: artillery-config.yml
      scenarios:
      - 50 concurrent users uploading novels
      - 100 simultaneous comic generations
      - Database connection pool stress test
    manual_testing:
    - step: Cross-browser testing on physical devices
      expected: Consistent UI/UX across Chrome, Firefox, Safari on desktop and mobile
    - step: Production-like data volume testing
      expected: System performance remains stable with realistic data loads
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup staging environment with production-like configuration
      done: false
    - task: Create test data fixtures and database seeding utilities
      done: false
    - task: Implement E2E tests for complete user workflows
      done: false
    - task: Build API contract testing with Pact framework
      done: false
    - task: Configure load testing with Artillery for concurrent scenarios
      done: false
    - task: Develop error scenario testing (timeouts, failures, edge cases)
      done: false
    - task: Setup cross-browser testing configuration
      done: false
    - task: Integrate testing pipeline with GitHub Actions CI/CD
      done: false
    - task: Create comprehensive test documentation and runbooks
      done: false
    - task: Validate production deployment gates and rollback procedures
      done: false
- key: T92
  title: Production Deployment
  type: Task
  milestone: M7 - Launch & Release
  iteration: I7
  priority: p0
  effort: 3
  area: release
  dependsOn:
  - T91
  agent_notes:
    research_findings: '**Context:**

      Production deployment is the final critical step to launch Morpheus to real
      users. This involves setting up robust, scalable infrastructure that can handle
      real traffic, ensure high availability, implement proper monitoring, and maintain
      security standards. The platform needs to serve both the customer-facing storefront
      and creator dashboard while processing computationally intensive novel-to-comic
      transformations. This is essential for generating revenue and validating the
      product-market fit.


      **Technical Approach:**

      - Container orchestration with Docker + Kubernetes or managed services (Vercel
      for Next.js apps, Railway/Render for Fastify backend)

      - Multi-environment setup (staging, production) with environment-specific configurations

      - CDN integration for static assets and image delivery (Cloudflare/AWS CloudFront)

      - Load balancing and auto-scaling for the Fastify backend

      - Database connection pooling and read replicas for Supabase

      - CI/CD pipeline with automated testing, building, and deployment

      - Infrastructure as Code (Terraform/Pulumi) for reproducible deployments

      - Comprehensive monitoring with health checks, metrics, and alerting

      - Security hardening: HTTPS, rate limiting, DDoS protection, secret management


      **Dependencies:**

      - External: Docker, Kubernetes/managed platform, monitoring tools (DataDog/New
      Relic), CDN provider

      - Internal: All application services must be production-ready, database migrations,
      environment configuration management


      **Risks:**

      - Downtime during initial deployment: Use blue-green or rolling deployments

      - Database migration failures: Implement rollback procedures and test migrations
      thoroughly

      - Resource exhaustion under load: Implement auto-scaling and load testing

      - Secret leakage: Use proper secret management (Vault, K8s secrets, platform-managed)

      - RunPod API rate limits/failures: Implement circuit breakers and fallback mechanisms

      - Cost overruns: Set up billing alerts and resource quotas


      **Complexity Notes:**

      This is significantly more complex than initially appears. Beyond basic deployment,
      it requires orchestrating multiple services (frontend, backend, database, ML
      APIs), handling asynchronous job processing, managing secrets across environments,
      implementing proper observability, and ensuring the system can handle the computational
      load of image generation at scale.


      **Key Files:**

      - apps/backend/Dockerfile: Containerize Fastify application

      - apps/dashboard/next.config.js: Production optimizations and environment variables

      - apps/storefront/next.config.js: CDN configuration and performance tuning

      - packages/database/migrations/: Production database schema

      - .github/workflows/deploy.yml: CI/CD pipeline configuration

      - docker-compose.prod.yml: Production service orchestration

      - k8s/: Kubernetes manifests for deployment, services, ingress

      - terraform/: Infrastructure as code definitions

      '
    design_decisions:
    - decision: 'Hybrid deployment: Vercel for Next.js frontends, managed container
        service for Fastify backend'
      rationale: Leverages platform strengths - Vercel excels at Next.js deployment
        with global CDN, while managed containers provide flexibility for the backend
        without K8s complexity
      alternatives_considered:
      - Full Kubernetes cluster
      - All-in-one platform like Railway
      - AWS ECS with ALB
    - decision: Implement asynchronous job processing with Redis/BullMQ for comic
        generation
      rationale: Comic generation is CPU/time intensive and should not block HTTP
        requests. Queue system allows for better resource management and user experience
      alternatives_considered:
      - Synchronous processing with long timeouts
      - Serverless functions
      - Background jobs in database
    - decision: Use Supabase managed PostgreSQL with connection pooling
      rationale: Supabase provides production-grade managed database with built-in
        connection pooling, backups, and monitoring without operational overhead
      alternatives_considered:
      - Self-managed PostgreSQL on VPS
      - AWS RDS
      - PlanetScale MySQL
    researched_at: '2026-02-07T19:20:59.611249'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:47:36.498822'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: "Deploy Next.js applications to Vercel with environment-specific configurations\
      \ and CDN optimization. \nContainerize the Fastify backend and deploy to a managed\
      \ container platform (Railway/Render) with auto-scaling enabled.\nImplement\
      \ a job queue system using Redis and BullMQ to handle asynchronous comic generation\
      \ tasks.\nSet up comprehensive monitoring with health check endpoints, error\
      \ tracking, and performance metrics.\nCreate a CI/CD pipeline that runs tests,\
      \ builds containers, and deploys through staging to production environments.\n"
    external_dependencies:
    - name: bullmq
      version: ^5.0.0
      reason: Robust job queue system for handling comic generation tasks asynchronously
    - name: ioredis
      version: ^5.3.0
      reason: Redis client for job queue and caching in production environment
    - name: '@sentry/node'
      version: ^7.0.0
      reason: Error tracking and performance monitoring in production
    - name: pino
      version: ^8.0.0
      reason: High-performance JSON logger for production logging and observability
    - name: helmet
      version: ^7.0.0
      reason: Security headers and hardening for the Fastify backend in production
    - name: '@fastify/rate-limit'
      version: ^9.0.0
      reason: Rate limiting to protect against abuse and ensure fair resource usage
    files_to_modify:
    - path: apps/backend/package.json
      changes: Add production dependencies, health check scripts, and container startup
        commands
    - path: apps/backend/src/server.ts
      changes: Add graceful shutdown handling, health check endpoints, and production
        logging
    - path: apps/dashboard/next.config.js
      changes: Configure CDN settings, environment variables, and performance optimizations
    - path: apps/storefront/next.config.js
      changes: Add image optimization, CDN integration, and production build settings
    - path: packages/database/src/index.ts
      changes: Implement connection pooling, read replica support, and production
        connection strings
    - path: .github/workflows/test.yml
      changes: Extend to include deployment pipeline and environment promotion
    new_files:
    - path: apps/backend/Dockerfile
      purpose: Containerize Fastify application with multi-stage build and security
        hardening
    - path: apps/backend/src/health.ts
      purpose: Comprehensive health check endpoints for Kubernetes readiness and liveness
        probes
    - path: apps/backend/src/queue/index.ts
      purpose: Redis-based job queue implementation using BullMQ for comic generation
        tasks
    - path: docker-compose.prod.yml
      purpose: Production container orchestration with proper networking and volume
        mounts
    - path: k8s/backend-deployment.yaml
      purpose: Kubernetes deployment manifest with auto-scaling, resource limits,
        and health checks
    - path: k8s/ingress.yaml
      purpose: Nginx ingress controller configuration with SSL termination and load
        balancing
    - path: terraform/main.tf
      purpose: Infrastructure as code for cloud resources, networking, and managed
        services
    - path: terraform/monitoring.tf
      purpose: Monitoring stack deployment with Prometheus, Grafana, and alerting
        rules
    - path: .env.production
      purpose: Production environment variables with database URLs, API keys, and
        service endpoints
    - path: scripts/deploy.sh
      purpose: Deployment automation script with rollback capability and health verification
    - path: scripts/migrate.sh
      purpose: Database migration script with backup and rollback procedures
    - path: monitoring/alerts.yaml
      purpose: Alert manager configuration for critical system notifications
  acceptance_criteria:
  - criterion: All services (storefront, dashboard, backend) are successfully deployed
      and accessible via HTTPS with proper SSL certificates
    verification: curl -I https://app.morpheus.com returns 200, curl -I https://dashboard.morpheus.com
      returns 200, curl -I https://api.morpheus.com/health returns 200
  - criterion: Comic generation workflow completes end-to-end in production environment
      with proper job queue processing
    verification: Submit novel through storefront, verify job creation in Redis, confirm
      comic generation via RunPod API, validate final output delivery
  - criterion: System handles concurrent load with auto-scaling enabled and response
      times under 2s for API calls
    verification: Load test with 100 concurrent users, verify auto-scaling triggers,
      measure 95th percentile response times < 2000ms
  - criterion: Monitoring and alerting system captures critical metrics and sends
      notifications for failures
    verification: Health checks report green status, error tracking captures exceptions,
      uptime monitoring shows >99.5% availability
  - criterion: Database migrations and environment configurations are properly applied
      across staging and production
    verification: Run npm run db:migrate in production, verify all tables exist, confirm
      environment variables are correctly set
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/health.test.ts
      coverage_target: 90%
      scenarios:
      - Health endpoint returns proper status
      - Database connection check
      - Redis connectivity validation
    - file: apps/backend/src/__tests__/queue.test.ts
      coverage_target: 85%
      scenarios:
      - Job creation and processing
      - Queue failure handling
      - Job retry logic
    integration_tests:
    - file: apps/backend/src/__tests__/integration/deployment.test.ts
      scenarios:
      - Full comic generation pipeline
      - Environment variable loading
      - Database connection pooling
      - External API connectivity
    load_tests:
    - file: tests/load/production-load.js
      scenarios:
      - Concurrent user simulation
      - Auto-scaling validation
      - Resource utilization monitoring
    manual_testing:
    - step: Deploy to staging environment and verify all services start
      expected: All containers healthy, logs show no errors, health checks pass
    - step: Test complete user journey from novel upload to comic delivery
      expected: Comic generated successfully, files delivered via CDN, user receives
        notification
    - step: Simulate service failure and verify recovery
      expected: Auto-scaling responds, circuit breakers engage, graceful degradation
        occurs
  estimates:
    development: 8
    code_review: 2
    testing: 3
    documentation: 1
    total: 14
  progress:
    status: not-started
    checklist:
    - task: Set up container registry and build multi-stage Dockerfiles
      done: false
    - task: Configure Vercel deployments for Next.js apps with environment-specific
        settings
      done: false
    - task: Deploy Fastify backend to managed container platform with auto-scaling
      done: false
    - task: Implement Redis job queue system with BullMQ for asynchronous processing
      done: false
    - task: Set up Supabase production database with connection pooling and read replicas
      done: false
    - task: Configure CDN integration for static assets and image delivery
      done: false
    - task: Implement comprehensive monitoring with health checks and error tracking
      done: false
    - task: Create CI/CD pipeline with automated testing and deployment
      done: false
    - task: Perform load testing and validate auto-scaling behavior
      done: false
    - task: Configure security measures including rate limiting, HTTPS, and secret
        management
      done: false
    - task: Create runbooks and deployment documentation
      done: false
    - task: Conduct production readiness review and go-live checklist
      done: false
- key: T93
  title: Launch & Release Notes
  type: Task
  milestone: M7 - Launch & Release
  iteration: I7
  priority: p0
  effort: 2
  area: release
  dependsOn:
  - T92
  agent_notes:
    research_findings: '**Context:**

      This task encompasses the complete launch process for Morpheus, including production
      deployment, release notes creation, version management, and post-launch monitoring.
      As a p0 milestone task, this represents the culmination of all development work
      and the transition from development to production operations. The task needs
      to coordinate technical deployment with marketing communications, user onboarding,
      and operational readiness.


      **Technical Approach:**

      - Implement semantic versioning with conventional commits for automated changelog
      generation

      - Use GitHub Actions for CI/CD pipeline with automated deployment to production

      - Deploy backend to Railway/Render with horizontal scaling capabilities

      - Deploy frontend to Vercel with edge functions and CDN optimization

      - Implement comprehensive monitoring with Sentry for error tracking and analytics

      - Create automated release notes from commit messages and PR descriptions

      - Set up database migrations and backup strategies for Supabase production

      - Configure production environment variables and secrets management


      **Dependencies:**

      - External: [@semantic-release/github, @semantic-release/changelog, sentry,
      posthog, vercel, railway-cli]

      - Internal: All M1-M6 milestone deliverables, authentication system, payment
      processing, ML pipeline, admin dashboard


      **Risks:**

      - Database migration failures: Implement rollback procedures and staging environment
      testing

      - ML service overload on launch: Configure rate limiting and queue management
      with Bull/Redis

      - Payment processing issues: Test Stripe webhooks thoroughly in production environment

      - Frontend performance under load: Implement proper caching strategies and CDN
      configuration

      - API rate limiting from OpenAI/Anthropic: Implement fallback strategies and
      user queue system


      **Complexity Notes:**

      This is significantly more complex than initially estimated as it requires orchestrating
      multiple deployment targets, ensuring zero-downtime deployment, coordinating
      external service configurations, and managing production data safely. The complexity
      extends beyond pure technical implementation to include operational procedures
      and incident response planning.


      **Key Files:**

      - .github/workflows/release.yml: Production deployment pipeline

      - apps/backend/package.json: Version management and build scripts

      - apps/dashboard/next.config.js: Production optimizations and environment config

      - packages/database/migrations/: Production database schema updates

      - docs/CHANGELOG.md: Automated release notes generation

      - deployment/docker/: Production containerization configs

      '
    design_decisions:
    - decision: Use semantic-release for automated versioning and changelog generation
      rationale: Automates release process, ensures consistent versioning, generates
        professional release notes from commit history
      alternatives_considered:
      - Manual versioning
      - Custom release scripts
      - GitHub Releases only
    - decision: Deploy backend to Railway with Docker containers
      rationale: Provides easy scaling, built-in monitoring, seamless integration
        with GitHub, and cost-effective for startup phase
      alternatives_considered:
      - AWS ECS
      - Google Cloud Run
      - DigitalOcean App Platform
    - decision: Implement feature flags using environment variables and database toggles
      rationale: Allows gradual rollout of features and quick rollback without redeployment
      alternatives_considered:
      - LaunchDarkly
      - Split.io
      - No feature flags
    researched_at: '2026-02-07T19:21:22.586659'
    researched_by: research-agent-claude-sonnet-4
    planned_at: '2026-02-08T14:48:00.497284'
    planned_by: planning-agent-claude-sonnet-4
  technical_notes:
    approach: 'Implement a comprehensive launch pipeline using semantic-release for
      automated versioning, GitHub Actions for CI/CD orchestration, and multi-stage
      deployment to Vercel (frontend) and Railway (backend). Configure production
      monitoring with Sentry and PostHog, implement database migration strategies
      with Supabase, and create automated release notes generation. Set up proper
      environment configuration management and implement gradual feature rollout capabilities
      with monitoring dashboards for launch day operations.

      '
    external_dependencies:
    - name: semantic-release
      version: ^22.0.0
      reason: Automated versioning and release notes generation from commit messages
    - name: '@sentry/node'
      version: ^7.80.0
      reason: Production error tracking and performance monitoring for backend services
    - name: '@sentry/nextjs'
      version: ^7.80.0
      reason: Frontend error tracking and user session recording
    - name: posthog-js
      version: ^1.90.0
      reason: User analytics and feature usage tracking post-launch
    - name: railway
      version: ^3.4.0
      reason: CLI tools for backend deployment and production management
    - name: conventional-changelog-cli
      version: ^4.1.0
      reason: Generate formatted changelogs from conventional commit messages
    files_to_modify:
    - path: package.json
      changes: Add semantic-release configuration and release scripts
    - path: apps/backend/src/app.ts
      changes: Initialize Sentry, health check endpoints, graceful shutdown
    - path: apps/dashboard/next.config.js
      changes: Add production optimizations, environment variables
    - path: apps/backend/src/config/database.ts
      changes: Add production connection pooling and migration configs
    - path: packages/shared/src/monitoring.ts
      changes: Add monitoring utilities and error tracking helpers
    new_files:
    - path: .github/workflows/release.yml
      purpose: Production deployment pipeline with semantic-release
    - path: .github/workflows/staging.yml
      purpose: Staging deployment for pre-launch testing
    - path: apps/backend/src/services/release.service.ts
      purpose: Handle version management and release operations
    - path: apps/backend/src/middleware/monitoring.ts
      purpose: Request/response monitoring and error tracking
    - path: apps/backend/src/routes/health.ts
      purpose: Health check endpoints for load balancer
    - path: apps/dashboard/src/pages/admin/launch-dashboard.tsx
      purpose: Real-time launch monitoring interface
    - path: deployment/production/.env.example
      purpose: Production environment variable template
    - path: deployment/scripts/migrate.sh
      purpose: Database migration script with rollback
    - path: deployment/scripts/rollback.sh
      purpose: Automated rollback procedure
    - path: docs/DEPLOYMENT.md
      purpose: Production deployment and operations guide
    - path: docs/LAUNCH_RUNBOOK.md
      purpose: Launch day procedures and incident response
    - path: k6/load-test.js
      purpose: Load testing script for launch preparation
    - path: release.config.js
      purpose: Semantic-release configuration
  acceptance_criteria:
  - criterion: Production deployment pipeline successfully deploys backend to Railway
      and frontend to Vercel with zero downtime
    verification: Run `npm run deploy:prod` and verify services are accessible at
      production URLs with health checks returning 200
  - criterion: Automated release notes generation creates comprehensive changelog
      from semantic commits
    verification: Tag a release and verify CHANGELOG.md is updated with features,
      fixes, and breaking changes from commit history
  - criterion: Production monitoring captures errors, performance metrics, and user
      analytics
    verification: Check Sentry dashboard shows error tracking, PostHog shows user
      events, and custom metrics are reported
  - criterion: Database migrations run successfully with rollback capability in production
    verification: Execute migration in staging, verify schema changes, test rollback
      procedure
  - criterion: Launch day operations dashboard provides real-time system health and
      user metrics
    verification: Access admin dashboard showing API response times, active users,
      error rates, and ML pipeline status
  testing:
    unit_tests:
    - file: apps/backend/src/__tests__/services/release.test.ts
      coverage_target: 90%
      scenarios:
      - Version increment logic
      - Changelog generation
      - Migration validation
    - file: apps/backend/src/__tests__/middleware/monitoring.test.ts
      coverage_target: 85%
      scenarios:
      - Error tracking initialization
      - Performance metrics collection
      - Health check endpoints
    integration_tests:
    - file: apps/backend/src/__tests__/integration/deployment.test.ts
      scenarios:
      - Database migration workflow
      - Service health checks
      - Environment configuration loading
    - file: apps/backend/src/__tests__/integration/monitoring.test.ts
      scenarios:
      - Error reporting to Sentry
      - Analytics event tracking
      - Performance monitoring
    e2e_tests:
    - file: apps/dashboard/cypress/e2e/launch-monitoring.cy.ts
      scenarios:
      - Admin dashboard displays production metrics
      - Error alerts trigger notifications
      - User flow tracking works end-to-end
    manual_testing:
    - step: Deploy to staging environment and run full user journey
      expected: All features work without errors, monitoring captures events
    - step: Simulate high load with k6 load testing
      expected: System handles expected launch traffic, auto-scaling works
    - step: Test rollback procedure on staging
      expected: Previous version restored within 5 minutes
  estimates:
    development: 5
    code_review: 1
    testing: 2
    documentation: 1
    total: 9
  progress:
    status: not-started
    checklist:
    - task: Setup semantic-release and versioning configuration
      done: false
    - task: Create production CI/CD pipeline with GitHub Actions
      done: false
    - task: Configure production environment variables and secrets
      done: false
    - task: Implement monitoring and error tracking with Sentry/PostHog
      done: false
    - task: Create database migration and rollback procedures
      done: false
    - task: Build launch monitoring dashboard for operations team
      done: false
    - task: Setup production deployments for Railway and Vercel
      done: false
    - task: Create load testing and performance validation
      done: false
    - task: Write deployment documentation and runbooks
      done: false
    - task: Conduct staging deployment and rollback testing
      done: false
